{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5084161,"sourceType":"datasetVersion","datasetId":2952181}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install medpy\n! pip install segmentation_models_pytorch\n! pip install -U albumentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:09:56.132331Z","iopub.execute_input":"2025-03-15T09:09:56.132564Z","iopub.status.idle":"2025-03-15T09:10:23.015317Z","shell.execute_reply.started":"2025-03-15T09:09:56.132543Z","shell.execute_reply":"2025-03-15T09:10:23.014443Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting medpy\n  Downloading medpy-0.5.2.tar.gz (156 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from medpy) (1.13.1)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from medpy) (1.26.4)\nRequirement already satisfied: SimpleITK>=2.1 in /usr/local/lib/python3.10/dist-packages (from medpy) (2.4.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->medpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->medpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->medpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->medpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->medpy) (2024.2.0)\nBuilding wheels for collected packages: medpy\n  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for medpy: filename=MedPy-0.5.2-cp310-cp310-linux_x86_64.whl size=762838 sha256=2c68dc339f4d6d539b85b98aa7e9995c8ee22c1df3a233041dc8a130462ca04f\n  Stored in directory: /root/.cache/pip/wheels/a1/b8/63/bdf557940ec60d1b8822e73ff9fbe7727ac19f009d46b5d175\nSuccessfully built medpy\nInstalling collected packages: medpy\nSuccessfully installed medpy-0.5.2\nCollecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\nCollecting efficientnet-pytorch>=0.6.1 (from segmentation_models_pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.29.0)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (11.0.0)\nCollecting pretrainedmodels>=0.7.1 (from segmentation_models_pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.17.0)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.0.12)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2.4.1)\nCollecting munch (from pretrainedmodels>=0.7.1->segmentation_models_pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9->segmentation_models_pytorch) (0.4.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation_models_pytorch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nDownloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=6eb3dc58d16839ca14aa8959db787aaf5ff89c2b480796097bae995991974573\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=efa82239403fab7a3841436313a83816e45114f981ab844a6fbe54a7cd7822f9\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, pretrainedmodels, segmentation_models_pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.4.0\nRequirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\nCollecting albumentations\n  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\nCollecting albucore==0.0.23 (from albumentations)\n  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\nCollecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nDownloading albumentations-2.0.5-py3-none-any.whl (290 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\nDownloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\n  Attempting uninstall: albucore\n    Found existing installation: albucore 0.0.19\n    Uninstalling albucore-0.0.19:\n      Successfully uninstalled albucore-0.0.19\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.4.20\n    Uninstalling albumentations-1.4.20:\n      Successfully uninstalled albumentations-1.4.20\nSuccessfully installed albucore-0.0.23 albumentations-2.0.5 simsimd-6.2.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport copy\nimport glob\nimport random\nimport warnings\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport cv2  # CLAHE 적용을 위해 OpenCV 사용\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nimport torchvision.models as models\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage.segmentation import find_boundaries\nfrom skimage.measure import label, regionprops\nimport scipy.ndimage\n\n# Albumentations 기반 augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# medpy HD metric\nfrom medpy.metric import binary as medpy_binary\n\n###############################################\n# Seed 및 Warning 설정\n###############################################\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\nwarnings.filterwarnings('ignore')\n\n###############################################\n# BUSI Segmentation Dataset\n###############################################\nclass BUSISegmentationDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform  \n        self.samples = []  # 반드시 초기화\n        self._prepare_samples()\n\n    def _prepare_samples(self):\n        labels = os.listdir(self.data_path)\n        for label in labels:\n            folder_path = os.path.join(self.data_path, label)\n            if not os.path.isdir(folder_path):\n                continue\n            files = os.listdir(folder_path)\n            image_files = sorted([f for f in files if '_mask' not in f and f.endswith('.png')])\n            mask_files  = sorted([f for f in files if '_mask' in f and f.endswith('.png')])\n            pattern_img = re.compile(rf'{re.escape(label)} \\((\\d+)\\)\\.png')\n            pattern_mask = re.compile(rf'{re.escape(label)} \\((\\d+)\\)_mask(?:_\\d+)?\\.png')\n            mask_dict = {}\n            for mf in mask_files:\n                m = pattern_mask.fullmatch(mf)\n                if m:\n                    idx = m.group(1)\n                    mask_dict.setdefault(idx, []).append(mf)\n            for im in image_files:\n                m = pattern_img.fullmatch(im)\n                if m:\n                    idx = m.group(1)\n                    img_path = os.path.join(folder_path, im)\n                    if idx in mask_dict:\n                        mask_paths = [os.path.join(folder_path, mf) for mf in mask_dict[idx]]\n                        combined_mask = None\n                        for mp in mask_paths:\n                            mask_img = Image.open(mp).convert('L')\n                            mask_arr = np.array(mask_img)\n                            mask_binary = (mask_arr > 128).astype(np.uint8)\n                            if combined_mask is None:\n                                combined_mask = mask_binary\n                            else:\n                                combined_mask = np.maximum(combined_mask, mask_binary)\n                        self.samples.append((img_path, combined_mask, label))\n                    else:\n                        image_pil = Image.open(img_path)\n                        empty_mask = np.zeros(image_pil.size[::-1], dtype=np.uint8)\n                        self.samples.append((img_path, empty_mask, label))\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, mask_array, label = self.samples[index]\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.fromarray((mask_array * 255).astype(np.uint8))\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        return image, mask, label\n\n###############################################\n# Per-image Z-score normalization (adaptive)\n###############################################\ndef z_score_normalize(tensor):\n    # tensor: (C, H, W)\n    mean = tensor.mean()\n    std = tensor.std() + 1e-6\n    return (tensor - mean) / std\n\n###############################################\n# CLAHE Augmentation 함수 (OpenCV)\n###############################################\ndef apply_clahe(image, clipLimit=2.0, tileGridSize=(8,8)):\n    img_np = np.array(image)\n    if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl, a, b))\n        final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    else:\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        final = clahe.apply(img_np)\n    return Image.fromarray(final)\n\n###############################################\n# joint_transform (Albumentations 기반 Augmentation)\n###############################################\ndef joint_transform(image, mask, size=(224,224)):\n    # 1. Geometric transforms (image와 mask에 동시에 적용)\n    geom_transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=15, p=0.5),\n        A.ElasticTransform(alpha=100, sigma=10, alpha_affine=15, p=0.5),\n        A.Resize(height=size[0], width=size[1])\n    ])\n    image_np = np.array(image)\n    mask_np = np.array(mask)\n    augmented = geom_transform(image=image_np, mask=mask_np)\n    image = augmented['image']\n    mask = augmented['mask']\n    \n    # 2. Intensity transforms (오직 image에만 적용)\n    intensity_transform = A.Compose([\n        A.RandomBrightnessContrast(p=0.5),\n        A.CLAHE(clip_limit=3.0, p=0.5),\n        A.GaussianBlur(p=0.3)\n    ])\n    image = intensity_transform(image=image)['image']\n    \n    # 3. ToTensor 및 per-image z-score normalization (image에 적용)\n    image = transforms.ToTensor()(image)\n    image = z_score_normalize(image)\n    mask = transforms.ToTensor()(mask)\n    return image, mask\n\n###############################################\n# CutMix 함수 (강화: alpha=1.5, p=0.7)\n###############################################\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(images, masks, alpha=1.5, p=0.7):\n    if np.random.rand() > p:\n        return images, masks\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(images.size(0)).to(images.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n    masks[:, :, bbx1:bbx2, bby1:bby2] = masks[rand_index, :, bbx1:bbx2, bby1:bby2]\n    return images, masks\n\n###############################################\n# Advanced 후처리: Morphological Closing 포함\n###############################################\ndef postprocess_mask(mask, min_size=100):\n    # 기본적으로 작은 객체 제거\n    labeled_mask = label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    # Morphological closing (hole filling)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n###############################################\n# 1. AttentionGate 모듈\n###############################################\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(AttentionGate, self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi\n\n###############################################\n# 2. MultiScaleFusion 모듈\n###############################################\nclass MultiScaleFusion(nn.Module):\n    def __init__(self, in_channels_list, out_channels):\n        super(MultiScaleFusion, self).__init__()\n        self.convs = nn.ModuleList([nn.Conv2d(ch, out_channels, kernel_size=1) for ch in in_channels_list])\n        self.fuse = nn.Sequential(\n            nn.Conv2d(len(in_channels_list)*out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, features):\n        target_size = features[-1].size()[2:]\n        processed = []\n        for i, f in enumerate(features):\n            if f.size()[2:] != target_size:\n                f = F.interpolate(f, size=target_size, mode='bilinear', align_corners=False)\n            f = self.convs[i](f)\n            processed.append(f)\n        out = torch.cat(processed, dim=1)\n        out = self.fuse(out)\n        return out\n\n###############################################\n# 3. 기타 모델 관련 모듈\n###############################################\nclass StochasticDepth(nn.Module):\n    def __init__(self, p, mode=\"row\"):\n        super(StochasticDepth, self).__init__()\n        self.p = p\n        self.mode = mode\n\n    def forward(self, x):\n        if not self.training or self.p == 0.0:\n            return x\n        survival_rate = 1 - self.p\n        if self.mode == \"row\":\n            batch_size = x.shape[0]\n            noise = torch.rand(batch_size, 1, 1, 1, device=x.device, dtype=x.dtype)\n            binary_mask = (noise < survival_rate).float()\n            return x / survival_rate * binary_mask\n        else:\n            if torch.rand(1).item() < self.p:\n                return torch.zeros_like(x)\n            else:\n                return x\n\nclass GhostModule(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, ratio=2, dw_kernel_size=3, stride=1, padding=0, use_relu=True):\n        super(GhostModule, self).__init__()\n        self.out_channels = out_channels\n        self.primary_channels = int(torch.ceil(torch.tensor(out_channels / ratio)))\n        self.cheap_channels = out_channels - self.primary_channels\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(in_channels, self.primary_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(self.primary_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n        self.cheap_conv = nn.Sequential(\n            nn.Conv2d(self.primary_channels, self.cheap_channels, dw_kernel_size, stride=1,\n                      padding=dw_kernel_size//2, groups=self.primary_channels, bias=False),\n            nn.BatchNorm2d(self.cheap_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_conv(x1)\n        out = torch.cat([x1, x2], dim=1)\n        return out[:, :self.out_channels, :, :].contiguous()\n\ndef ghost_conv_block(in_channels, out_channels, use_relu=True):\n    return nn.Sequential(\n        GhostModule(in_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu),\n        GhostModule(out_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu)\n    )\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\nclass DynamicGating(nn.Module):\n    def __init__(self, num_branches, hidden_dim=64, dropout_prob=0.1, init_temperature=2.0, iterations=3):\n        super(DynamicGating, self).__init__()\n        self.temperature = init_temperature\n        self.iterations = iterations\n        self.dropout_prob = dropout_prob\n        self.fc = nn.Sequential(\n            nn.Linear(num_branches, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, num_branches)\n        )\n        self.layernorm = nn.LayerNorm(num_branches)\n        self.softmax = nn.Softmax(dim=1)\n        self.res_scale = nn.Parameter(torch.ones(1))\n        self.saved_log_probs = None\n        self.entropy = None\n        self.rl_loss = 0.0\n\n    def forward(self, features):\n        norm_features = self.layernorm(features)\n        logits = self.fc(norm_features)\n        logits = self.layernorm(logits)\n        scaled_logits = logits / self.temperature\n        gates = self.softmax(scaled_logits)\n        for _ in range(self.iterations - 1):\n            updated_features = norm_features + self.res_scale * gates\n            logits = self.fc(updated_features)\n            logits = self.layernorm(logits)\n            scaled_logits = logits / self.temperature\n            new_gates = self.softmax(scaled_logits)\n            gates = 0.5 * gates + 0.5 * new_gates\n        return gates\n\n###############################################\n###############################################\n###############################################\n\nclass QuadAgentBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5):\n        super().__init__()\n        branch_channels = out_channels // 4\n        self.branch1 = GhostModule(in_channels, branch_channels, ratio=4)\n        # branch2: kernel_size=3, padding=1로 spatial size 유지\n        self.branch2 = GhostModule(in_channels, branch_channels, kernel_size=3, padding=1, ratio=4)\n        self.branch3 = nn.Sequential(\n            GhostModule(in_channels, branch_channels, ratio=4),\n            SELayer(branch_channels, reduction=32)\n        )\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        # num_branches를 3으로 설정\n        self.gating = DynamicGating(num_branches=3, hidden_dim=gating_hidden_dim, dropout_prob=gating_dropout, init_temperature=gating_temperature)\n        self.fusion_conv = GhostModule(branch_channels * 3, out_channels, ratio=2)\n        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False) if in_channels != out_channels else nn.Identity()\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob)\n\n    def forward(self, x):\n        b1 = self.branch1(x)\n        b2 = self.branch2(x)\n        b3 = self.branch3(x)\n        gap_b1 = self.gap(b1).view(x.size(0), -1)\n        gap_b2 = self.gap(b2).view(x.size(0), -1)\n        gap_b3 = self.gap(b3).view(x.size(0), -1)\n        features = torch.stack([gap_b1.mean(dim=1), gap_b2.mean(dim=1), gap_b3.mean(dim=1)], dim=1)\n        gates = self.gating(features)  # (B, 3)\n        b1 = b1 * gates[:, 0].view(-1, 1, 1, 1)\n        b2 = b2 * gates[:, 1].view(-1, 1, 1, 1)\n        b3 = b3 * gates[:, 2].view(-1, 1, 1, 1)\n        out = torch.cat([b1, b2, b3], dim=1)\n        out = self.fusion_conv(out)\n        res = self.res_conv(x)\n        res = self.stochastic_depth(res)\n        return out + res\n\n\n\"\"\"\n🔥 결과 기대 효과\n개선 방법\t설명\t기대 효과\n브랜치 개수 축소\t4개 → 3개로 변경\t연산량 약 25% 절감\nDilation → Dynamic Conv\tDepthwise 적용\t경량화 및 성능 유지\nSE → Lightweight SE\t채널 축소 후 적용\t성능 유지, 메모리 절감\nGating 개선\tSoftmax → Gumbel-Softmax\t학습 속도 및 게이팅 성능 향상\nFusion 최적화\tConv1x1 → GhostFusion\t연산량 절감, 성능 유지\n\n\"\"\"\n###############################################\n###############################################\n###############################################\n\n\n###############################################\n# 4. PretrainedResNetUNet_AttentionFusion (전체 모델)\n###############################################\n###############################################\n# EfficientNet-B0 Backbone 기반 UNet with Attention + Lite Bottleneck\n###############################################\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm  # timm 라이브러리 (ViT 백본)\nfrom medpy.metric import binary as medpy_binary\n\n###############################################\n# Transformer Bottleneck (추가)\n###############################################\nclass TransformerBottleneck(nn.Module):\n    def __init__(self, d_model=768, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1):\n        super(TransformerBottleneck, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n                                                    dim_feedforward=dim_feedforward, dropout=dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    def forward(self, x):\n        # x: (B, C, H, W) -> (HW, B, C)\n        B, C, H, W = x.size()\n        x = x.view(B, C, H*W).permute(2, 0, 1)\n        x = self.transformer(x)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\n###############################################\n# UNet Decoder (간단한 Decoder)\n###############################################\nclass UNetDecoder(nn.Module):\n    def __init__(self):\n        super(UNetDecoder, self).__init__()\n        self.up1 = nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True)\n        )\n        self.up2 = nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2)\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n        self.up3 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(96, 96, kernel_size=3, padding=1),\n            nn.BatchNorm2d(96),\n            nn.ReLU(inplace=True)\n        )\n        self.up4 = nn.ConvTranspose2d(96, 64, kernel_size=2, stride=2)\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        x = self.up1(x)\n        x = self.conv1(x)\n        x = self.up2(x)\n        x = self.conv2(x)\n        x = self.up3(x)\n        x = self.conv3(x)\n        x = self.up4(x)\n        x = self.conv4(x)\n        return x\n\n###############################################\n# DARKViT_UNet: ViT 기반 Encoder + QuadAgentBlock + Transformer Bottleneck + UNet Decoder\n###############################################\nfrom timm.models import create_model\n\nclass DARKViT_UNet(nn.Module):\n    def __init__(self, out_channels=1):\n        super(DARKViT_UNet, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True)\n        self.bottleneck = QuadAgentBlock(in_channels=1024, out_channels=1024, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1024, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1024, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n    \n    def forward(self, x):\n        features_list = self.encoder(x)  # features_list: [f0, f1, f2, f3]\n        # f3가 NHWC 형식로 반환되는 경우 차원 변환\n        f3 = features_list[-1].permute(0, 3, 1, 2)  # (B,1024,H,W)\n        b_out = self.bottleneck(f3)\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        # 만약 decoder가 NCHW 형식을 요구한다면, features_list 나 f3만 교체합니다.\n        features_list[-1] = t_out2\n        decoded = self.decoder(features_list)\n        return self.final_conv(decoded)\n\n\n\n###############################################\n# 5. Loss & Metric Functions\n###############################################\nfrom segmentation_models_pytorch.losses import LovaszLoss\nlv_loss = LovaszLoss(mode='binary')\n\ndef focal_tversky_loss(pred, target, alpha=0.5, beta=0.5, gamma=4/3, smooth=1e-6):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred = pred.view(pred.size(0), -1)\n    target = target.view(target.size(0), -1)\n    tp = (pred * target).sum(dim=1)\n    fp = ((1 - target) * pred).sum(dim=1)\n    fn = (target * (1 - pred)).sum(dim=1)\n    tversky_index = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n    loss = (1 - tversky_index) ** gamma\n    return loss.mean()\n\ndef boundary_loss(pred, target):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_np = pred.detach().cpu().numpy()\n    target_np = target.detach().cpu().numpy()\n    boundary_masks = []\n    for i in range(pred_np.shape[0]):\n        gt_mask = target_np[i, 0]\n        boundary = find_boundaries(gt_mask, mode='thick')\n        boundary_masks.append(boundary.astype(np.float32))\n    boundary_masks = np.stack(boundary_masks, axis=0)[:, None, :, :]\n    boundary_masks_torch = torch.from_numpy(boundary_masks).to(pred.device)\n    intersect = (pred * boundary_masks_torch).sum()\n    denom = pred.sum() + boundary_masks_torch.sum()\n    boundary_dice = (2.0 * intersect) / (denom + 1e-6)\n    return 1.0 - boundary_dice\n\ndef combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    loss_ft = focal_tversky_loss(outputs, masks)\n    if pos_weight is not None:\n        loss_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, masks.float())\n    else:\n        loss_bce = nn.BCEWithLogitsLoss()(outputs, masks.float())\n    bl = boundary_loss(outputs, masks)\n    lv = lv_loss(outputs, masks)\n    return 0.4 * lv + 0.3 * loss_ft + 0.3 * loss_bce + lambda_boundary * bl\n\ndef dice_f1_precision_recall(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    precision = intersection / (pred_bin.sum() + smooth)\n    recall = intersection / (target_bin.sum() + smooth)\n    f1 = 2 * (precision * recall) / (precision + recall + smooth)\n    return f1.item(), precision.item(), recall.item()\n\ndef iou_metric(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    union = pred_bin.sum() + target_bin.sum() - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef postprocess_mask(mask, min_size=100):\n    labeled_mask = label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    # Kernel 크기를 (7,7)로 늘려 후처리 강화\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7,7))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n\ndef compute_hd_metric(outputs, masks, threshold=0.5):\n    # outputs와 masks를 후처리하여 binary mask로 변환한 후, 각 sample마다 HD (hd95)를 계산\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n    masks_np = masks.detach().cpu().numpy()\n    hd_list = []\n    for i in range(outputs.size(0)):\n        pred_bin = (pred_probs[i, 0] > threshold).astype(np.uint8)\n        gt_bin = (masks_np[i, 0] > 0.5).astype(np.uint8)\n        try:\n            hd = medpy_binary.hd95(pred_bin, gt_bin)\n        except Exception:\n            hd = np.nan\n        hd_list.append(hd)\n    return np.nanmean(hd_list)\n\n\ndef compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach()\n    processed_preds = []\n    for i in range(pred_probs.size(0)):\n        pred_np = pred_probs[i].cpu().numpy()[0]\n        pred_bin = (pred_np > threshold).astype(np.uint8)\n        processed = postprocess_mask(pred_bin, min_size=100)\n        processed_preds.append(processed)\n    batch_iou = []\n    batch_f1 = []\n    batch_precision = []\n    batch_recall = []\n    for i in range(pred_probs.size(0)):\n        pred = processed_preds[i]\n        gt = masks[i].cpu().numpy()[0]\n        intersection = np.sum(pred * gt)\n        union = np.sum(pred) + np.sum(gt) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        batch_iou.append(iou)\n        precision = intersection / (np.sum(pred) + smooth)\n        recall = intersection / (np.sum(gt) + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        batch_f1.append(f1)\n        batch_precision.append(precision)\n        batch_recall.append(recall)\n    probs = pred_probs.cpu().numpy().flatten()\n    masks_np = masks.cpu().numpy().flatten()\n    try:\n        auc_score = roc_auc_score(masks_np, probs)\n    except ValueError:\n        auc_score = float('nan')\n    return auc_score, np.mean(batch_iou), np.mean(batch_f1), np.mean(batch_precision), np.mean(batch_recall)\n\ndef find_optimal_threshold_iou(model, dataloader, device, smooth=1e-6):\n    thresholds = np.linspace(0.1, 0.9, 17)  # 0.05 간격 세밀한 탐색\n    best_threshold = 0.65\n    best_iou = 0.0\n    model.eval()\n    with torch.no_grad():\n        for t in thresholds:\n            total_iou = 0.0\n            sample_count = 0\n            for images, masks, _ in dataloader:\n                images = images.to(device)\n                masks = masks.to(device)\n                outputs = model(images)\n                pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n                masks_np = masks.cpu().numpy()\n                for i in range(outputs.size(0)):\n                    pred_bin = (pred_probs[i, 0] > t).astype(np.uint8)\n                    pred_processed = postprocess_mask(pred_bin, min_size=100)\n                    gt = masks_np[i, 0]\n                    intersection = np.sum(pred_processed * gt)\n                    union = np.sum(pred_processed) + np.sum(gt) - intersection\n                    iou_val = (intersection + smooth) / (union + smooth)\n                    total_iou += iou_val\n                    sample_count += 1\n            avg_iou = total_iou / sample_count if sample_count > 0 else 0\n            if avg_iou > best_iou:\n                best_iou = avg_iou\n                best_threshold = t\n    return best_threshold, best_iou\n\ndef evaluate_with_metrics(model, dataloader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None):\n    model.eval()\n    total_loss = 0.0\n    total_auc  = 0.0\n    total_iou  = 0.0\n    total_f1   = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    total_hd = 0.0  # HD 합계\n    total_samples = 0\n    with torch.no_grad():\n        for images, masks, _ in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=lambda_boundary)\n            auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=threshold, smooth=1e-6)\n            hd_val = compute_hd_metric(outputs, masks, threshold=threshold)\n            bs = images.size(0)\n            total_loss += loss.item() * bs\n            total_auc  += auc_score * bs\n            total_iou  += iou_val * bs\n            total_f1   += f1_val * bs\n            total_precision += prec * bs\n            total_recall += rec * bs\n            total_hd += hd_val * bs\n            total_samples += bs\n    avg_loss = total_loss / total_samples\n    avg_auc  = total_auc / total_samples\n    avg_iou  = total_iou / total_samples\n    avg_f1   = total_f1 / total_samples\n    avg_precision = total_precision / total_samples\n    avg_recall = total_recall / total_samples\n    avg_hd = total_hd / total_samples\n    return avg_loss, avg_auc, avg_iou, avg_f1, avg_precision, avg_recall, avg_hd\n\n\n###############################################\n# Hausdorff Distance (HD) 계산 함수\n###############################################\ndef hausdorff_distance(pred, target):\n    # pred, target: numpy binary masks\n    return medpy_binary.hd95(pred, target)\n\n###############################################\n# TTA (Test Time Augmentation) 함수\n###############################################\ndef tta_predict(model, image, device):\n    # image: single image tensor, shape (C, H, W)\n    def identity(x): return x\n    def hflip(x): return torch.flip(x, dims=[-1])\n    def vflip(x): return torch.flip(x, dims=[-2])\n    def rot90(x): return torch.rot90(x, k=1, dims=[-2, -1])\n    # 역변환 함수: flip은 동일한 연산으로 복구, rot90의 경우 3회 회전하면 원상복구\n    def inv_hflip(x): return torch.flip(x, dims=[-1])\n    def inv_vflip(x): return torch.flip(x, dims=[-2])\n    def inv_rot90(x): return torch.rot90(x, k=3, dims=[-2, -1])\n    \n    transforms_list = [\n        (identity, identity),\n        (hflip, inv_hflip),\n        (vflip, inv_vflip),\n        (rot90, inv_rot90)\n    ]\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for aug, inv in transforms_list:\n            augmented = aug(image)\n            output = model(augmented.unsqueeze(0).to(device))\n            output = torch.sigmoid(output)\n            output = inv(output).cpu()\n            predictions.append(output)\n    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n    return avg_prediction\n\n###############################################\n# Optimizer Scheduler 함수\n###############################################\ndef get_lambda_rl(epoch):\n    return min(0.1, epoch / 100.0)\n\ndef get_lambda_entropy(epoch):\n    return max(0.01, 0.1 - (epoch / 100.0))\n\nclass WarmupCosineScheduler:\n    def __init__(self, optimizer, warmup_steps, max_steps, max_lr, min_lr=1e-6):\n        self.optimizer = optimizer\n        self.warmup_steps = warmup_steps\n        self.max_steps = max_steps\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.global_step = 0\n\n    def step(self):\n        self.global_step += 1\n        if self.global_step < self.warmup_steps:\n            lr = self.max_lr * (self.global_step / self.warmup_steps)\n        else:\n            progress = (self.global_step - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n            lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + math.cos(math.pi * progress))\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n\n###############################################\n# 데이터 로더 및 BUSI Dataset 설정\n###############################################\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nfull_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\nindices = np.arange(len(full_dataset))\ntrain_val_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\ntrain_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=42)\ntrain_dataset = Subset(full_dataset, train_idx)\nval_dataset   = Subset(full_dataset, val_idx)\ntest_dataset  = Subset(full_dataset, test_idx)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=16, shuffle=True, num_workers=4,\n    worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id),\n    pin_memory=True, persistent_workers=True\n)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\n\n###############################################\n# 모델, Optimizer, Scheduler 설정\n###############################################\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = DARKViT_UNet(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\ndef calculate_pos_weight(loader):\n    total_pixels = 0\n    positive_pixels = 0\n    for images, masks, _ in loader:\n        positive_pixels += masks.sum().item()\n        total_pixels += masks.numel()\n    negative_pixels = total_pixels - positive_pixels\n    return torch.tensor(negative_pixels / (positive_pixels + 1e-6)).to(device)\n\npos_weight = calculate_pos_weight(train_loader)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nnum_epochs = 500\n\nscheduler = WarmupCosineScheduler(\n    optimizer, warmup_steps=10, max_steps=num_epochs,\n    max_lr=1e-4, min_lr=1e-5\n)\n\n###############################################\n# Training Loop\n###############################################\nfrom torch.cuda.amp import GradScaler, autocast\nscaler = GradScaler()\npatience = 20\nbest_val_f1 = 0.0\npatience_counter = 0\n\nfor epoch in range(num_epochs):\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    current_lambda_rl = get_lambda_rl(epoch)\n    current_lambda_entropy = get_lambda_entropy(epoch)\n\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        # CutMix 활성화 (alpha=1.5, p=0.7)\n        images, masks = cutmix_data(images, masks, alpha=1.5, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss_seg = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=0.2, lambda_lovasz=0.6)\n            policy_loss = torch.tensor(0.0, device=loss_seg.device)  # RL 미사용\n            loss_total = loss_seg + current_lambda_rl * policy_loss\n        scaler.scale(loss_total).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        bs = images.size(0)\n        epoch_loss += loss_total.item() * bs\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step()\n    avg_loss = epoch_loss / total_samples\n    avg_auc = epoch_auc / total_samples\n    avg_iou = epoch_iou / total_samples\n    avg_f1 = epoch_f1 / total_samples\n    avg_precision = epoch_precision / total_samples\n    avg_recall = epoch_recall / total_samples\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n\n    \n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model.pth')\n        patience_counter = 0\n        print(\"🍀 New best validation Dice achieved! Dice: {:.4f} 🍀\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T07:55:04.347638Z","iopub.execute_input":"2025-03-15T07:55:04.347913Z","iopub.status.idle":"2025-03-15T07:55:24.081646Z","shell.execute_reply.started":"2025-03-15T07:55:04.347891Z","shell.execute_reply":"2025-03-15T07:55:24.080443Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/788M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55eb104de88541fa8b011108cc884012"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-0e5fde135e12>\u001b[0m in \u001b[0;36m<cell line: 819>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;31m###############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDARKViT_UNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-0e5fde135e12>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, out_channels)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_bottleneck1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerBottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_bottleneck2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerBottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNetDecoder_Swin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'UNetDecoder_Swin' is not defined"],"ename":"NameError","evalue":"name 'UNetDecoder_Swin' is not defined","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"test_loss, test_auc, test_iou, test_f1, test_precision, test_recall, test_hd = evaluate_with_metrics(model, test_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\nprint(f\"[Test] Loss: {test_loss:.4f}, AUC: {test_auc:.4f}, IoU: {test_iou:.4f}, Dice/F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, HD95: {test_hd:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T03:49:51.938392Z","iopub.execute_input":"2025-03-14T03:49:51.938690Z","iopub.status.idle":"2025-03-14T03:49:59.689957Z","shell.execute_reply.started":"2025-03-14T03:49:51.938665Z","shell.execute_reply":"2025-03-14T03:49:59.688693Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"[Test] Loss: 1.0922, AUC: 0.9580, IoU: 0.6721, Dice/F1: 0.6004, Precision: 0.6322, Recall: 0.6067, HD95: 23.9212\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# DARK-Swin","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport copy\nimport glob\nimport random\nimport warnings\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport cv2  # CLAHE 적용을 위해 OpenCV 사용\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage.segmentation import find_boundaries\nfrom skimage.measure import label, regionprops\nimport scipy.ndimage\n\n# Albumentations 기반 augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# medpy HD metric\nfrom medpy.metric import binary as medpy_binary\nfrom torchvision.transforms import ColorJitter\n\n###############################################\n# Seed 및 Warning 설정\n###############################################\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\nwarnings.filterwarnings('ignore')\n\n###############################################\n# BUSI Segmentation Dataset\n###############################################\nclass BUSISegmentationDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform  \n        self.samples = []  # 반드시 초기화\n        self._prepare_samples()\n\n    def _prepare_samples(self):\n        labels = os.listdir(self.data_path)\n        for label in labels:\n            folder_path = os.path.join(self.data_path, label)\n            if not os.path.isdir(folder_path):\n                continue\n            files = os.listdir(folder_path)\n            image_files = sorted([f for f in files if '_mask' not in f and f.endswith('.png')])\n            mask_files  = sorted([f for f in files if '_mask' in f and f.endswith('.png')])\n            pattern_img = re.compile(rf'{re.escape(label)} \\((\\d+)\\)\\.png')\n            pattern_mask = re.compile(rf'{re.escape(label)} \\((\\d+)\\)_mask(?:_\\d+)?\\.png')\n            mask_dict = {}\n            for mf in mask_files:\n                m = pattern_mask.fullmatch(mf)\n                if m:\n                    idx = m.group(1)\n                    mask_dict.setdefault(idx, []).append(mf)\n            for im in image_files:\n                m = pattern_img.fullmatch(im)\n                if m:\n                    idx = m.group(1)\n                    img_path = os.path.join(folder_path, im)\n                    if idx in mask_dict:\n                        mask_paths = [os.path.join(folder_path, mf) for mf in mask_dict[idx]]\n                        combined_mask = None\n                        for mp in mask_paths:\n                            mask_img = Image.open(mp).convert('L')\n                            mask_arr = np.array(mask_img)\n                            mask_binary = (mask_arr > 128).astype(np.uint8)\n                            if combined_mask is None:\n                                combined_mask = mask_binary\n                            else:\n                                combined_mask = np.maximum(combined_mask, mask_binary)\n                        self.samples.append((img_path, combined_mask, label))\n                    else:\n                        image_pil = Image.open(img_path)\n                        empty_mask = np.zeros(image_pil.size[::-1], dtype=np.uint8)\n                        self.samples.append((img_path, empty_mask, label))\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, mask_array, label = self.samples[index]\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.fromarray((mask_array * 255).astype(np.uint8))\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        return image, mask, label\n\n###############################################\n# Per-image Z-score normalization (adaptive)\n###############################################\ndef z_score_normalize(tensor):\n    mean = tensor.mean()\n    std = tensor.std() + 1e-6\n    return (tensor - mean) / std\n\n###############################################\n# CLAHE Augmentation 함수 (OpenCV)\n###############################################\ndef apply_clahe(image, clipLimit=2.0, tileGridSize=(8,8)):\n    img_np = np.array(image)\n    if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl, a, b))\n        final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    else:\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        final = clahe.apply(img_np)\n    return Image.fromarray(final)\n\n###############################################\n# joint_transform (Albumentations 기반 Augmentation)\n###############################################\ndef joint_transform(image, mask, size=(224,224)):\n    geom_transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=10, p=0.5),\n        A.ElasticTransform(alpha=10, sigma=5, alpha_affine=5, p=0.3),\n        A.Resize(height=size[0], width=size[1])\n    ])\n    image_np = np.array(image)\n    mask_np = np.array(mask)\n    augmented = geom_transform(image=image_np, mask=mask_np)\n    image = augmented['image']\n    mask = augmented['mask']\n    \n    intensity_transform = A.Compose([\n        A.RandomBrightnessContrast(p=0.5),\n        #A.CLAHE(clip_limit=3.0, p=0.5),\n        A.CLAHE(clip_limit=1.0, tile_grid_size=(8,8), p=0.5),\n        A.GaussianBlur(p=0.3)\n    ])\n    image = intensity_transform(image=image)['image']\n    \n    image = transforms.ToTensor()(image)\n    image = z_score_normalize(image)\n    mask = transforms.ToTensor()(mask)\n    return image, mask\n\n###############################################\n# CutMix 함수 (alpha=1.5, p=0.7)\n###############################################\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(images, masks, alpha=1.5, p=0.7):\n    if np.random.rand() > p:\n        return images, masks\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(images.size(0)).to(images.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n    masks[:, :, bbx1:bbx2, bby1:bby2] = masks[rand_index, :, bbx1:bbx2, bby1:bby2]\n    return images, masks\n\n###############################################\n# Advanced 후처리: Morphological Closing (Kernel 9×9)\n###############################################\ndef postprocess_mask(mask, min_size=100):\n    labeled_mask = label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n###############################################\n# AttentionGate 모듈\n###############################################\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(AttentionGate, self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi\n\n###############################################\n# MultiScaleFusion 모듈\n###############################################\nclass MultiScaleFusion(nn.Module):\n    def __init__(self, in_channels_list, out_channels):\n        super(MultiScaleFusion, self).__init__()\n        self.convs = nn.ModuleList([nn.Conv2d(ch, out_channels, kernel_size=1) for ch in in_channels_list])\n        self.fuse = nn.Sequential(\n            nn.Conv2d(len(in_channels_list) * out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, features):\n        target_size = features[-1].size()[2:]\n        processed = []\n        for i, f in enumerate(features):\n            if f.size()[2:] != target_size:\n                f = F.interpolate(f, size=target_size, mode='bilinear', align_corners=False)\n            f = self.convs[i](f)\n            processed.append(f)\n        out = torch.cat(processed, dim=1)\n        out = self.fuse(out)\n        return out\n\n###############################################\n# 기타 모델 관련 모듈 (StochasticDepth, GhostModule, SELayer, DynamicGating)\n###############################################\nclass StochasticDepth(nn.Module):\n    def __init__(self, p, mode=\"row\"):\n        super(StochasticDepth, self).__init__()\n        self.p = p\n        self.mode = mode\n\n    def forward(self, x):\n        if not self.training or self.p == 0.0:\n            return x\n        survival_rate = 1 - self.p\n        if self.mode == \"row\":\n            batch_size = x.shape[0]\n            noise = torch.rand(batch_size, 1, 1, 1, device=x.device, dtype=x.dtype)\n            binary_mask = (noise < survival_rate).float()\n            return x / survival_rate * binary_mask\n        else:\n            if torch.rand(1).item() < self.p:\n                return torch.zeros_like(x)\n            else:\n                return x\n\nclass GhostModule(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, ratio=2, dw_kernel_size=3, stride=1, padding=0, use_relu=True):\n        super(GhostModule, self).__init__()\n        self.out_channels = out_channels\n        self.primary_channels = int(torch.ceil(torch.tensor(out_channels / ratio)))\n        self.cheap_channels = out_channels - self.primary_channels\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(in_channels, self.primary_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(self.primary_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n        self.cheap_conv = nn.Sequential(\n            nn.Conv2d(self.primary_channels, self.cheap_channels, dw_kernel_size, stride=1,\n                      padding=dw_kernel_size // 2, groups=self.primary_channels, bias=False),\n            nn.BatchNorm2d(self.cheap_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_conv(x1)\n        out = torch.cat([x1, x2], dim=1)\n        return out[:, :self.out_channels, :, :].contiguous()\n\ndef ghost_conv_block(in_channels, out_channels, use_relu=True):\n    return nn.Sequential(\n        GhostModule(in_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu),\n        GhostModule(out_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu)\n    )\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\nclass DynamicGating(nn.Module):\n    def __init__(self, num_branches, hidden_dim=64, dropout_prob=0.1, init_temperature=2.0, iterations=3):\n        super(DynamicGating, self).__init__()\n        self.temperature = init_temperature\n        self.iterations = iterations\n        self.dropout_prob = dropout_prob\n        self.fc = nn.Sequential(\n            nn.Linear(num_branches, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, num_branches)\n        )\n        self.layernorm = nn.LayerNorm(num_branches)\n        self.softmax = nn.Softmax(dim=1)\n        self.res_scale = nn.Parameter(torch.ones(1))\n        self.saved_log_probs = None\n        self.entropy = None\n        self.rl_loss = 0.0\n\n    def forward(self, features):\n        norm_features = self.layernorm(features)\n        logits = self.fc(norm_features)\n        logits = self.layernorm(logits)\n        scaled_logits = logits / self.temperature\n        gates = self.softmax(scaled_logits)\n        for _ in range(self.iterations - 1):\n            updated_features = norm_features + self.res_scale * gates\n            logits = self.fc(updated_features)\n            logits = self.layernorm(logits)\n            scaled_logits = logits / self.temperature\n            new_gates = self.softmax(scaled_logits)\n            gates = 0.5 * gates + 0.5 * new_gates\n        return gates\n\n###############################################\n# QuadAgentBlock (Lite 버전, 3 브랜치)\n###############################################\nclass QuadAgentBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5):\n        super().__init__()\n        branch_channels = out_channels // 4\n        self.branch1 = GhostModule(in_channels, branch_channels, ratio=4)\n        self.branch2 = GhostModule(in_channels, branch_channels, kernel_size=3, padding=1, ratio=4)\n        self.branch3 = nn.Sequential(\n            GhostModule(in_channels, branch_channels, ratio=4),\n            SELayer(branch_channels, reduction=32)\n        )\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.gating = DynamicGating(num_branches=3, hidden_dim=gating_hidden_dim, dropout_prob=gating_dropout, init_temperature=gating_temperature)\n        self.fusion_conv = GhostModule(branch_channels * 3, out_channels, ratio=2)\n        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False) if in_channels != out_channels else nn.Identity()\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob)\n\n    def forward(self, x):\n        b1 = self.branch1(x)\n        b2 = self.branch2(x)\n        b3 = self.branch3(x)\n        gap_b1 = self.gap(b1).view(x.size(0), -1)\n        gap_b2 = self.gap(b2).view(x.size(0), -1)\n        gap_b3 = self.gap(b3).view(x.size(0), -1)\n        features = torch.stack([gap_b1.mean(dim=1), gap_b2.mean(dim=1), gap_b3.mean(dim=1)], dim=1)\n        gates = self.gating(features)\n        b1 = b1 * gates[:, 0].view(-1, 1, 1, 1)\n        b2 = b2 * gates[:, 1].view(-1, 1, 1, 1)\n        b3 = b3 * gates[:, 2].view(-1, 1, 1, 1)\n        out = torch.cat([b1, b2, b3], dim=1)\n        out = self.fusion_conv(out)\n        res = self.res_conv(x)\n        res = self.stochastic_depth(res)\n        return out + res\n\n\n###############################################\n# DARKViT_UNet: ViT 기반 Encoder + QuadAgentBlock + Transformer Bottleneck + UNet Decoder\n###############################################\nfrom timm.models import create_model\n\nclass DARKViT_UNet(nn.Module):\n    def __init__(self, out_channels=1):\n        super(DARKViT_UNet, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True)\n        self.bottleneck = QuadAgentBlock(in_channels=1536, out_channels=1536, \n                                         gating_dropout=0.3, gating_hidden_dim=32, \n                                         gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, \n                                                             dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, \n                                                             dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n    \n    def forward(self, x):\n        # encoder에서 feature map 리스트 얻기: [f0, f1, f2, f3]\n        features = self.encoder(x)\n        \n        # 만약 각 feature map이 (N, H, W, C) 형식이라면 (N, C, H, W)로 변환\n        for i in range(len(features)):\n            # f0: 192, f1:384, f2:768, f3:1536 채널이어야 함\n            if features[i].shape[1] not in {192, 384, 768, 1536}:\n                features[i] = features[i].permute(0, 3, 1, 2).contiguous()\n        \n        # 반환된 순서는 [f0, f1, f2, f3]이므로, decoder가 기대하는 [f3, f2, f1, f0] 순서로 재정렬\n        f0, f1, f2, f3 = features  \n        features_reordered = [f3, f2, f1, f0]\n        \n        # 이제 features_reordered[0]는 f3: (B,1536,7,7)\n        b_out = self.bottleneck(features_reordered[0])\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        features_reordered[0] = t_out2\n        \n        decoded = self.decoder(features_reordered)\n        return self.final_conv(decoded)\n\n\n\n###############################################\n# UNet Decoder for Swin Backbone\n###############################################\nclass UNetDecoder_Swin(nn.Module):\n    def __init__(self):\n        super(UNetDecoder_Swin, self).__init__()\n        # Swin‑Large feature sizes (가정):\n        # f0: (B,192,56,56), f1: (B,384,28,28), f2: (B,768,14,14), f3: (B,1536,7,7)\n        self.up1 = nn.ConvTranspose2d(1536, 768, kernel_size=2, stride=2)  # 7->14\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(768 + 768, 768, kernel_size=3, padding=1),\n            nn.BatchNorm2d(768),\n            nn.ReLU(inplace=True)\n        )\n        self.up2 = nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2)   # 14->28\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(384 + 384, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True)\n        )\n        self.up3 = nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2)   # 28->56\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(192 + 192, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n        self.up4 = nn.ConvTranspose2d(192, 64, kernel_size=2, stride=2)    # 56->112\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, features_list):\n        # features_list: [f0, f1, f2, f3]\n        f3, f2, f1, f0 = features_list  # ✅ 순서 맞추기\n        x = f3  # (B,1536,7,7)\n        x = self.up1(x)  # -> (B,768,14,14)\n        x = torch.cat([x, f2], dim=1)  # f2: (B,768,14,14) → (B,1536,14,14)\n        x = self.conv1(x)  # -> (B,768,14,14)\n        x = self.up2(x)  # -> (B,384,28,28)\n        x = torch.cat([x, f1], dim=1)  # f1: (B,384,28,28) → (B,768,28,28)\n        x = self.conv2(x)  # -> (B,384,28,28)\n        x = self.up3(x)  # -> (B,192,56,56)\n        x = torch.cat([x, f0], dim=1)  # f0: (B,192,56,56) → (B,384,56,56)\n        x = self.conv3(x)  # -> (B,192,56,56)\n        x = self.up4(x)  # -> (B,64,112,112)\n        x = self.conv4(x)  # -> (B,64,112,112)\n        x = F.interpolate(x, size=(224,224), mode='bilinear', align_corners=False)\n        return x\n\n###############################################\n# Transformer Bottleneck (연속 두 개 적용)\n###############################################\nclass TransformerBottleneck(nn.Module):\n    def __init__(self, d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1):\n        super(TransformerBottleneck, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                    dim_feedforward=dim_feedforward, dropout=dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    def forward(self, x):\n        B, C, H, W = x.size()\n        x = x.view(B, C, H * W).permute(2, 0, 1)\n        x = self.transformer(x)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\n###############################################\n# Swin‑Transformer Backbone 기반 UNet with Attention + Lite Bottleneck\n###############################################\nfrom timm.models import create_model\n\nclass PretrainedSwin_UNet_AttentionFusion(nn.Module):\n    def __init__(self, out_channels=1):\n        super(PretrainedSwin_UNet_AttentionFusion, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True)\n        self.bottleneck = QuadAgentBlock(in_channels=1536, out_channels=1536, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        features_list = self.encoder(x)  # [f0, f1, f2, f3]\n        \n        # 디버깅: Feature Map Shape 확인\n        #print(\"Original feature shapes:\")\n        #for i, f in enumerate(features_list):\n        #    print(f\"f{i}: {f.shape}\")\n\n        # (N, H, W, C) → (N, C, H, W) 변환이 필요한 경우 적용\n        for i in range(len(features_list)):\n            if features_list[i].shape[1] not in {192, 384, 768, 1536}:  \n                features_list[i] = features_list[i].permute(0, 3, 1, 2).contiguous()\n\n        # 디코더에 올바른 순서로 전달되도록 변환\n        f0, f1, f2, f3 = features_list  # Swin Transformer의 기본 반환 순서\n        features_list = [f3, f2, f1, f0]  # (B, 1536, 7, 7) → (B, 192, 56, 56)\n\n        # 디버깅: 변환 후 Feature Map Shape 확인\n        #print(\"\\nReordered feature shapes (for decoder):\")\n        #for i, f in enumerate(features_list):\n        #    print(f\"features_list[{i}]: {f.shape}\")\n\n        # Bottleneck과 Transformer Bottleneck 적용\n        b_out = self.bottleneck(features_list[0])  # f3가 들어가야 함\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        features_list[0] = t_out2  # Bottleneck을 거친 f3\n\n        # UNet 디코더 적용\n        decoded = self.decoder(features_list)\n        return self.final_conv(decoded)\n\n\n\n\n###############################################\n# Loss & Metric Functions (HD 포함)\n###############################################\nfrom segmentation_models_pytorch.losses import LovaszLoss\nlv_loss = LovaszLoss(mode='binary')\n\ndef focal_tversky_loss(pred, target, alpha=0.5, beta=0.5, gamma=4/3, smooth=1e-6):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred = pred.view(pred.size(0), -1)\n    target = target.view(target.size(0), -1)\n    tp = (pred * target).sum(dim=1)\n    fp = ((1 - target) * pred).sum(dim=1)\n    fn = (target * (1 - pred)).sum(dim=1)\n    tversky_index = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n    loss = (1 - tversky_index) ** gamma\n    return loss.mean()\n\ndef boundary_loss(pred, target):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_np = pred.detach().cpu().numpy()\n    target_np = target.detach().cpu().numpy()\n    boundary_masks = []\n    for i in range(pred_np.shape[0]):\n        gt_mask = target_np[i, 0]\n        boundary = find_boundaries(gt_mask, mode='thick')\n        boundary_masks.append(boundary.astype(np.float32))\n    boundary_masks = np.stack(boundary_masks, axis=0)[:, None, :, :]\n    boundary_masks_torch = torch.from_numpy(boundary_masks).to(pred.device)\n    intersect = (pred * boundary_masks_torch).sum()\n    denom = pred.sum() + boundary_masks_torch.sum()\n    boundary_dice = (2.0 * intersect) / (denom + 1e-6)\n    return 1.0 - boundary_dice\n\ndef combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    loss_ft = focal_tversky_loss(outputs, masks)\n    if pos_weight is not None:\n        loss_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, masks.float())\n    else:\n        loss_bce = nn.BCEWithLogitsLoss()(outputs, masks.float())\n    bl = boundary_loss(outputs, masks)\n    lv = lv_loss(outputs, masks)\n    return 0.4 * lv + 0.3 * loss_ft + 0.3 * loss_bce + lambda_boundary * bl\n\ndef dice_f1_precision_recall(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    precision = intersection / (pred_bin.sum() + smooth)\n    recall = intersection / (target_bin.sum() + smooth)\n    f1 = 2 * (precision * recall) / (precision + recall + smooth)\n    return f1.item(), precision.item(), recall.item()\n\ndef iou_metric(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    union = pred_bin.sum() + target_bin.sum() - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach()\n    processed_preds = []\n    for i in range(pred_probs.size(0)):\n        pred_np = pred_probs[i].cpu().numpy()[0]\n        pred_bin = (pred_np > threshold).astype(np.uint8)\n        processed = postprocess_mask(pred_bin, min_size=100)\n        processed_preds.append(processed)\n    batch_iou = []\n    batch_f1 = []\n    batch_precision = []\n    batch_recall = []\n    for i in range(pred_probs.size(0)):\n        pred = processed_preds[i]\n        gt = masks[i].cpu().numpy()[0]\n        intersection = np.sum(pred * gt)\n        union = np.sum(pred) + np.sum(gt) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        batch_iou.append(iou)\n        precision = intersection / (np.sum(pred) + smooth)\n        recall = intersection / (np.sum(gt) + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        batch_f1.append(f1)\n        batch_precision.append(precision)\n        batch_recall.append(recall)\n    probs = pred_probs.cpu().numpy().flatten()\n    masks_np = masks.cpu().numpy().flatten()\n    try:\n        auc_score = roc_auc_score(masks_np, probs)\n    except ValueError:\n        auc_score = float('nan')\n    return auc_score, np.mean(batch_iou), np.mean(batch_f1), np.mean(batch_precision), np.mean(batch_recall)\n\ndef hausdorff_distance(pred, target):\n    return medpy_binary.hd95(pred, target)\n\ndef compute_hd_metric(outputs, masks, threshold=0.5):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n    masks_np = masks.detach().cpu().numpy()\n    hd_list = []\n    for i in range(outputs.size(0)):\n        pred_bin = (pred_probs[i, 0] > threshold).astype(np.uint8)\n        gt_bin = (masks_np[i, 0] > 0.5).astype(np.uint8)\n        try:\n            hd = medpy_binary.hd95(pred_bin, gt_bin)\n        except Exception:\n            hd = np.nan\n        hd_list.append(hd)\n    return np.nanmean(hd_list)\n\ndef evaluate_with_metrics(model, dataloader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None):\n    model.eval()\n    total_loss = 0.0\n    total_auc  = 0.0\n    total_iou  = 0.0\n    total_f1   = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    total_hd = 0.0\n    total_samples = 0\n    with torch.no_grad():\n        for images, masks, _ in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=lambda_boundary)\n            auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=threshold, smooth=1e-6)\n            hd_val = compute_hd_metric(outputs, masks, threshold=threshold)\n            bs = images.size(0)\n            total_loss += loss.item() * bs\n            total_auc  += auc_score * bs\n            total_iou  += iou_val * bs\n            total_f1   += f1_val * bs\n            total_precision += prec * bs\n            total_recall += rec * bs\n            total_hd += hd_val * bs\n            total_samples += bs\n    avg_loss = total_loss / total_samples\n    avg_auc  = total_auc / total_samples\n    avg_iou  = total_iou / total_samples\n    avg_f1   = total_f1 / total_samples\n    avg_precision = total_precision / total_samples\n    avg_recall = total_recall / total_samples\n    avg_hd = total_hd / total_samples\n    return avg_loss, avg_auc, avg_iou, avg_f1, avg_precision, avg_recall, avg_hd\n\n###############################################\n# TTA (Test Time Augmentation) 함수 (Noise & ColorJitter 추가)\n###############################################\ndef tta_predict(model, image, device):\n    cj = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n    def add_noise(x, std=0.05):\n        noise = torch.randn_like(x) * std\n        return x + noise\n    def identity(x): return x\n    def hflip(x): return torch.flip(x, dims=[-1])\n    def vflip(x): return torch.flip(x, dims=[-2])\n    def rot90(x): return torch.rot90(x, k=1, dims=[-2, -1])\n    def inv_hflip(x): return torch.flip(x, dims=[-1])\n    def inv_vflip(x): return torch.flip(x, dims=[-2])\n    def inv_rot90(x): return torch.rot90(x, k=3, dims=[-2, -1])\n    \n    transforms_list = [\n        (identity, identity),\n        (hflip, inv_hflip),\n        (vflip, inv_vflip),\n        (rot90, inv_rot90),\n        (lambda x: add_noise(cj(x)), lambda x: x)\n    ]\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for aug, inv in transforms_list:\n            augmented = aug(image)\n            output = model(augmented.unsqueeze(0).to(device))\n            output = torch.sigmoid(output)\n            output = inv(output).cpu()\n            predictions.append(output)\n    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n    return avg_prediction\n\n###############################################\n# Optimizer Scheduler (SGDR: CosineAnnealingWarmRestarts)\n###############################################\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\n###############################################\n# 데이터 로더 및 BUSI Dataset 설정\n###############################################\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nfull_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\nindices = np.arange(len(full_dataset))\ntrain_val_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\ntrain_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=42)\ntrain_dataset = Subset(full_dataset, train_idx)\nval_dataset = Subset(full_dataset, val_idx)\ntest_dataset = Subset(full_dataset, test_idx)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=16, shuffle=True, num_workers=4,\n    worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id),\n    pin_memory=True, persistent_workers=True\n)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\n\ndef calculate_pos_weight(loader):\n    total_pixels = 0\n    positive_pixels = 0\n    for images, masks, _ in loader:\n        positive_pixels += masks.sum().item()\n        total_pixels += masks.numel()\n    negative_pixels = total_pixels - positive_pixels\n    return torch.tensor(negative_pixels / (positive_pixels + 1e-6)).to(device)\n\n###############################################\n# 모델, Optimizer, Scheduler 설정\n###############################################\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\npos_weight = calculate_pos_weight(train_loader)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nnum_epochs = 500\n\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n\n\"\"\"###############################################\n# Training Loop (Early Stopping patience=50)\n###############################################\nfrom torch.cuda.amp import GradScaler, autocast\nscaler = GradScaler()\npatience = 15\nbest_val_f1 = 0.0\npatience_counter = 0\n\nfor epoch in range(num_epochs):\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss_seg = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=0.2, lambda_lovasz=0.6)\n            loss_total = loss_seg  # RL 미사용\n        scaler.scale(loss_total).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        bs = images.size(0)\n        epoch_loss += loss_total.item() * bs\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples\n    avg_auc = epoch_auc / total_samples\n    avg_iou = epoch_iou / total_samples\n    avg_f1 = epoch_f1 / total_samples\n    avg_precision = epoch_precision / total_samples\n    avg_recall = epoch_recall / total_samples\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n    \n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model.pth')\n        patience_counter = 0\n        print(\"🍀 New best validation Dice achieved! Dice: {:.4f} 🍀\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T07:55:38.941587Z","iopub.execute_input":"2025-03-15T07:55:38.941935Z","iopub.status.idle":"2025-03-15T07:55:52.346672Z","shell.execute_reply.started":"2025-03-15T07:55:38.941911Z","shell.execute_reply":"2025-03-15T07:55:52.345665Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'###############################################\\n# Training Loop (Early Stopping patience=50)\\n###############################################\\nfrom torch.cuda.amp import GradScaler, autocast\\nscaler = GradScaler()\\npatience = 15\\nbest_val_f1 = 0.0\\npatience_counter = 0\\n\\nfor epoch in range(num_epochs):\\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\\n    if hasattr(model, \\'bottleneck\\'):\\n        if isinstance(model, nn.DataParallel):\\n            model.module.bottleneck.gating.temperature = current_temp\\n        else:\\n            model.bottleneck.gating.temperature = current_temp\\n\\n    model.train()\\n    epoch_loss = 0.0\\n    epoch_auc = 0.0\\n    epoch_iou = 0.0\\n    epoch_f1 = 0.0\\n    epoch_precision = 0.0\\n    epoch_recall = 0.0\\n    total_samples = 0\\n\\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\\n        images = images.to(device)\\n        masks = masks.to(device)\\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\\n        optimizer.zero_grad()\\n        with autocast():\\n            outputs = model(images)\\n            loss_seg = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=0.2, lambda_lovasz=0.6)\\n            loss_total = loss_seg  # RL 미사용\\n        scaler.scale(loss_total).backward()\\n        scaler.unscale_(optimizer)\\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n        scaler.step(optimizer)\\n        scaler.update()\\n        bs = images.size(0)\\n        epoch_loss += loss_total.item() * bs\\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\\n        epoch_auc += auc_score * bs\\n        epoch_iou += iou_val * bs\\n        epoch_f1 += f1_val * bs\\n        epoch_precision += prec * bs\\n        epoch_recall += rec * bs\\n        total_samples += bs\\n\\n    scheduler.step(epoch)\\n    avg_loss = epoch_loss / total_samples\\n    avg_auc = epoch_auc / total_samples\\n    avg_iou = epoch_iou / total_samples\\n    avg_f1 = epoch_f1 / total_samples\\n    avg_precision = epoch_precision / total_samples\\n    avg_recall = epoch_recall / total_samples\\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\\n    \\n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\\n    \\n    if val_f1 > best_val_f1:\\n        best_val_f1 = val_f1\\n        best_weights = copy.deepcopy(model.state_dict())\\n        torch.save(best_weights, \\'best_model.pth\\')\\n        patience_counter = 0\\n        print(\"🍀 New best validation Dice achieved! Dice: {:.4f} 🍀\".format(best_val_f1))\\n    else:\\n        patience_counter += 1\\n        if patience_counter >= patience:\\n            print(f\"Early stopping at epoch {epoch+1}\")\\n            break\\n'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test_loss, test_auc, test_iou, test_f1, test_precision, test_recall, test_hd = evaluate_with_metrics(model, test_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\nprint(f\"[Test] Loss: {test_loss:.4f}, AUC: {test_auc:.4f}, IoU: {test_iou:.4f}, Dice/F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, HD95: {test_hd:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T07:12:15.215514Z","iopub.execute_input":"2025-03-14T07:12:15.215825Z","iopub.status.idle":"2025-03-14T07:12:24.460828Z","shell.execute_reply.started":"2025-03-14T07:12:15.215800Z","shell.execute_reply":"2025-03-14T07:12:24.459431Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"[Test] Loss: 1.0127, AUC: 0.9585, IoU: 0.7442, Dice/F1: 0.6459, Precision: 0.6761, Recall: 0.6465, HD95: 20.2945\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### 🔍 **SelectiveGatingBlock과 유사한 네이밍 아이디어**\nSelective Gating의 개념을 반영하면서도 고유성을 유지할 수 있는 새로운 이름들을 제안해볼게요.\n\n---\n## 🚀 **새로운 네이밍 후보**\n| **이름**                         | **설명** |\n|----------------------------------|------------------------------------------------------------|\n| **SelectiveGateBlock**           | `SelectiveGatingBlock`을 더 간결하게 표현 |\n| **AdaptiveSelectionGate**        | 선택적(Gating) & 적응적(Adaptive) 요소를 반영 |\n| **FilteredGatingBlock**          | 불필요한 Feature를 \"걸러내는(Filter)\" 개념을 반영 |\n| **DynamicSelectionGate**         | 동적으로 선택하는 Gating을 강조 |\n| **SmartSelectiveGate**           | \"스마트한 Feature 선택\"을 강조 |\n| **FeatureSelectionGate**         | Feature Selection을 직접적으로 표현 |\n| **GatedSelectionModule (GSM)**   | Selection 개념과 Gating을 모듈로 조합 (논문에 적절) |\n| **S-GateBlock**                  | \"Selective\"의 약자로 줄인 형태 (코드에서 직관적) |\n| **ActiveSelectionGate**          | \"Active Selection\"을 통해 Feature를 강조하는 느낌 |\n| **PrioritizedGatingBlock**       | 중요한 Feature를 우선적으로 반영하는 개념 |\n\n---\n## 🔥 **최종 추천**\n✅ **`SelectiveGateBlock`** → 기존 이름에서 더 직관적으로 축약  \n✅ **`DynamicSelectionGate`** → 동적 선택이라는 핵심 개념을 강조  \n✅ **`GatedSelectionModule (GSM)`** → 논문에서 사용하기 좋은 고급스러운 네이밍  \n✅ **`PrioritizedGatingBlock`** → 중요한 Feature를 선별하는 개념을 포함  \n\n📌 **최종 의견:**  \n- **Selective Gating** 개념을 유지하려면 `SelectiveGateBlock`이 가장 직관적  \n- 좀 더 역동적인 느낌을 주고 싶다면 `DynamicSelectionGate` 추천  \n- 논문에서는 **GSM (GatedSelectionModule)** 같은 약어가 깔끔할 수도 있음!  \n\n> 👉 **\"SelectiveGatingBlock\" 대신 `SelectiveGateBlock`, `DynamicSelectionGate`, `GatedSelectionModule (GSM)`을 추천합니다!** 🚀\n\n\n\n### 📑 **논문에 적합한 새로운 블록 네이밍 10개 추천**  \n\n논문에서 사용할 수 있도록 **고급스럽고 기술적인 느낌을 주는 이름**을 선정했어!  \n\n---\n\n### **1️⃣ Feature Selection & Gating 기반**  \n1. **AdaptiveSelectiveGate (ASG)** → 선택적 Gating + 적응적 구조  \n2. **DynamicFeatureGating (DFG)** → Feature 선택을 동적으로 수행  \n3. **HierarchicalSelectionGate (HSG)** → 계층적으로 Feature를 선택하는 개념  \n4. **PriorityGatedFusion (PGF)** → 중요한 Feature를 우선적으로 선택 후 결합  \n5. **Context-Aware Selection Gate (CSG)** → 문맥(Context) 기반으로 선택적 활성화  \n\n---\n\n### **2️⃣ Gating & Fusion을 강조한 이름**  \n6. **EfficientSelectiveGating (ESG)** → 경량화와 선택적 Gating을 강조  \n7. **Multi-Scale Gated Fusion (MSGF)** → 다중 해상도 Feature의 선택적 융합  \n8. **Neural Adaptive Selection Gate (NASG)** → 신경망 기반 적응적 선택  \n9. **Attentive Gating Module (AGM)** → Attention과 Gating을 결합한 구조  \n10. **Selective Representation Learning Block (SRLB)** → 선택적 Feature 학습을 강조  \n\n---\n\n### ✅ **논문용 최종 추천 Top 3**  \n1. **AdaptiveSelectiveGate (ASG)** → 논문에 쓰기 직관적이면서 깔끔  \n2. **HierarchicalSelectionGate (HSG)** → 계층적 구조 강조 (논문에서 고급스럽게 보임)  \n3. **Neural Adaptive Selection Gate (NASG)** → 신경망 기반 적응적 선택 (고급 AI 모델 느낌)  \n\n📌 **특징**  \n- 약어를 포함해 논문에서 사용하기 용이  \n- 기존 `SelectiveGatingBlock`과 차별화되면서도 같은 개념을 포함  \n- 최신 연구 트렌드(Adaptive, Dynamic, Context-Aware 등) 반영  \n\n> 🚀 **\"논문용 네이밍\"으로 `AdaptiveSelectiveGate (ASG)`, `HierarchicalSelectionGate (HSG)`, `Neural Adaptive Selection Gate (NASG)` 추천!**","metadata":{}},{"cell_type":"markdown","source":"# 256x256","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport copy\nimport glob\nimport random\nimport warnings\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport cv2  # CLAHE 적용을 위해 OpenCV 사용\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage.segmentation import find_boundaries\nfrom skimage.measure import label, regionprops\nimport scipy.ndimage\n\n# Albumentations 기반 augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# medpy HD metric\nfrom medpy.metric import binary as medpy_binary\nfrom torchvision.transforms import ColorJitter\n\n###############################################\n# Seed 및 Warning 설정\n###############################################\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\nwarnings.filterwarnings('ignore')\n\n###############################################\n# BUSI Segmentation Dataset\n###############################################\nclass BUSISegmentationDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform  \n        self.samples = []  # 반드시 초기화\n        self._prepare_samples()\n\n    def _prepare_samples(self):\n        labels = os.listdir(self.data_path)\n        for label in labels:\n            folder_path = os.path.join(self.data_path, label)\n            if not os.path.isdir(folder_path):\n                continue\n            files = os.listdir(folder_path)\n            image_files = sorted([f for f in files if '_mask' not in f and f.endswith('.png')])\n            mask_files  = sorted([f for f in files if '_mask' in f and f.endswith('.png')])\n            pattern_img = re.compile(rf'{re.escape(label)} \\((\\d+)\\)\\.png')\n            pattern_mask = re.compile(rf'{re.escape(label)} \\((\\d+)\\)_mask(?:_\\d+)?\\.png')\n            mask_dict = {}\n            for mf in mask_files:\n                m = pattern_mask.fullmatch(mf)\n                if m:\n                    idx = m.group(1)\n                    mask_dict.setdefault(idx, []).append(mf)\n            for im in image_files:\n                m = pattern_img.fullmatch(im)\n                if m:\n                    idx = m.group(1)\n                    img_path = os.path.join(folder_path, im)\n                    if idx in mask_dict:\n                        mask_paths = [os.path.join(folder_path, mf) for mf in mask_dict[idx]]\n                        combined_mask = None\n                        for mp in mask_paths:\n                            mask_img = Image.open(mp).convert('L')\n                            mask_arr = np.array(mask_img)\n                            mask_binary = (mask_arr > 128).astype(np.uint8)\n                            if combined_mask is None:\n                                combined_mask = mask_binary\n                            else:\n                                combined_mask = np.maximum(combined_mask, mask_binary)\n                        self.samples.append((img_path, combined_mask, label))\n                    else:\n                        image_pil = Image.open(img_path)\n                        empty_mask = np.zeros(image_pil.size[::-1], dtype=np.uint8)\n                        self.samples.append((img_path, empty_mask, label))\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, mask_array, label = self.samples[index]\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.fromarray((mask_array * 255).astype(np.uint8))\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        return image, mask, label\n\n###############################################\n# Per-image Z-score normalization (adaptive)\n###############################################\ndef z_score_normalize(tensor):\n    mean = tensor.mean()\n    std = tensor.std() + 1e-6\n    return (tensor - mean) / std\n\n###############################################\n# CLAHE Augmentation 함수 (OpenCV)\n###############################################\ndef apply_clahe(image, clipLimit=2.0, tileGridSize=(8,8)):\n    img_np = np.array(image)\n    if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl, a, b))\n        final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    else:\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        final = clahe.apply(img_np)\n    return Image.fromarray(final)\n\n###############################################\n# joint_transform (Albumentations 기반 Augmentation)\n###############################################\ndef joint_transform(image, mask, size=(256,256)):  # 기본 size를 (256,256)으로 변경\n    geom_transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=10, p=0.5),\n        A.ElasticTransform(alpha=10, sigma=5, alpha_affine=5, p=0.3),\n        A.Resize(height=size[0], width=size[1])\n    ])\n    image_np = np.array(image)\n    mask_np = np.array(mask)\n    augmented = geom_transform(image=image_np, mask=mask_np)\n    image = augmented['image']\n    mask = augmented['mask']\n    \n    intensity_transform = A.Compose([\n        A.RandomBrightnessContrast(p=0.5),\n        #A.CLAHE(clip_limit=3.0, p=0.5),\n        A.CLAHE(clip_limit=1.0, tile_grid_size=(8,8), p=0.5),\n        A.GaussianBlur(p=0.3)\n    ])\n    image = intensity_transform(image=image)['image']\n    \n    image = transforms.ToTensor()(image)\n    image = z_score_normalize(image)\n    mask = transforms.ToTensor()(mask)\n    return image, mask\n\n###############################################\n# CutMix 함수 (alpha=1.5, p=0.7)\n###############################################\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(images, masks, alpha=1.5, p=0.7):\n    if np.random.rand() > p:\n        return images, masks\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(images.size(0)).to(images.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n    masks[:, :, bbx1:bbx2, bby1:bby2] = masks[rand_index, :, bbx1:bbx2, bby1:bby2]\n    return images, masks\n\n###############################################\n# Advanced 후처리: Morphological Closing (Kernel 9×9)\n###############################################\ndef postprocess_mask(mask, min_size=100):\n    labeled_mask = label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n###############################################\n# AttentionGate 모듈\n###############################################\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(AttentionGate, self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi\n\n###############################################\n# MultiScaleFusion 모듈\n###############################################\nclass MultiScaleFusion(nn.Module):\n    def __init__(self, in_channels_list, out_channels):\n        super(MultiScaleFusion, self).__init__()\n        self.convs = nn.ModuleList([nn.Conv2d(ch, out_channels, kernel_size=1) for ch in in_channels_list])\n        self.fuse = nn.Sequential(\n            nn.Conv2d(len(in_channels_list) * out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, features):\n        target_size = features[-1].size()[2:]\n        processed = []\n        for i, f in enumerate(features):\n            if f.size()[2:] != target_size:\n                f = F.interpolate(f, size=target_size, mode='bilinear', align_corners=False)\n            f = self.convs[i](f)\n            processed.append(f)\n        out = torch.cat(processed, dim=1)\n        out = self.fuse(out)\n        return out\n\n###############################################\n# 기타 모델 관련 모듈 (StochasticDepth, GhostModule, SELayer, DynamicGating)\n###############################################\nclass StochasticDepth(nn.Module):\n    def __init__(self, p, mode=\"row\"):\n        super(StochasticDepth, self).__init__()\n        self.p = p\n        self.mode = mode\n\n    def forward(self, x):\n        if not self.training or self.p == 0.0:\n            return x\n        survival_rate = 1 - self.p\n        if self.mode == \"row\":\n            batch_size = x.shape[0]\n            noise = torch.rand(batch_size, 1, 1, 1, device=x.device, dtype=x.dtype)\n            binary_mask = (noise < survival_rate).float()\n            return x / survival_rate * binary_mask\n        else:\n            if torch.rand(1).item() < self.p:\n                return torch.zeros_like(x)\n            else:\n                return x\n\nclass GhostModule(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, ratio=2, dw_kernel_size=3, stride=1, padding=0, use_relu=True):\n        super(GhostModule, self).__init__()\n        self.out_channels = out_channels\n        self.primary_channels = int(torch.ceil(torch.tensor(out_channels / ratio)))\n        self.cheap_channels = out_channels - self.primary_channels\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(in_channels, self.primary_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(self.primary_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n        self.cheap_conv = nn.Sequential(\n            nn.Conv2d(self.primary_channels, self.cheap_channels, dw_kernel_size, stride=1,\n                      padding=dw_kernel_size // 2, groups=self.primary_channels, bias=False),\n            nn.BatchNorm2d(self.cheap_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_conv(x1)\n        out = torch.cat([x1, x2], dim=1)\n        return out[:, :self.out_channels, :, :].contiguous()\n\ndef ghost_conv_block(in_channels, out_channels, use_relu=True):\n    return nn.Sequential(\n        GhostModule(in_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu),\n        GhostModule(out_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu)\n    )\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\nclass DynamicGating(nn.Module):\n    def __init__(self, num_branches, hidden_dim=64, dropout_prob=0.1, init_temperature=2.0, iterations=3):\n        super(DynamicGating, self).__init__()\n        self.temperature = init_temperature\n        self.iterations = iterations\n        self.dropout_prob = dropout_prob\n        self.fc = nn.Sequential(\n            nn.Linear(num_branches, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, num_branches)\n        )\n        self.layernorm = nn.LayerNorm(num_branches)\n        self.softmax = nn.Softmax(dim=1)\n        self.res_scale = nn.Parameter(torch.ones(1))\n        self.saved_log_probs = None\n        self.entropy = None\n        self.rl_loss = 0.0\n\n    def forward(self, features):\n        norm_features = self.layernorm(features)\n        logits = self.fc(norm_features)\n        logits = self.layernorm(logits)\n        scaled_logits = logits / self.temperature\n        gates = self.softmax(scaled_logits)\n        for _ in range(self.iterations - 1):\n            updated_features = norm_features + self.res_scale * gates\n            logits = self.fc(updated_features)\n            logits = self.layernorm(logits)\n            scaled_logits = logits / self.temperature\n            new_gates = self.softmax(scaled_logits)\n            gates = 0.5 * gates + 0.5 * new_gates\n        return gates\n\n###############################################\n# QuadAgentBlock (Lite 버전, 3 브랜치)\n###############################################\nclass QuadAgentBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5):\n        super().__init__()\n        branch_channels = out_channels // 4\n        self.branch1 = GhostModule(in_channels, branch_channels, ratio=4)\n        self.branch2 = GhostModule(in_channels, branch_channels, kernel_size=3, padding=1, ratio=4)\n        self.branch3 = nn.Sequential(\n            GhostModule(in_channels, branch_channels, ratio=4),\n            SELayer(branch_channels, reduction=32)\n        )\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.gating = DynamicGating(num_branches=3, hidden_dim=gating_hidden_dim, dropout_prob=gating_dropout, init_temperature=gating_temperature)\n        self.fusion_conv = GhostModule(branch_channels * 3, out_channels, ratio=2)\n        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False) if in_channels != out_channels else nn.Identity()\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob)\n\n    def forward(self, x):\n        b1 = self.branch1(x)\n        b2 = self.branch2(x)\n        b3 = self.branch3(x)\n        gap_b1 = self.gap(b1).view(x.size(0), -1)\n        gap_b2 = self.gap(b2).view(x.size(0), -1)\n        gap_b3 = self.gap(b3).view(x.size(0), -1)\n        features = torch.stack([gap_b1.mean(dim=1), gap_b2.mean(dim=1), gap_b3.mean(dim=1)], dim=1)\n        gates = self.gating(features)\n        b1 = b1 * gates[:, 0].view(-1, 1, 1, 1)\n        b2 = b2 * gates[:, 1].view(-1, 1, 1, 1)\n        b3 = b3 * gates[:, 2].view(-1, 1, 1, 1)\n        out = torch.cat([b1, b2, b3], dim=1)\n        out = self.fusion_conv(out)\n        res = self.res_conv(x)\n        res = self.stochastic_depth(res)\n        return out + res\n\n###############################################\n# DARKViT_UNet: ViT 기반 Encoder + QuadAgentBlock + Transformer Bottleneck + UNet Decoder\n###############################################\nfrom timm.models import create_model\n\nclass DARKViT_UNet(nn.Module):\n    def __init__(self, out_channels=1):\n        super(DARKViT_UNet, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True)\n        self.bottleneck = QuadAgentBlock(in_channels=1536, out_channels=1536, \n                                         gating_dropout=0.3, gating_hidden_dim=32, \n                                         gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, \n                                                             dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, \n                                                             dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n    \n    def forward(self, x):\n        # encoder에서 feature map 리스트 얻기: [f0, f1, f2, f3]\n        features = self.encoder(x)\n        \n        # 만약 각 feature map이 (N, H, W, C) 형식이라면 (N, C, H, W)로 변환\n        for i in range(len(features)):\n            # f0: 192, f1:384, f2:768, f3:1536 채널이어야 함\n            if features[i].shape[1] not in {192, 384, 768, 1536}:\n                features[i] = features[i].permute(0, 3, 1, 2).contiguous()\n        \n        # 반환된 순서는 [f0, f1, f2, f3]이므로, decoder가 기대하는 [f3, f2, f1, f0] 순서로 재정렬\n        f0, f1, f2, f3 = features  \n        features_reordered = [f3, f2, f1, f0]\n        \n        # 이제 features_reordered[0]는 f3: (B,1536,7,7)\n        b_out = self.bottleneck(features_reordered[0])\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        features_reordered[0] = t_out2\n        \n        decoded = self.decoder(features_reordered)\n        return self.final_conv(decoded)\n\n###############################################\n# UNet Decoder for Swin Backbone\n###############################################\nclass UNetDecoder_Swin(nn.Module):\n    def __init__(self):\n        super(UNetDecoder_Swin, self).__init__()\n        # Swin‑Large feature sizes (가정):\n        # f0: (B,192,56,56), f1: (B,384,28,28), f2: (B,768,14,14), f3: (B,1536,7,7)\n        self.up1 = nn.ConvTranspose2d(1536, 768, kernel_size=2, stride=2)  # 7->14\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(768 + 768, 768, kernel_size=3, padding=1),\n            nn.BatchNorm2d(768),\n            nn.ReLU(inplace=True)\n        )\n        self.up2 = nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2)   # 14->28\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(384 + 384, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True)\n        )\n        self.up3 = nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2)   # 28->56\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(192 + 192, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n        self.up4 = nn.ConvTranspose2d(192, 64, kernel_size=2, stride=2)    # 56->112\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, features_list):\n        # features_list: [f0, f1, f2, f3]\n        f3, f2, f1, f0 = features_list  # ✅ 순서 맞추기\n        x = f3  # (B,1536,7,7)\n        x = self.up1(x)  # -> (B,768,14,14)\n        x = torch.cat([x, f2], dim=1)  # f2: (B,768,14,14) → (B,1536,14,14)\n        x = self.conv1(x)  # -> (B,768,14,14)\n        x = self.up2(x)  # -> (B,384,28,28)\n        x = torch.cat([x, f1], dim=1)  # f1: (B,384,28,28) → (B,768,28,28)\n        x = self.conv2(x)  # -> (B,384,28,28)\n        x = self.up3(x)  # -> (B,192,56,56)\n        x = torch.cat([x, f0], dim=1)  # f0: (B,192,56,56) → (B,384,56,56)\n        x = self.conv3(x)  # -> (B,192,56,56)\n        x = self.up4(x)  # -> (B,64,112,112)\n        x = self.conv4(x)  # -> (B,64,112,112)\n        # 최종 interpolation을 (256,256)으로 변경\n        x = F.interpolate(x, size=(256,256), mode='bilinear', align_corners=False)\n        return x\n\n###############################################\n# Transformer Bottleneck (연속 두 개 적용)\n###############################################\nclass TransformerBottleneck(nn.Module):\n    def __init__(self, d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1):\n        super(TransformerBottleneck, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                    dim_feedforward=dim_feedforward, dropout=dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    def forward(self, x):\n        B, C, H, W = x.size()\n        x = x.view(B, C, H * W).permute(2, 0, 1)\n        x = self.transformer(x)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\n###############################################\n# Swin‑Transformer Backbone 기반 UNet with Attention + Lite Bottleneck\n###############################################\nfrom timm.models import create_model\n\nclass PretrainedSwin_UNet_AttentionFusion(nn.Module):\n    def __init__(self, out_channels=1):\n        super(PretrainedSwin_UNet_AttentionFusion, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True, img_size=256)\n        self.bottleneck = QuadAgentBlock(in_channels=1536, out_channels=1536, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        features_list = self.encoder(x)  # [f0, f1, f2, f3]\n\n        # (N, H, W, C) → (N, C, H, W) 변환이 필요한 경우 적용\n        for i in range(len(features_list)):\n            if features_list[i].shape[1] not in {192, 384, 768, 1536}:  \n                features_list[i] = features_list[i].permute(0, 3, 1, 2).contiguous()\n\n        # 디코더에 올바른 순서로 전달되도록 변환\n        f0, f1, f2, f3 = features_list  # Swin Transformer의 기본 반환 순서\n        features_list = [f3, f2, f1, f0]  # (B, 1536, 7, 7) → (B, 192, 56, 56)\n\n        # Bottleneck과 Transformer Bottleneck 적용\n        b_out = self.bottleneck(features_list[0])\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        features_list[0] = t_out2\n\n        decoded = self.decoder(features_list)\n        return self.final_conv(decoded)\n\n###############################################\n# Loss & Metric Functions (HD 포함)\n###############################################\nfrom segmentation_models_pytorch.losses import LovaszLoss\nlv_loss = LovaszLoss(mode='binary')\n\ndef focal_tversky_loss(pred, target, alpha=0.5, beta=0.5, gamma=4/3, smooth=1e-6):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred = pred.view(pred.size(0), -1)\n    target = target.view(target.size(0), -1)\n    tp = (pred * target).sum(dim=1)\n    fp = ((1 - target) * pred).sum(dim=1)\n    fn = (target * (1 - pred)).sum(dim=1)\n    tversky_index = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n    loss = (1 - tversky_index) ** gamma\n    return loss.mean()\n\ndef boundary_loss(pred, target):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_np = pred.detach().cpu().numpy()\n    target_np = target.detach().cpu().numpy()\n    boundary_masks = []\n    for i in range(pred_np.shape[0]):\n        gt_mask = target_np[i, 0]\n        boundary = find_boundaries(gt_mask, mode='thick')\n        boundary_masks.append(boundary.astype(np.float32))\n    boundary_masks = np.stack(boundary_masks, axis=0)[:, None, :, :]\n    boundary_masks_torch = torch.from_numpy(boundary_masks).to(pred.device)\n    intersect = (pred * boundary_masks_torch).sum()\n    denom = pred.sum() + boundary_masks_torch.sum()\n    boundary_dice = (2.0 * intersect) / (denom + 1e-6)\n    return 1.0 - boundary_dice\n\ndef combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    loss_ft = focal_tversky_loss(outputs, masks)\n    if pos_weight is not None:\n        loss_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, masks.float())\n    else:\n        loss_bce = nn.BCEWithLogitsLoss()(outputs, masks.float())\n    bl = boundary_loss(outputs, masks)\n    lv = lv_loss(outputs, masks)\n    return 0.4 * lv + 0.3 * loss_ft + 0.3 * loss_bce + lambda_boundary * bl\n\ndef dice_f1_precision_recall(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    precision = intersection / (pred_bin.sum() + smooth)\n    recall = intersection / (target_bin.sum() + smooth)\n    f1 = 2 * (precision * recall) / (precision + recall + smooth)\n    return f1.item(), precision.item(), recall.item()\n\ndef iou_metric(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    union = pred_bin.sum() + target_bin.sum() - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach()\n    processed_preds = []\n    for i in range(pred_probs.size(0)):\n        pred_np = pred_probs[i].cpu().numpy()[0]\n        pred_bin = (pred_np > threshold).astype(np.uint8)\n        processed = postprocess_mask(pred_bin, min_size=100)\n        processed_preds.append(processed)\n    batch_iou = []\n    batch_f1 = []\n    batch_precision = []\n    batch_recall = []\n    for i in range(pred_probs.size(0)):\n        pred = processed_preds[i]\n        gt = masks[i].cpu().numpy()[0]\n        intersection = np.sum(pred * gt)\n        union = np.sum(pred) + np.sum(gt) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        batch_iou.append(iou)\n        precision = intersection / (np.sum(pred) + smooth)\n        recall = intersection / (np.sum(gt) + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        batch_f1.append(f1)\n        batch_precision.append(precision)\n        batch_recall.append(recall)\n    probs = pred_probs.cpu().numpy().flatten()\n    masks_np = masks.cpu().numpy().flatten()\n    try:\n        auc_score = roc_auc_score(masks_np, probs)\n    except ValueError:\n        auc_score = float('nan')\n    return auc_score, np.mean(batch_iou), np.mean(batch_f1), np.mean(batch_precision), np.mean(batch_recall)\n\ndef hausdorff_distance(pred, target):\n    return medpy_binary.hd95(pred, target)\n\ndef compute_hd_metric(outputs, masks, threshold=0.5):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n    masks_np = masks.detach().cpu().numpy()\n    hd_list = []\n    for i in range(outputs.size(0)):\n        pred_bin = (pred_probs[i, 0] > threshold).astype(np.uint8)\n        gt_bin = (masks_np[i, 0] > 0.5).astype(np.uint8)\n        try:\n            hd = medpy_binary.hd95(pred_bin, gt_bin)\n        except Exception:\n            hd = np.nan\n        hd_list.append(hd)\n    return np.nanmean(hd_list)\n\ndef evaluate_with_metrics(model, dataloader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None):\n    model.eval()\n    total_loss = 0.0\n    total_auc  = 0.0\n    total_iou  = 0.0\n    total_f1   = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    total_hd = 0.0\n    total_samples = 0\n    with torch.no_grad():\n        for images, masks, _ in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=lambda_boundary)\n            auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=threshold, smooth=1e-6)\n            hd_val = compute_hd_metric(outputs, masks, threshold=threshold)\n            bs = images.size(0)\n            total_loss += loss.item() * bs\n            total_auc  += auc_score * bs\n            total_iou  += iou_val * bs\n            total_f1   += f1_val * bs\n            total_precision += prec * bs\n            total_recall += rec * bs\n            total_hd += hd_val * bs\n            total_samples += bs\n    avg_loss = total_loss / total_samples\n    avg_auc  = total_auc / total_samples\n    avg_iou  = total_iou / total_samples\n    avg_f1   = total_f1 / total_samples\n    avg_precision = total_precision / total_samples\n    avg_recall = total_recall / total_samples\n    avg_hd = total_hd / total_samples\n    return avg_loss, avg_auc, avg_iou, avg_f1, avg_precision, avg_recall, avg_hd\n\n###############################################\n# TTA (Test Time Augmentation) 함수 (Noise & ColorJitter 추가)\n###############################################\ndef tta_predict(model, image, device):\n    cj = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n    def add_noise(x, std=0.05):\n        noise = torch.randn_like(x) * std\n        return x + noise\n    def identity(x): return x\n    def hflip(x): return torch.flip(x, dims=[-1])\n    def vflip(x): return torch.flip(x, dims=[-2])\n    def rot90(x): return torch.rot90(x, k=1, dims=[-2, -1])\n    def inv_hflip(x): return torch.flip(x, dims=[-1])\n    def inv_vflip(x): return torch.flip(x, dims=[-2])\n    def inv_rot90(x): return torch.rot90(x, k=3, dims=[-2, -1])\n    \n    transforms_list = [\n        (identity, identity),\n        (hflip, inv_hflip),\n        (vflip, inv_vflip),\n        (rot90, inv_rot90),\n        (lambda x: add_noise(cj(x)), lambda x: x)\n    ]\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for aug, inv in transforms_list:\n            augmented = aug(image)\n            output = model(augmented.unsqueeze(0).to(device))\n            output = torch.sigmoid(output)\n            output = inv(output).cpu()\n            predictions.append(output)\n    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n    return avg_prediction\n\n###############################################\n# Optimizer Scheduler (SGDR: CosineAnnealingWarmRestarts)\n###############################################\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\n###############################################\n# 데이터 로더 및 BUSI Dataset 설정\n###############################################\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nfull_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\nindices = np.arange(len(full_dataset))\ntrain_val_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\ntrain_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=42)\ntrain_dataset = Subset(full_dataset, train_idx)\nval_dataset = Subset(full_dataset, val_idx)\ntest_dataset = Subset(full_dataset, test_idx)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=16, shuffle=True, num_workers=4,\n    worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id),\n    pin_memory=True, persistent_workers=True\n)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\n\ndef calculate_pos_weight(loader):\n    total_pixels = 0\n    positive_pixels = 0\n    for images, masks, _ in loader:\n        positive_pixels += masks.sum().item()\n        total_pixels += masks.numel()\n    negative_pixels = total_pixels - positive_pixels\n    return torch.tensor(negative_pixels / (positive_pixels + 1e-6)).to(device)\n\n###############################################\n# 모델, Optimizer, Scheduler 설정\n###############################################\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\npos_weight = calculate_pos_weight(train_loader)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nnum_epochs = 500\n\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n\n###############################################\n# Training Loop (Early Stopping patience=15)\n###############################################\nfrom torch.cuda.amp import GradScaler, autocast\nscaler = GradScaler()\npatience = 15\nbest_val_f1 = 0.0\npatience_counter = 0\n\"\"\"\nfor epoch in range(num_epochs):\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss_seg = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=0.2, lambda_lovasz=0.6)\n            loss_total = loss_seg  # RL 미사용\n        scaler.scale(loss_total).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        bs = images.size(0)\n        epoch_loss += loss_total.item() * bs\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples\n    avg_auc = epoch_auc / total_samples\n    avg_iou = epoch_iou / total_samples\n    avg_f1 = epoch_f1 / total_samples\n    avg_precision = epoch_precision / total_samples\n    avg_recall = epoch_recall / total_samples\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n    \n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model.pth')\n        patience_counter = 0\n        print(\"🍀 New best validation Dice achieved! Dice: {:.4f} 🍀\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:10:45.096703Z","iopub.execute_input":"2025-03-15T09:10:45.097035Z","iopub.status.idle":"2025-03-15T09:11:19.884214Z","shell.execute_reply.started":"2025-03-15T09:10:45.097012Z","shell.execute_reply":"2025-03-15T09:11:19.883056Z"},"jupyter":{"source_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/788M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9c9af1ce06d4c6b9b3e3dec2ca2643d"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\nfor epoch in range(num_epochs):\\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\\n    if hasattr(model, \\'bottleneck\\'):\\n        if isinstance(model, nn.DataParallel):\\n            model.module.bottleneck.gating.temperature = current_temp\\n        else:\\n            model.bottleneck.gating.temperature = current_temp\\n\\n    model.train()\\n    epoch_loss = 0.0\\n    epoch_auc = 0.0\\n    epoch_iou = 0.0\\n    epoch_f1 = 0.0\\n    epoch_precision = 0.0\\n    epoch_recall = 0.0\\n    total_samples = 0\\n\\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\\n        images = images.to(device)\\n        masks = masks.to(device)\\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\\n        optimizer.zero_grad()\\n        with autocast():\\n            outputs = model(images)\\n            loss_seg = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=0.2, lambda_lovasz=0.6)\\n            loss_total = loss_seg  # RL 미사용\\n        scaler.scale(loss_total).backward()\\n        scaler.unscale_(optimizer)\\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n        scaler.step(optimizer)\\n        scaler.update()\\n        bs = images.size(0)\\n        epoch_loss += loss_total.item() * bs\\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\\n        epoch_auc += auc_score * bs\\n        epoch_iou += iou_val * bs\\n        epoch_f1 += f1_val * bs\\n        epoch_precision += prec * bs\\n        epoch_recall += rec * bs\\n        total_samples += bs\\n\\n    scheduler.step(epoch)\\n    avg_loss = epoch_loss / total_samples\\n    avg_auc = epoch_auc / total_samples\\n    avg_iou = epoch_iou / total_samples\\n    avg_f1 = epoch_f1 / total_samples\\n    avg_precision = epoch_precision / total_samples\\n    avg_recall = epoch_recall / total_samples\\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\\n    \\n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\\n    \\n    if val_f1 > best_val_f1:\\n        best_val_f1 = val_f1\\n        best_weights = copy.deepcopy(model.state_dict())\\n        torch.save(best_weights, \\'best_model.pth\\')\\n        patience_counter = 0\\n        print(\"🍀 New best validation Dice achieved! Dice: {:.4f} 🍀\".format(best_val_f1))\\n    else:\\n        patience_counter += 1\\n        if patience_counter >= patience:\\n            print(f\"Early stopping at epoch {epoch+1}\")\\n            break\\n'"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Puesudo Labeling","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport copy\nimport glob\nimport random\nimport warnings\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport cv2  # CLAHE 적용을 위해 OpenCV 사용\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage.segmentation import find_boundaries\nfrom skimage.measure import label, regionprops\nimport scipy.ndimage\n\n# Albumentations 기반 augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# medpy HD metric\nfrom medpy.metric import binary as medpy_binary\nfrom torchvision.transforms import ColorJitter\n\n###############################################\n# Seed 및 Warning 설정\n###############################################\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\nwarnings.filterwarnings('ignore')\n\n###############################################\n# BUSI Segmentation Dataset (라벨 데이터)\n###############################################\nclass BUSISegmentationDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform  \n        self.samples = []  # 반드시 초기화\n        self._prepare_samples()\n\n    def _prepare_samples(self):\n        labels = os.listdir(self.data_path)\n        for label in labels:\n            folder_path = os.path.join(self.data_path, label)\n            if not os.path.isdir(folder_path):\n                continue\n            files = os.listdir(folder_path)\n            image_files = sorted([f for f in files if '_mask' not in f and f.endswith('.png')])\n            mask_files  = sorted([f for f in files if '_mask' in f and f.endswith('.png')])\n            pattern_img = re.compile(rf'{re.escape(label)} \\((\\d+)\\)\\.png')\n            pattern_mask = re.compile(rf'{re.escape(label)} \\((\\d+)\\)_mask(?:_\\d+)?\\.png')\n            mask_dict = {}\n            for mf in mask_files:\n                m = pattern_mask.fullmatch(mf)\n                if m:\n                    idx = m.group(1)\n                    mask_dict.setdefault(idx, []).append(mf)\n            for im in image_files:\n                m = pattern_img.fullmatch(im)\n                if m:\n                    idx = m.group(1)\n                    img_path = os.path.join(folder_path, im)\n                    if idx in mask_dict:\n                        mask_paths = [os.path.join(folder_path, mf) for mf in mask_dict[idx]]\n                        combined_mask = None\n                        for mp in mask_paths:\n                            mask_img = Image.open(mp).convert('L')\n                            mask_arr = np.array(mask_img)\n                            mask_binary = (mask_arr > 128).astype(np.uint8)\n                            if combined_mask is None:\n                                combined_mask = mask_binary\n                            else:\n                                combined_mask = np.maximum(combined_mask, mask_binary)\n                        self.samples.append((img_path, combined_mask, label))\n                    else:\n                        image_pil = Image.open(img_path)\n                        empty_mask = np.zeros(image_pil.size[::-1], dtype=np.uint8)\n                        self.samples.append((img_path, empty_mask, label))\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, mask_array, label = self.samples[index]\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.fromarray((mask_array * 255).astype(np.uint8))\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        return image, mask, label\n\n###############################################\n# Per-image Z-score normalization (adaptive)\n###############################################\ndef z_score_normalize(tensor):\n    mean = tensor.mean()\n    std = tensor.std() + 1e-6\n    return (tensor - mean) / std\n\n###############################################\n# CLAHE Augmentation 함수 (OpenCV)\n###############################################\ndef apply_clahe(image, clipLimit=2.0, tileGridSize=(8,8)):\n    img_np = np.array(image)\n    if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl, a, b))\n        final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    else:\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        final = clahe.apply(img_np)\n    return Image.fromarray(final)\n\n###############################################\n# joint_transform (Albumentations 기반 Augmentation)\n# 여기서 입력 이미지와 mask를 256×256으로 resize\n###############################################\ndef joint_transform(image, mask, size=(256,256)):\n    geom_transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=10, p=0.5),\n        A.ElasticTransform(alpha=10, sigma=5, alpha_affine=5, p=0.3),\n        A.Resize(height=size[0], width=size[1])\n    ])\n    image_np = np.array(image)\n    mask_np = np.array(mask)\n    augmented = geom_transform(image=image_np, mask=mask_np)\n    image = augmented['image']\n    mask = augmented['mask']\n    \n    intensity_transform = A.Compose([\n        A.RandomBrightnessContrast(p=0.5),\n        A.CLAHE(clip_limit=1.0, tile_grid_size=(8,8), p=0.5),\n        A.GaussianBlur(p=0.3)\n    ])\n    image = intensity_transform(image=image)['image']\n    \n    image = transforms.ToTensor()(image)\n    image = z_score_normalize(image)\n    mask = transforms.ToTensor()(mask)\n    return image, mask\n\n###############################################\n# CutMix 함수 (alpha=1.5, p=0.7)\n###############################################\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(images, masks, alpha=1.5, p=0.7):\n    if np.random.rand() > p:\n        return images, masks\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(images.size(0)).to(images.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n    masks[:, :, bbx1:bbx2, bby1:bby2] = masks[rand_index, :, bbx1:bbx2, bby1:bby2]\n    return images, masks\n\n###############################################\n# Advanced 후처리: Morphological Closing (Kernel 9×9)\n###############################################\ndef postprocess_mask(mask, min_size=100):\n    labeled_mask = label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n# ----------------------------------------------------------------\n# ... (AttentionGate, MultiScaleFusion, StochasticDepth, GhostModule, SELayer, DynamicGating,\n# QuadAgentBlock, DARKViT_UNet, UNetDecoder_Swin, TransformerBottleneck, PretrainedSwin_UNet_AttentionFusion 등\n# 기존 코드 정의는 그대로 사용) \n# ----------------------------------------------------------------\n\n# 여기서는 이미 기존 코드의 모델, loss, metric 함수들을 정의했다고 가정합니다.\n# (위에 제공된 코드의 나머지 부분을 그대로 사용)\n\n###############################################\n# Unlabeled Dataset 클래스 (라벨 없는 데이터)\n###############################################\nclass UnlabeledDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform\n        # 폴더 내 모든 PNG 파일 경로 읽기\n        self.image_paths = sorted([os.path.join(data_path, fname) for fname in os.listdir(data_path) if fname.endswith('.png')])\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        img_path = self.image_paths[index]\n        image = Image.open(img_path).convert(\"RGB\")\n        # mask는 없으므로 더미 값을 사용 (예: None 혹은 0으로 채워진 이미지)\n        dummy_mask = Image.new(\"L\", image.size, 0)\n        if self.transform:\n            image, _ = self.transform(image, dummy_mask)\n        else:\n            image = transforms.ToTensor()(image)\n        return image, img_path\n\n###############################################\n# Pseudo Labeling 함수\n###############################################\ndef generate_pseudo_labels(model, unlabeled_loader, device, threshold=0.9):\n    model.eval()\n    pseudo_data = []  # (image, pseudo mask) 튜플 리스트\n    with torch.no_grad():\n        for images, paths in unlabeled_loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = torch.sigmoid(outputs)\n            # threshold 기준을 넘는 부분을 pseudo label로 사용 (여기서는 픽셀 단위)\n            pseudo_masks = (preds > threshold).float()\n            for i in range(images.size(0)):\n                # 각 이미지와 해당 pseudo mask를 CPU로 가져와 저장\n                pseudo_data.append((images[i].cpu(), pseudo_masks[i].cpu()))\n    return pseudo_data\n\n###############################################\n# Combined Dataset: labeled + pseudo labeled 데이터\n###############################################\nclass CombinedDataset(Dataset):\n    def __init__(self, labeled_dataset, pseudo_data):\n        self.labeled_dataset = labeled_dataset\n        self.pseudo_data = pseudo_data\n\n    def __len__(self):\n        return len(self.labeled_dataset) + len(self.pseudo_data)\n    \n    def __getitem__(self, index):\n        if index < len(self.labeled_dataset):\n            return self.labeled_dataset[index]\n        else:\n            pseudo_index = index - len(self.labeled_dataset)\n            image, mask = self.pseudo_data[pseudo_index]\n            # pseudo 데이터의 label은 'pseudo'로 구분\n            return image, mask, \"pseudo\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:11:26.269769Z","iopub.execute_input":"2025-03-15T09:11:26.270146Z","iopub.status.idle":"2025-03-15T09:11:26.301681Z","shell.execute_reply.started":"2025-03-15T09:11:26.270110Z","shell.execute_reply":"2025-03-15T09:11:26.300865Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\n\n# /kaggle/working/ 경로에 \"unlabeled-busi-images\" 폴더 생성\nfolder_path = '/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/'\nos.makedirs(folder_path, exist_ok=True)\nprint(f\"Folder created: {folder_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:11:31.066240Z","iopub.execute_input":"2025-03-15T09:11:31.066584Z","iopub.status.idle":"2025-03-15T09:11:31.072857Z","shell.execute_reply.started":"2025-03-15T09:11:31.066562Z","shell.execute_reply":"2025-03-15T09:11:31.072177Z"}},"outputs":[{"name":"stdout","text":"Folder created: /kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler, autocast\nfrom tqdm import tqdm\nimport copy\n\n# 1. Device, 모델, Optimizer, Scheduler 등 초기 설정\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 라벨 데이터셋과 unlabeled 데이터셋 생성\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nlabeled_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\n\nunlabeled_data_path = '/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/'  # 실제 unlabeled 데이터 경로로 수정\nunlabeled_dataset = UnlabeledDataset(unlabeled_data_path, transform=joint_transform)\n\nlabeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True, num_workers=4)\nunlabeled_loader = DataLoader(unlabeled_dataset, batch_size=16, shuffle=False, num_workers=4)\n\n# 모델 생성 (Swin 백본에 img_size=256 적용되어 있다고 가정)\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n\nscaler = GradScaler()\npatience = 15\nbest_val_f1 = 0.0\npatience_counter = 0\n\n# 2. 초기 학습 (라벨 데이터만 사용, 예: 10 에폭)\nnum_initial_epochs = 10\nprint(\"===> Initial training on labeled data...\")\n\nfor epoch in range(num_initial_epochs):\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_initial_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    total_samples = 0\n\n    for images, masks, _ in tqdm(labeled_loader, desc=f\"Initial Epoch {epoch+1}/{num_initial_epochs}\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        # CutMix 적용 (선택 사항)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        bs = images.size(0)\n        epoch_loss += loss.item() * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples\n    print(f\"Initial Epoch {epoch+1}/{num_initial_epochs} | Loss: {avg_loss:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:11:36.687000Z","iopub.execute_input":"2025-03-15T09:11:36.687343Z","iopub.status.idle":"2025-03-15T09:22:20.833743Z","shell.execute_reply.started":"2025-03-15T09:11:36.687317Z","shell.execute_reply":"2025-03-15T09:22:20.832443Z"}},"outputs":[{"name":"stdout","text":"===> Initial training on labeled data...\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 1/10: 100%|██████████| 49/49 [01:24<00:00,  1.73s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 1/10 | Loss: 1.0146\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 2/10: 100%|██████████| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 2/10 | Loss: 0.9148\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 3/10: 100%|██████████| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 3/10 | Loss: 0.8546\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 4/10: 100%|██████████| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 4/10 | Loss: 0.8292\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 5/10: 100%|██████████| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 5/10 | Loss: 0.7959\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 6/10: 100%|██████████| 49/49 [01:01<00:00,  1.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 6/10 | Loss: 0.7556\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 7/10: 100%|██████████| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 7/10 | Loss: 0.7172\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 8/10: 100%|██████████| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 8/10 | Loss: 0.7016\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 9/10: 100%|██████████| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 9/10 | Loss: 0.6822\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 10/10: 100%|██████████| 49/49 [01:01<00:00,  1.26s/it]","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 10/10 | Loss: 0.6603\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from skimage.measure import label as sk_label, regionprops\n\ndef postprocess_mask(mask, min_size=100):\n    # skimage.measure의 label 함수를 sk_label로 호출\n    labeled_mask = sk_label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n# 3. Pseudo Labeling: unlabeled 데이터에 대해 모델 예측 수행\nprint(\"===> Generating pseudo labels on unlabeled data...\")\npseudo_data = generate_pseudo_labels(model, unlabeled_loader, device, threshold=0.9)\nprint(f\"Pseudo labels generated for {len(pseudo_data)} images.\")\n\n# 4. 라벨 데이터와 pseudo labeled 데이터를 합쳐 CombinedDataset 생성\ncombined_dataset = CombinedDataset(labeled_dataset, pseudo_data)\ncombined_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True, num_workers=4)\n\n# 5. Fine-tuning on Combined Dataset (labeled + pseudo labeled)\nnum_finetune_epochs = 20\nprint(\"===> Fine-tuning on combined dataset (labeled + pseudo labeled)...\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:22:20.835730Z","iopub.execute_input":"2025-03-15T09:22:20.836112Z","iopub.status.idle":"2025-03-15T09:22:21.060944Z","shell.execute_reply.started":"2025-03-15T09:22:20.836075Z","shell.execute_reply":"2025-03-15T09:22:21.059589Z"}},"outputs":[{"name":"stdout","text":"===> Generating pseudo labels on unlabeled data...\nPseudo labels generated for 0 images.\n===> Fine-tuning on combined dataset (labeled + pseudo labeled)...\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"from torch.cuda.amp import GradScaler, autocast\nfrom tqdm import tqdm\nimport copy\n\n# 기본 설정\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 데이터셋 생성 (경로는 실제 데이터 경로로 수정)\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nlabeled_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\nunlabeled_data_path = '/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/'\nunlabeled_dataset = UnlabeledDataset(unlabeled_data_path, transform=joint_transform)\n\n# DataLoader (디버깅을 위해 num_workers=0 사용 - 필요 시 값을 늘리세요)\nlabeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True, num_workers=0)\nval_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=False, num_workers=0)\n\n# 모델 생성 (백본 생성 시 img_size=256 적용되어 있다고 가정)\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\n# 옵티마이저와 scheduler 설정: 학습률을 낮게 설정하여 안정화 (예: 1e-5)\noptimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\nscaler = GradScaler()\n\n# Early stopping 설정\npatience = 50\nbest_val_f1 = 0.0\npatience_counter = 0\n\n# 온도 스케줄: 온도가 최소 2.0 이하로 내려가지 않도록 함 (최대 5.0에서 선형 감소)\nmin_temp = 2.0\nmax_temp = 5.0\n\nnum_epochs = 500\ncombined_loader = None  # 이후 pseudo labeled 데이터를 위한 DataLoader\n\nfor epoch in range(num_epochs):\n    # 온도 조절: 선형적으로 감소하되 최소값은 min_temp로 유지\n    current_temp = max_temp - ((max_temp - min_temp) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    # 0~29 epoch: labeled 데이터만 사용, 이후부터 combined_loader 사용\n    if epoch < 30:\n        train_loader = labeled_loader\n    else:\n        # epoch 30에서 한 번만 pseudo labeling 생성\n        if epoch == 30:\n            print(\"===> Generating pseudo labels on unlabeled data...\")\n            pseudo_loader = DataLoader(unlabeled_dataset, batch_size=16, shuffle=False, num_workers=0)\n            pseudo_data = generate_pseudo_labels(model, pseudo_loader, device, threshold=0.9)\n            print(f\"Pseudo labels generated for {len(pseudo_data)} images.\")\n            combined_dataset = CombinedDataset(labeled_dataset, pseudo_data)\n            train_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True, num_workers=0)\n            combined_loader = train_loader\n        else:\n            train_loader = combined_loader\n\n    # Training phase\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        # CutMix 적용 (옵션)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6)\n        # 만약 손실값이 nan이면 해당 배치를 건너뛰기\n        if torch.isnan(loss):\n            print(\"Warning: loss is NaN, skipping batch\")\n            continue\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        # 더 엄격한 gradient clipping: max_norm=0.5\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n        scaler.step(optimizer)\n        scaler.update()\n\n        bs = images.size(0)\n        epoch_loss += loss.item() * bs\n\n        # 배치별 metric 계산 (compute_batch_metrics_new 함수는 내부에서 GPU 연산 대신 NumPy로 변환하는 부분이 있으므로, 여기서는 그대로 사용)\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-5)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples if total_samples > 0 else float('nan')\n    avg_auc = epoch_auc / total_samples if total_samples > 0 else float('nan')\n    avg_iou = epoch_iou / total_samples if total_samples > 0 else float('nan')\n    avg_f1 = epoch_f1 / total_samples if total_samples > 0 else float('nan')\n    avg_precision = epoch_precision / total_samples if total_samples > 0 else float('nan')\n    avg_recall = epoch_recall / total_samples if total_samples > 0 else float('nan')\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, \"\n          f\"Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    # Validation 평가 (라벨 데이터 사용)\n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(\n        model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, \"\n          f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n    \n    # Early stopping: validation Dice/F1가 개선되지 않으면 patience_counter 증가\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model_with_pseudo.pth')\n        patience_counter = 0\n        print(\"🍀 New best validation Dice achieved! Dice: {:.4f} 🍀\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"Training complete. Best model saved as 'best_model_with_pseudo.pth'.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-15T09:03:08.247793Z","iopub.execute_input":"2025-03-15T09:03:08.248038Z","execution_failed":"2025-03-15T09:09:20.796Z"},"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"from skimage.measure import label as sk_label, regionprops\n\ndef postprocess_mask(mask, min_size=100):\n    # skimage.measure의 label 함수를 sk_label로 호출\n    labeled_mask = sk_label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\ndef compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach()\n    processed_preds = []\n    for i in range(pred_probs.size(0)):\n        pred_np = pred_probs[i].cpu().numpy()[0]\n        pred_bin = (pred_np > threshold).astype(np.uint8)\n        processed = postprocess_mask(pred_bin, min_size=100)\n        processed_preds.append(processed)\n    batch_iou = []\n    batch_f1 = []\n    batch_precision = []\n    batch_recall = []\n    for i in range(pred_probs.size(0)):\n        pred = processed_preds[i]\n        gt = masks[i].cpu().numpy()[0]\n        intersection = np.sum(pred * gt)\n        union = np.sum(pred) + np.sum(gt) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        batch_iou.append(iou)\n        precision = intersection / (np.sum(pred) + smooth)\n        recall = intersection / (np.sum(gt) + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        batch_f1.append(f1)\n        batch_precision.append(precision)\n        batch_recall.append(recall)\n    # AUC 계산: ground truth에 0과 1이 모두 존재하지 않으면 0으로 반환\n    probs = pred_probs.cpu().numpy().flatten()\n    masks_np = masks.cpu().numpy().flatten()\n    if np.all(masks_np == 0) or np.all(masks_np == 1):\n        auc_score = 0.0\n    else:\n        try:\n            auc_score = roc_auc_score(masks_np, probs)\n        except Exception:\n            auc_score = 0.0\n    return auc_score, np.mean(batch_iou), np.mean(batch_f1), np.mean(batch_precision), np.mean(batch_recall)\n\ndef compute_hd_metric(outputs, masks, threshold=0.5):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n    masks_np = masks.detach().cpu().numpy()\n    hd_list = []\n    for i in range(outputs.size(0)):\n        pred_bin = (pred_probs[i, 0] > threshold).astype(np.uint8)\n        gt_bin = (masks_np[i, 0] > 0.5).astype(np.uint8)\n        # 두 마스크 모두 비어있으면 hd=0, 한쪽만 비어있으면 hd=100 (임의로 높은 값)\n        if np.sum(gt_bin)==0 and np.sum(pred_bin)==0:\n            hd = 0.0\n        elif np.sum(gt_bin)==0 or np.sum(pred_bin)==0:\n            hd = 100.0\n        else:\n            try:\n                hd = medpy_binary.hd95(pred_bin, gt_bin)\n            except Exception:\n                hd = np.nan\n        hd_list.append(hd)\n    return np.nanmean(hd_list) if len(hd_list) > 0 else 0.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:33:07.601231Z","iopub.execute_input":"2025-03-15T09:33:07.601612Z","iopub.status.idle":"2025-03-15T09:33:07.614894Z","shell.execute_reply.started":"2025-03-15T09:33:07.601588Z","shell.execute_reply":"2025-03-15T09:33:07.613975Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler, autocast\nfrom tqdm import tqdm\nimport copy\n\n# ------------------------------\n# 기본 설정: device, 데이터셋, DataLoader, 모델, optimizer, scheduler\n# ------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 라벨 데이터셋 (BUSI 데이터)\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nlabeled_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\n\n# unlabeled 데이터셋 (경로는 실제 unlabeled 데이터 위치로 수정)\nunlabeled_data_path = '/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/'\nunlabeled_dataset = UnlabeledDataset(unlabeled_data_path, transform=joint_transform)\n\n# DataLoader 설정\nlabeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True, num_workers=0)\nval_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=False, num_workers=0)  # 검증은 라벨 데이터 사용\n\n# 모델 생성 (백본 생성 시 img_size=256 적용되어 있다고 가정)\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\nscaler = GradScaler()\n\n# Early stopping 설정: patience 15\npatience = 15\nbest_val_f1 = 0.0\npatience_counter = 0\n\n# ------------------------------\n# Training Loop: 전체 500 epoch\n# 0~29 epoch: labeled 데이터만 사용\n# 30 epoch부터: pseudo labeling을 생성하여 labeled + pseudo 데이터로 학습\n# ------------------------------\nnum_epochs = 500\ncombined_loader = None  # 이후에 사용할 combined 데이터 로더\n\nfor epoch in range(num_epochs):\n    # 온도 조절 (예: 5.0 -> 1.0로 선형 감소)\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    # 0~29 epoch: labeled_loader 사용, 이후부터 combined_loader 사용\n    if epoch < 30:\n        train_loader = labeled_loader\n    else:\n        # epoch 30에서 한 번만 pseudo labeling 생성\n        if epoch == 30:\n            print(\"===> Generating pseudo labels on unlabeled data...\")\n            pseudo_data = generate_pseudo_labels(model, DataLoader(unlabeled_dataset, batch_size=16, shuffle=False, num_workers=4), device, threshold=0.9)\n            print(f\"Pseudo labels generated for {len(pseudo_data)} images.\")\n            # 라벨 데이터와 pseudo 데이터를 결합하여 CombinedDataset 생성\n            combined_dataset = CombinedDataset(labeled_dataset, pseudo_data)\n            train_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True, num_workers=4)\n            combined_loader = train_loader\n        else:\n            train_loader = combined_loader\n\n    # Training phase\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        bs = images.size(0)\n        epoch_loss += loss.item() * bs\n\n        # 배치별 metric 계산\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples\n    avg_auc = epoch_auc / total_samples\n    avg_iou = epoch_iou / total_samples\n    avg_f1 = epoch_f1 / total_samples\n    avg_precision = epoch_precision / total_samples\n    avg_recall = epoch_recall / total_samples\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    # Validation 평가 (라벨 데이터 사용)\n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(\n        model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n    \n    # Early stopping: validation Dice/F1가 개선되지 않으면 patience_counter 증가\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model_with_pseudo.pth')\n        patience_counter = 0\n        print(\"🍀 New best validation Dice achieved! Dice: {:.4f} 🍀\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"Training complete. Best model saved as 'best_model_with_pseudo.pth'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:33:11.336155Z","iopub.execute_input":"2025-03-15T09:33:11.336493Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/500 Training: 100%|██████████| 49/49 [01:47<00:00,  2.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500 | Loss: 1.0411, AUC: 0.8724, IoU: 0.4025, Dice/F1: 0.4417, Precision: 0.4259, Recall: 0.6278 (Temp: 5.00)\n[Val] Loss: 1.0729, AUC: 0.8207, IoU: 0.4467, Dice/F1: 0.5280, Precision: 0.4490, Recall: 0.7296, HD95: 54.2330\n🍀 New best validation Dice achieved! Dice: 0.5280 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/500 Training: 100%|██████████| 49/49 [01:41<00:00,  2.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/500 | Loss: 0.9619, AUC: 0.9496, IoU: 0.5168, Dice/F1: 0.5436, Precision: 0.5833, Recall: 0.5872 (Temp: 4.99)\n[Val] Loss: 1.0467, AUC: 0.8288, IoU: 0.5386, Dice/F1: 0.5379, Precision: 0.4766, Recall: 0.6975, HD95: 35.4523\n🍀 New best validation Dice achieved! Dice: 0.5379 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/500 Training: 100%|██████████| 49/49 [01:41<00:00,  2.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/500 | Loss: 0.9125, AUC: 0.9472, IoU: 0.5606, Dice/F1: 0.5732, Precision: 0.5921, Recall: 0.6266 (Temp: 4.98)\n[Val] Loss: 0.9464, AUC: 0.8183, IoU: 0.6432, Dice/F1: 0.5992, Precision: 0.7003, Recall: 0.5519, HD95: 28.1701\n🍀 New best validation Dice achieved! Dice: 0.5992 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/500 Training: 100%|██████████| 49/49 [01:41<00:00,  2.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/500 | Loss: 0.8651, AUC: 0.9588, IoU: 0.5884, Dice/F1: 0.5935, Precision: 0.6205, Recall: 0.6257 (Temp: 4.98)\n[Val] Loss: 0.9136, AUC: 0.8297, IoU: 0.6902, Dice/F1: 0.6459, Precision: 0.6442, Recall: 0.6849, HD95: 26.8383\n🍀 New best validation Dice achieved! Dice: 0.6459 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/500 Training: 100%|██████████| 49/49 [01:41<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/500 | Loss: 0.8130, AUC: 0.9564, IoU: 0.6242, Dice/F1: 0.6133, Precision: 0.6348, Recall: 0.6455 (Temp: 4.97)\n[Val] Loss: 0.7970, AUC: 0.8294, IoU: 0.7449, Dice/F1: 0.6575, Precision: 0.6984, Recall: 0.6487, HD95: 17.9238\n🍀 New best validation Dice achieved! Dice: 0.6575 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/500 | Loss: 0.7998, AUC: 0.9636, IoU: 0.6401, Dice/F1: 0.6295, Precision: 0.6456, Recall: 0.6645 (Temp: 4.96)\n[Val] Loss: 0.7831, AUC: 0.8403, IoU: 0.7529, Dice/F1: 0.6628, Precision: 0.6818, Recall: 0.6698, HD95: 17.3736\n🍀 New best validation Dice achieved! Dice: 0.6628 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/500 | Loss: 0.7372, AUC: 0.9723, IoU: 0.6921, Dice/F1: 0.6530, Precision: 0.6800, Recall: 0.6640 (Temp: 4.95)\n[Val] Loss: 0.8010, AUC: 0.8445, IoU: 0.7595, Dice/F1: 0.6768, Precision: 0.6627, Recall: 0.7132, HD95: 16.8521\n🍀 New best validation Dice achieved! Dice: 0.6768 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/500 Training: 100%|██████████| 49/49 [01:41<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/500 | Loss: 0.7493, AUC: 0.9778, IoU: 0.6717, Dice/F1: 0.6528, Precision: 0.6763, Recall: 0.6743 (Temp: 4.94)\n[Val] Loss: 0.7432, AUC: 0.8478, IoU: 0.7865, Dice/F1: 0.6952, Precision: 0.6884, Recall: 0.7191, HD95: 14.5468\n🍀 New best validation Dice achieved! Dice: 0.6952 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/500 | Loss: 0.7089, AUC: 0.9833, IoU: 0.6902, Dice/F1: 0.6624, Precision: 0.6856, Recall: 0.6757 (Temp: 4.94)\n[Val] Loss: 0.6951, AUC: 0.8484, IoU: 0.8020, Dice/F1: 0.7038, Precision: 0.7098, Recall: 0.7136, HD95: 13.0606\n🍀 New best validation Dice achieved! Dice: 0.7038 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/500 | Loss: 0.6972, AUC: 0.9854, IoU: 0.7224, Dice/F1: 0.6941, Precision: 0.7109, Recall: 0.7050 (Temp: 4.93)\n[Val] Loss: 0.6928, AUC: 0.8492, IoU: 0.8072, Dice/F1: 0.7088, Precision: 0.7163, Recall: 0.7158, HD95: 12.6782\n🍀 New best validation Dice achieved! Dice: 0.7088 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/500 | Loss: 0.6853, AUC: 0.9874, IoU: 0.7355, Dice/F1: 0.6881, Precision: 0.7024, Recall: 0.7029 (Temp: 4.92)\n[Val] Loss: 0.6853, AUC: 0.8505, IoU: 0.8136, Dice/F1: 0.7094, Precision: 0.7145, Recall: 0.7191, HD95: 12.0262\n🍀 New best validation Dice achieved! Dice: 0.7094 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/500 | Loss: 0.7033, AUC: 0.9791, IoU: 0.7045, Dice/F1: 0.6580, Precision: 0.6829, Recall: 0.6688 (Temp: 4.91)\n[Val] Loss: 0.8074, AUC: 0.8476, IoU: 0.7770, Dice/F1: 0.7064, Precision: 0.6906, Recall: 0.7375, HD95: 16.9035\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/500 Training:  16%|█▋        | 8/49 [00:16<01:24,  2.07s/it]","output_type":"stream"},{"name":"stdout","text":"[Val] Loss: 0.7330, AUC: 0.8485, IoU: 0.7989, Dice/F1: 0.7055, Precision: 0.7069, Recall: 0.7186, HD95: 13.8151\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/500 Training: 100%|██████████| 49/49 [01:39<00:00,  2.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/500 | Loss: 0.7098, AUC: 0.9848, IoU: 0.6932, Dice/F1: 0.6739, Precision: 0.6992, Recall: 0.6955 (Temp: 4.90)\n[Val] Loss: 0.7033, AUC: 0.8491, IoU: 0.7952, Dice/F1: 0.7062, Precision: 0.7279, Recall: 0.7028, HD95: 14.5662\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/500 Training: 100%|██████████| 49/49 [01:41<00:00,  2.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/500 | Loss: 0.6825, AUC: 0.9858, IoU: 0.7220, Dice/F1: 0.6760, Precision: 0.6935, Recall: 0.6928 (Temp: 4.89)\n[Val] Loss: 0.6796, AUC: 0.8499, IoU: 0.8073, Dice/F1: 0.7103, Precision: 0.7140, Recall: 0.7253, HD95: 14.2156\n🍀 New best validation Dice achieved! Dice: 0.7103 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/500 | Loss: 0.6644, AUC: 0.9865, IoU: 0.7305, Dice/F1: 0.6813, Precision: 0.7016, Recall: 0.6958 (Temp: 4.88)\n[Val] Loss: 0.6798, AUC: 0.8511, IoU: 0.8094, Dice/F1: 0.7163, Precision: 0.7374, Recall: 0.7112, HD95: 13.0505\n🍀 New best validation Dice achieved! Dice: 0.7163 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/500 | Loss: 0.6410, AUC: 0.9900, IoU: 0.7484, Dice/F1: 0.6918, Precision: 0.7100, Recall: 0.7040 (Temp: 4.87)\n[Val] Loss: 0.6598, AUC: 0.8524, IoU: 0.8230, Dice/F1: 0.7236, Precision: 0.7219, Recall: 0.7400, HD95: 11.6106\n🍀 New best validation Dice achieved! Dice: 0.7236 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/500 | Loss: 0.6433, AUC: 0.9891, IoU: 0.7370, Dice/F1: 0.6924, Precision: 0.7157, Recall: 0.7032 (Temp: 4.86)\n[Val] Loss: 0.5870, AUC: 0.8512, IoU: 0.8356, Dice/F1: 0.7288, Precision: 0.7340, Recall: 0.7355, HD95: 10.1023\n🍀 New best validation Dice achieved! Dice: 0.7288 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/500 | Loss: 0.6225, AUC: 0.9905, IoU: 0.7419, Dice/F1: 0.7063, Precision: 0.7294, Recall: 0.7141 (Temp: 4.86)\n[Val] Loss: 0.5731, AUC: 0.8530, IoU: 0.8371, Dice/F1: 0.7276, Precision: 0.7208, Recall: 0.7456, HD95: 10.0082\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/500 | Loss: 0.6024, AUC: 0.9920, IoU: 0.7615, Dice/F1: 0.7171, Precision: 0.7330, Recall: 0.7264 (Temp: 4.85)\n[Val] Loss: 0.5905, AUC: 0.8533, IoU: 0.8385, Dice/F1: 0.7277, Precision: 0.7345, Recall: 0.7331, HD95: 10.3703\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/500 | Loss: 0.6161, AUC: 0.9908, IoU: 0.7480, Dice/F1: 0.7075, Precision: 0.7246, Recall: 0.7233 (Temp: 4.84)\n[Val] Loss: 0.5568, AUC: 0.8536, IoU: 0.8460, Dice/F1: 0.7344, Precision: 0.7464, Recall: 0.7345, HD95: 9.4208\n🍀 New best validation Dice achieved! Dice: 0.7344 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/500 | Loss: 0.5878, AUC: 0.9931, IoU: 0.7615, Dice/F1: 0.7120, Precision: 0.7297, Recall: 0.7199 (Temp: 4.83)\n[Val] Loss: 0.5992, AUC: 0.8538, IoU: 0.8411, Dice/F1: 0.7317, Precision: 0.7225, Recall: 0.7522, HD95: 9.8819\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/500 Training: 100%|██████████| 49/49 [01:41<00:00,  2.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/500 | Loss: 0.5679, AUC: 0.9948, IoU: 0.7809, Dice/F1: 0.7230, Precision: 0.7373, Recall: 0.7299 (Temp: 4.82)\n[Val] Loss: 0.5680, AUC: 0.8539, IoU: 0.8446, Dice/F1: 0.7345, Precision: 0.7248, Recall: 0.7556, HD95: 9.7436\n🍀 New best validation Dice achieved! Dice: 0.7345 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/500 Training: 100%|██████████| 49/49 [01:41<00:00,  2.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/500 | Loss: 0.5712, AUC: 0.9938, IoU: 0.7827, Dice/F1: 0.7211, Precision: 0.7287, Recall: 0.7348 (Temp: 4.82)\n[Val] Loss: 0.5736, AUC: 0.8541, IoU: 0.8534, Dice/F1: 0.7401, Precision: 0.7354, Recall: 0.7545, HD95: 9.4841\n🍀 New best validation Dice achieved! Dice: 0.7401 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/500 | Loss: 0.5525, AUC: 0.9940, IoU: 0.7896, Dice/F1: 0.7353, Precision: 0.7426, Recall: 0.7483 (Temp: 4.81)\n[Val] Loss: 0.5249, AUC: 0.8543, IoU: 0.8551, Dice/F1: 0.7399, Precision: 0.7596, Recall: 0.7321, HD95: 8.7593\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26/500 | Loss: 0.5583, AUC: 0.9938, IoU: 0.7866, Dice/F1: 0.7353, Precision: 0.7504, Recall: 0.7424 (Temp: 4.80)\n[Val] Loss: 0.5313, AUC: 0.8545, IoU: 0.8615, Dice/F1: 0.7450, Precision: 0.7455, Recall: 0.7535, HD95: 8.5376\n🍀 New best validation Dice achieved! Dice: 0.7450 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/500 Training: 100%|██████████| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27/500 | Loss: 0.5379, AUC: 0.9958, IoU: 0.7882, Dice/F1: 0.7248, Precision: 0.7348, Recall: 0.7339 (Temp: 4.79)\n[Val] Loss: 0.5061, AUC: 0.8545, IoU: 0.8627, Dice/F1: 0.7444, Precision: 0.7520, Recall: 0.7450, HD95: 8.0646\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/500 Training:  51%|█████     | 25/49 [00:51<00:48,  2.04s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Best model 가중치 로드 (파일명은 학습 시 저장한 파일명으로 변경)\nmodel.load_state_dict(torch.load(\"best_model_with_pseudo.pth\", map_location=device))\n\n# 모델을 evaluation 모드로 전환\nmodel.eval()\nwith torch.no_grad():\n    test_loss, test_auc, test_iou, test_f1, test_precision, test_recall, test_hd = evaluate_with_metrics(\n        model, test_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight\n    )\n\nprint(f\"[Test] Loss: {test_loss:.4f}, AUC: {test_auc:.4f}, IoU: {test_iou:.4f}, Dice/F1: {test_f1:.4f}, \"\n      f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, HD95: {test_hd:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:47:17.984664Z","iopub.execute_input":"2025-03-15T11:47:17.985067Z","iopub.status.idle":"2025-03-15T11:47:29.951580Z","shell.execute_reply.started":"2025-03-15T11:47:17.985031Z","shell.execute_reply":"2025-03-15T11:47:29.950307Z"}},"outputs":[{"name":"stdout","text":"[Test] Loss: 0.4169, AUC: 0.9989, IoU: 0.8918, Dice/F1: 0.7728, Precision: 0.7755, Recall: 0.7755, HD95: 5.6043\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"a = 1\na","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# full code","metadata":{}},{"cell_type":"code","source":"! pip install medpy\n! pip install segmentation_models_pytorch\n! pip install -U albumentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T08:28:01.485328Z","iopub.execute_input":"2025-03-15T08:28:01.485596Z","iopub.status.idle":"2025-03-15T08:28:27.515898Z","shell.execute_reply.started":"2025-03-15T08:28:01.485562Z","shell.execute_reply":"2025-03-15T08:28:27.515028Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting medpy\n  Downloading medpy-0.5.2.tar.gz (156 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from medpy) (1.13.1)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from medpy) (1.26.4)\nRequirement already satisfied: SimpleITK>=2.1 in /usr/local/lib/python3.10/dist-packages (from medpy) (2.4.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->medpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->medpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->medpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->medpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->medpy) (2024.2.0)\nBuilding wheels for collected packages: medpy\n  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for medpy: filename=MedPy-0.5.2-cp310-cp310-linux_x86_64.whl size=762835 sha256=045e8a1051aaa6bff85809d7c68395da517aa5dae22f0fa368964d5b67899b5f\n  Stored in directory: /root/.cache/pip/wheels/a1/b8/63/bdf557940ec60d1b8822e73ff9fbe7727ac19f009d46b5d175\nSuccessfully built medpy\nInstalling collected packages: medpy\nSuccessfully installed medpy-0.5.2\nCollecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\nCollecting efficientnet-pytorch>=0.6.1 (from segmentation_models_pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.29.0)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (11.0.0)\nCollecting pretrainedmodels>=0.7.1 (from segmentation_models_pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.17.0)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.0.12)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2.4.1)\nCollecting munch (from pretrainedmodels>=0.7.1->segmentation_models_pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9->segmentation_models_pytorch) (0.4.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation_models_pytorch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nDownloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=b8a602d8f8bb6772c8df742c89576c6283dc839dbadb0bba27ea1891a9716ba3\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=aa69f5f0f1582f2a4f6fb5e9d93a70ffd412619058e938e26d91f805e6acb0ab\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, pretrainedmodels, segmentation_models_pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.4.0\nRequirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\nCollecting albumentations\n  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\nCollecting albucore==0.0.23 (from albumentations)\n  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\nCollecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nDownloading albumentations-2.0.5-py3-none-any.whl (290 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\nDownloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\n  Attempting uninstall: albucore\n    Found existing installation: albucore 0.0.19\n    Uninstalling albucore-0.0.19:\n      Successfully uninstalled albucore-0.0.19\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.4.20\n    Uninstalling albumentations-1.4.20:\n      Successfully uninstalled albumentations-1.4.20\nSuccessfully installed albucore-0.0.23 albumentations-2.0.5 simsimd-6.2.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport re\nimport random\nimport warnings\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage.segmentation import find_boundaries\nfrom skimage.measure import label as sk_label, regionprops\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom medpy.metric import binary as medpy_binary\nfrom torchvision.transforms import ColorJitter\n\nimport os\n\n# /kaggle/working/ 경로에 \"unlabeled-busi-images\" 폴더 생성\nfolder_path = '/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/'\nos.makedirs(folder_path, exist_ok=True)\nprint(f\"Folder created: {folder_path}\")\n\n\n# ------------------------------\n# Seed 및 Warning 설정\n# ------------------------------\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\nwarnings.filterwarnings('ignore')\n\n# ------------------------------\n# BUSI Segmentation Dataset\n# ------------------------------\nclass BUSISegmentationDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform  \n        self.samples = []\n        self._prepare_samples()\n\n    def _prepare_samples(self):\n        labels = os.listdir(self.data_path)\n        for label in labels:\n            folder_path = os.path.join(self.data_path, label)\n            if not os.path.isdir(folder_path):\n                continue\n            files = os.listdir(folder_path)\n            image_files = sorted([f for f in files if '_mask' not in f and f.endswith('.png')])\n            mask_files  = sorted([f for f in files if '_mask' in f and f.endswith('.png')])\n            pattern_img = re.compile(rf'{re.escape(label)} \\((\\d+)\\)\\.png')\n            pattern_mask = re.compile(rf'{re.escape(label)} \\((\\d+)\\)_mask(?:_\\d+)?\\.png')\n            mask_dict = {}\n            for mf in mask_files:\n                m = pattern_mask.fullmatch(mf)\n                if m:\n                    idx = m.group(1)\n                    mask_dict.setdefault(idx, []).append(mf)\n            for im in image_files:\n                m = pattern_img.fullmatch(im)\n                if m:\n                    idx = m.group(1)\n                    img_path = os.path.join(folder_path, im)\n                    if idx in mask_dict:\n                        mask_paths = [os.path.join(folder_path, mf) for mf in mask_dict[idx]]\n                        combined_mask = None\n                        for mp in mask_paths:\n                            mask_img = Image.open(mp).convert('L')\n                            mask_arr = np.array(mask_img)\n                            mask_binary = (mask_arr > 128).astype(np.uint8)\n                            if combined_mask is None:\n                                combined_mask = mask_binary\n                            else:\n                                combined_mask = np.maximum(combined_mask, mask_binary)\n                        self.samples.append((img_path, combined_mask, label))\n                    else:\n                        image_pil = Image.open(img_path)\n                        empty_mask = np.zeros(image_pil.size[::-1], dtype=np.uint8)\n                        self.samples.append((img_path, empty_mask, label))\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, mask_array, label = self.samples[index]\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.fromarray((mask_array * 255).astype(np.uint8))\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        return image, mask, label\n\n# ------------------------------\n# Utility Functions\n# ------------------------------\ndef z_score_normalize(tensor):\n    mean = tensor.mean()\n    std = tensor.std() + 1e-6\n    return (tensor - mean) / std\n\ndef joint_transform(image, mask, size=(256,256)):\n    geom_transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=10, p=0.5),\n        A.ElasticTransform(alpha=10, sigma=5, alpha_affine=5, p=0.3),\n        A.Resize(height=size[0], width=size[1])\n    ])\n    image_np = np.array(image)\n    mask_np = np.array(mask)\n    augmented = geom_transform(image=image_np, mask=mask_np)\n    image = augmented['image']\n    mask = augmented['mask']\n    \n    intensity_transform = A.Compose([\n        A.RandomBrightnessContrast(p=0.5),\n        A.CLAHE(clip_limit=1.0, tile_grid_size=(8,8), p=0.5),\n        A.GaussianBlur(p=0.3)\n    ])\n    image = intensity_transform(image=image)['image']\n    \n    image = transforms.ToTensor()(image)\n    image = z_score_normalize(image)\n    mask = transforms.ToTensor()(mask)\n    return image, mask\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(images, masks, alpha=1.5, p=0.7):\n    if np.random.rand() > p:\n        return images, masks\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(images.size(0)).to(images.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n    masks[:, :, bbx1:bbx2, bby1:bby2] = masks[rand_index, :, bbx1:bbx2, bby1:bby2]\n    return images, masks\n\ndef postprocess_mask(mask, min_size=100):\n    # postprocess_mask: 빈 영역을 제거하는 처리\n    labeled_mask = sk_label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n# ------------------------------\n# Pseudo Labeling 관련 Dataset 및 함수\n# ------------------------------\nclass UnlabeledDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform\n        self.image_paths = sorted([os.path.join(data_path, fname) for fname in os.listdir(data_path) if fname.endswith('.png')])\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        img_path = self.image_paths[index]\n        image = Image.open(img_path).convert(\"RGB\")\n        dummy_mask = Image.new(\"L\", image.size, 0)\n        if self.transform:\n            image, _ = self.transform(image, dummy_mask)\n        else:\n            image = transforms.ToTensor()(image)\n        return image, img_path\n\ndef generate_pseudo_labels(model, unlabeled_loader, device, threshold=0.9):\n    model.eval()\n    pseudo_data = []\n    with torch.no_grad():\n        for images, paths in unlabeled_loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = torch.sigmoid(outputs)\n            pseudo_masks = (preds > threshold).float()\n            for i in range(images.size(0)):\n                pseudo_data.append((images[i].cpu(), pseudo_masks[i].cpu()))\n    return pseudo_data\n\nclass CombinedDataset(Dataset):\n    def __init__(self, labeled_dataset, pseudo_data):\n        self.labeled_dataset = labeled_dataset\n        self.pseudo_data = pseudo_data\n\n    def __len__(self):\n        return len(self.labeled_dataset) + len(self.pseudo_data)\n    \n    def __getitem__(self, index):\n        if index < len(self.labeled_dataset):\n            return self.labeled_dataset[index]\n        else:\n            pseudo_index = index - len(self.labeled_dataset)\n            image, mask = self.pseudo_data[pseudo_index]\n            return image, mask, \"pseudo\"\n\n# ------------------------------\n# Model 아키텍처 (PretrainedSwin_UNet_AttentionFusion 등)\n# ------------------------------\n# ※ 모델 관련 코드는 그대로 사용합니다. (필요한 경우 중복된 부분은 제거하였음)\nfrom timm.models import create_model\n\nclass UNetDecoder_Swin(nn.Module):\n    def __init__(self):\n        super(UNetDecoder_Swin, self).__init__()\n        self.up1 = nn.ConvTranspose2d(1536, 768, kernel_size=2, stride=2)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(768 + 768, 768, kernel_size=3, padding=1),\n            nn.BatchNorm2d(768),\n            nn.ReLU(inplace=True)\n        )\n        self.up2 = nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2)\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(384 + 384, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True)\n        )\n        self.up3 = nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2)\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(192 + 192, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n        self.up4 = nn.ConvTranspose2d(192, 64, kernel_size=2, stride=2)\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, features_list):\n        f3, f2, f1, f0 = features_list\n        x = f3\n        x = self.up1(x)\n        x = torch.cat([x, f2], dim=1)\n        x = self.conv1(x)\n        x = self.up2(x)\n        x = torch.cat([x, f1], dim=1)\n        x = self.conv2(x)\n        x = self.up3(x)\n        x = torch.cat([x, f0], dim=1)\n        x = self.conv3(x)\n        x = self.up4(x)\n        x = self.conv4(x)\n        x = F.interpolate(x, size=(256,256), mode='bilinear', align_corners=False)\n        return x\n\nclass TransformerBottleneck(nn.Module):\n    def __init__(self, d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1):\n        super(TransformerBottleneck, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                    dim_feedforward=dim_feedforward, dropout=dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    def forward(self, x):\n        B, C, H, W = x.size()\n        x = x.view(B, C, H * W).permute(2, 0, 1)\n        x = self.transformer(x)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\nclass PretrainedSwin_UNet_AttentionFusion(nn.Module):\n    def __init__(self, out_channels=1):\n        super(PretrainedSwin_UNet_AttentionFusion, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True, img_size=256)\n        self.bottleneck = QuadAgentBlock(in_channels=1536, out_channels=1536, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        features_list = self.encoder(x)  # [f0, f1, f2, f3]\n\n        # (N, H, W, C) → (N, C, H, W) 변환이 필요한 경우 적용\n        for i in range(len(features_list)):\n            if features_list[i].shape[1] not in {192, 384, 768, 1536}:  \n                features_list[i] = features_list[i].permute(0, 3, 1, 2).contiguous()\n\n        # 디코더에 올바른 순서로 전달되도록 변환\n        f0, f1, f2, f3 = features_list  # Swin Transformer의 기본 반환 순서\n        features_list = [f3, f2, f1, f0]  # (B, 1536, 7, 7) → (B, 192, 56, 56)\n\n        # Bottleneck과 Transformer Bottleneck 적용\n        b_out = self.bottleneck(features_list[0])\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        features_list[0] = t_out2\n\n        decoded = self.decoder(features_list)\n        return self.final_conv(decoded)\n\n# ------------------------------\n# Loss 및 Metric 함수\n# ------------------------------\nfrom segmentation_models_pytorch.losses import LovaszLoss\nlv_loss = LovaszLoss(mode='binary')\n\ndef focal_tversky_loss(pred, target, alpha=0.5, beta=0.5, gamma=4/3, smooth=1e-5):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred = pred.view(pred.size(0), -1)\n    target = target.view(target.size(0), -1)\n    tp = (pred * target).sum(dim=1)\n    fp = ((1 - target) * pred).sum(dim=1)\n    fn = (target * (1 - pred)).sum(dim=1)\n    tversky_index = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n    loss = (1 - tversky_index) ** gamma\n    return loss.mean()\n\ndef boundary_loss(pred, target):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_np = pred.detach().cpu().numpy()\n    target_np = target.detach().cpu().numpy()\n    boundary_masks = []\n    for i in range(pred_np.shape[0]):\n        gt_mask = target_np[i, 0]\n        boundary = find_boundaries(gt_mask, mode='thick')\n        boundary_masks.append(boundary.astype(np.float32))\n    boundary_masks = np.stack(boundary_masks, axis=0)[:, None, :, :]\n    boundary_masks_torch = torch.from_numpy(boundary_masks).to(pred.device)\n    intersect = (pred * boundary_masks_torch).sum()\n    denom = pred.sum() + boundary_masks_torch.sum()\n    boundary_dice = (2.0 * intersect) / (denom + 1e-5)\n    return 1.0 - boundary_dice\n\ndef combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    loss_ft = focal_tversky_loss(outputs, masks)\n    if pos_weight is not None:\n        loss_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, masks.float())\n    else:\n        loss_bce = nn.BCEWithLogitsLoss()(outputs, masks.float())\n    bl = boundary_loss(outputs, masks)\n    lv = lv_loss(outputs, masks)\n    return 0.4 * lv + 0.3 * loss_ft + 0.3 * loss_bce + lambda_boundary * bl\n\ndef dice_f1_precision_recall(pred, target, threshold=0.5, smooth=1e-5):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    precision = intersection / (pred_bin.sum() + smooth)\n    recall = intersection / (target_bin.sum() + smooth)\n    f1 = 2 * (precision * recall) / (precision + recall + smooth)\n    return f1.item(), precision.item(), recall.item()\n\ndef iou_metric(pred, target, threshold=0.5, smooth=1e-5):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    union = pred_bin.sum() + target_bin.sum() - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-5):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach()\n    processed_preds = []\n    for i in range(pred_probs.size(0)):\n        pred_np = pred_probs[i].cpu().numpy()[0]\n        pred_bin = (pred_np > threshold).astype(np.uint8)\n        processed = postprocess_mask(pred_bin, min_size=100)\n        processed_preds.append(processed)\n    batch_iou = []\n    batch_f1 = []\n    batch_precision = []\n    batch_recall = []\n    for i in range(pred_probs.size(0)):\n        pred = processed_preds[i]\n        gt = masks[i].cpu().numpy()[0]\n        intersection = np.sum(pred * gt)\n        union = np.sum(pred) + np.sum(gt) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        batch_iou.append(iou)\n        precision = intersection / (np.sum(pred) + smooth)\n        recall = intersection / (np.sum(gt) + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        batch_f1.append(f1)\n        batch_precision.append(precision)\n        batch_recall.append(recall)\n    probs = pred_probs.cpu().numpy().flatten()\n    masks_np = masks.cpu().numpy().flatten()\n    # AUC 계산: 만약 ground truth에 0 또는 1만 존재하면 0을 반환\n    if np.all(masks_np == 0) or np.all(masks_np == 1):\n        auc_score = 0.0\n    else:\n        try:\n            auc_score = roc_auc_score(masks_np, probs)\n        except ValueError:\n            auc_score = 0.0\n    return auc_score, np.mean(batch_iou), np.mean(batch_f1), np.mean(batch_precision), np.mean(batch_recall)\n\ndef compute_hd_metric(outputs, masks, threshold=0.5):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n    masks_np = masks.detach().cpu().numpy()\n    hd_list = []\n    for i in range(outputs.size(0)):\n        pred_bin = (pred_probs[i, 0] > threshold).astype(np.uint8)\n        gt_bin = (masks_np[i, 0] > 0.5).astype(np.uint8)\n        try:\n            hd = medpy_binary.hd95(pred_bin, gt_bin)\n        except Exception:\n            hd = np.nan\n        hd_list.append(hd)\n    return np.nanmean(hd_list)\n\ndef evaluate_with_metrics(model, dataloader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None):\n    model.eval()\n    total_loss = 0.0\n    total_auc  = 0.0\n    total_iou  = 0.0\n    total_f1   = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    total_hd = 0.0\n    total_samples = 0\n    with torch.no_grad():\n        for images, masks, _ in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=lambda_boundary)\n            auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=threshold, smooth=1e-5)\n            hd_val = compute_hd_metric(outputs, masks, threshold=threshold)\n            bs = images.size(0)\n            total_loss += loss.item() * bs\n            total_auc  += auc_score * bs\n            total_iou  += iou_val * bs\n            total_f1   += f1_val * bs\n            total_precision += prec * bs\n            total_recall += rec * bs\n            total_hd += hd_val * bs\n            total_samples += bs\n    avg_loss = total_loss / total_samples\n    avg_auc  = total_auc / total_samples\n    avg_iou  = total_iou / total_samples\n    avg_f1   = total_f1 / total_samples\n    avg_precision = total_precision / total_samples\n    avg_recall = total_recall / total_samples\n    avg_hd = total_hd / total_samples\n    return avg_loss, avg_auc, avg_iou, avg_f1, avg_precision, avg_recall, avg_hd\n\n# ------------------------------\n# TTA 함수 (Test Time Augmentation)\n# ------------------------------\ndef tta_predict(model, image, device):\n    cj = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n    def add_noise(x, std=0.05):\n        noise = torch.randn_like(x) * std\n        return x + noise\n    def identity(x): return x\n    def hflip(x): return torch.flip(x, dims=[-1])\n    def vflip(x): return torch.flip(x, dims=[-2])\n    def rot90(x): return torch.rot90(x, k=1, dims=[-2, -1])\n    def inv_hflip(x): return torch.flip(x, dims=[-1])\n    def inv_vflip(x): return torch.flip(x, dims=[-2])\n    def inv_rot90(x): return torch.rot90(x, k=3, dims=[-2, -1])\n    \n    transforms_list = [\n        (identity, identity),\n        (hflip, inv_hflip),\n        (vflip, inv_vflip),\n        (rot90, inv_rot90),\n        (lambda x: add_noise(cj(x)), lambda x: x)\n    ]\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for aug, inv in transforms_list:\n            augmented = aug(image)\n            output = model(augmented.unsqueeze(0).to(device))\n            output = torch.sigmoid(output)\n            output = inv(output).cpu()\n            predictions.append(output)\n    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n    return avg_prediction\n\n# ------------------------------\n# 데이터 로더 설정 (테스트 포함)\n# ------------------------------\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nfull_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\nindices = np.arange(len(full_dataset))\ntrain_val_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\ntrain_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=42)\ntrain_dataset = Subset(full_dataset, train_idx)\nval_dataset = Subset(full_dataset, val_idx)\ntest_dataset = Subset(full_dataset, test_idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0,\n                          worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id))\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n\ndef calculate_pos_weight(loader):\n    total_pixels = 0\n    positive_pixels = 0\n    for images, masks, _ in loader:\n        positive_pixels += masks.sum().item()\n        total_pixels += masks.numel()\n    negative_pixels = total_pixels - positive_pixels\n    return torch.tensor(negative_pixels / (positive_pixels + 1e-6)).to(device)\n\n# ------------------------------\n# 모델, Optimizer, Scheduler 설정 (Training Loop)\n# ------------------------------\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\npos_weight = calculate_pos_weight(train_loader)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n\nscaler = GradScaler()\npatience = 50\nbest_val_f1 = 0.0\npatience_counter = 0\n\nmin_temp = 2.0\nmax_temp = 5.0\n\nnum_epochs = 500\ncombined_loader = None\n\nfor epoch in range(num_epochs):\n    current_temp = max_temp - ((max_temp - min_temp) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    if epoch < 30:\n        train_loader = train_loader  # labeled_loader 사용\n    else:\n        if epoch == 30:\n            print(\"===> Generating pseudo labels on unlabeled data...\")\n            pseudo_loader = DataLoader(UnlabeledDataset('/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/', transform=joint_transform),\n                                       batch_size=16, shuffle=False, num_workers=0)\n            pseudo_data = generate_pseudo_labels(model, pseudo_loader, device, threshold=0.9)\n            print(f\"Pseudo labels generated for {len(pseudo_data)} images.\")\n            combined_dataset = CombinedDataset(train_dataset, pseudo_data)\n            combined_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True, num_workers=0)\n            train_loader = combined_loader\n        else:\n            train_loader = combined_loader\n\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6)\n        if torch.isnan(loss):\n            print(\"Warning: loss is NaN, skipping batch\")\n            continue\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n        scaler.step(optimizer)\n        scaler.update()\n\n        bs = images.size(0)\n        epoch_loss += loss.item() * bs\n\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-5)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples if total_samples > 0 else float('nan')\n    avg_auc = epoch_auc / total_samples if total_samples > 0 else float('nan')\n    avg_iou = epoch_iou / total_samples if total_samples > 0 else float('nan')\n    avg_f1 = epoch_f1 / total_samples if total_samples > 0 else float('nan')\n    avg_precision = epoch_precision / total_samples if total_samples > 0 else float('nan')\n    avg_recall = epoch_recall / total_samples if total_samples > 0 else float('nan')\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n    \n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model_with_pseudo.pth')\n        patience_counter = 0\n        print(\"🍀 New best validation Dice achieved! Dice: {:.4f} 🍀\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"Training complete. Best model saved as 'best_model_with_pseudo.pth'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T08:29:50.130748Z","iopub.execute_input":"2025-03-15T08:29:50.131102Z","iopub.status.idle":"2025-03-15T08:32:24.231181Z","shell.execute_reply.started":"2025-03-15T08:29:50.131072Z","shell.execute_reply":"2025-03-15T08:32:24.229959Z"}},"outputs":[{"name":"stdout","text":"Folder created: /kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/500 Training: 100%|██████████| 30/30 [01:23<00:00,  2.79s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500 | Loss: 1.0333, AUC: 0.8713, IoU: 0.3501, Dice/F1: 0.4121, Precision: 0.4011, Recall: 0.5522 (Temp: 5.00)\n[Val] Loss: 1.0126, AUC: 0.9479, IoU: 0.4751, Dice/F1: 0.5198, Precision: 0.4828, Recall: 0.6790, HD95: 66.5344\n🍀 New best validation Dice achieved! Dice: 0.5198 🍀\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/500 Training:  37%|███▋      | 11/30 [00:25<00:43,  2.29s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-aaa0dd000a28>\u001b[0m in \u001b[0;36m<cell line: 570>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mauc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_batch_metrics_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0mepoch_auc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mauc_score\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mepoch_iou\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0miou_val\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-aaa0dd000a28>\u001b[0m in \u001b[0;36mcompute_batch_metrics_new\u001b[0;34m(outputs, masks, threshold, smooth)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mauc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mauc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         return _average_binary_score(\n\u001b[0m\u001b[1;32m    573\u001b[0m             \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_fpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_fpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    342\u001b[0m         )\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \"\"\"\n\u001b[0;32m--> 992\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;31m# sort scores and corresponding truth values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m     \u001b[0mdesc_score_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mergesort\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \"\"\"\n\u001b[0;32m-> 1133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}