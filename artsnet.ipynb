{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5084161,"sourceType":"datasetVersion","datasetId":2952181}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install medpy\n! pip install segmentation_models_pytorch\n! pip install -U albumentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:09:56.132331Z","iopub.execute_input":"2025-03-15T09:09:56.132564Z","iopub.status.idle":"2025-03-15T09:10:23.015317Z","shell.execute_reply.started":"2025-03-15T09:09:56.132543Z","shell.execute_reply":"2025-03-15T09:10:23.014443Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting medpy\n  Downloading medpy-0.5.2.tar.gz (156 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from medpy) (1.13.1)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from medpy) (1.26.4)\nRequirement already satisfied: SimpleITK>=2.1 in /usr/local/lib/python3.10/dist-packages (from medpy) (2.4.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->medpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->medpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->medpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->medpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->medpy) (2024.2.0)\nBuilding wheels for collected packages: medpy\n  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for medpy: filename=MedPy-0.5.2-cp310-cp310-linux_x86_64.whl size=762838 sha256=2c68dc339f4d6d539b85b98aa7e9995c8ee22c1df3a233041dc8a130462ca04f\n  Stored in directory: /root/.cache/pip/wheels/a1/b8/63/bdf557940ec60d1b8822e73ff9fbe7727ac19f009d46b5d175\nSuccessfully built medpy\nInstalling collected packages: medpy\nSuccessfully installed medpy-0.5.2\nCollecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\nCollecting efficientnet-pytorch>=0.6.1 (from segmentation_models_pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.29.0)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (11.0.0)\nCollecting pretrainedmodels>=0.7.1 (from segmentation_models_pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.17.0)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.0.12)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2.4.1)\nCollecting munch (from pretrainedmodels>=0.7.1->segmentation_models_pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9->segmentation_models_pytorch) (0.4.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation_models_pytorch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nDownloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=6eb3dc58d16839ca14aa8959db787aaf5ff89c2b480796097bae995991974573\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=efa82239403fab7a3841436313a83816e45114f981ab844a6fbe54a7cd7822f9\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, pretrainedmodels, segmentation_models_pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.4.0\nRequirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\nCollecting albumentations\n  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\nCollecting albucore==0.0.23 (from albumentations)\n  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\nCollecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nDownloading albumentations-2.0.5-py3-none-any.whl (290 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\nDownloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\n  Attempting uninstall: albucore\n    Found existing installation: albucore 0.0.19\n    Uninstalling albucore-0.0.19:\n      Successfully uninstalled albucore-0.0.19\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.4.20\n    Uninstalling albumentations-1.4.20:\n      Successfully uninstalled albumentations-1.4.20\nSuccessfully installed albucore-0.0.23 albumentations-2.0.5 simsimd-6.2.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport copy\nimport glob\nimport random\nimport warnings\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport cv2  # CLAHE Ï†ÅÏö©ÏùÑ ÏúÑÌï¥ OpenCV ÏÇ¨Ïö©\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nimport torchvision.models as models\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage.segmentation import find_boundaries\nfrom skimage.measure import label, regionprops\nimport scipy.ndimage\n\n# Albumentations Í∏∞Î∞ò augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# medpy HD metric\nfrom medpy.metric import binary as medpy_binary\n\n###############################################\n# Seed Î∞è Warning ÏÑ§Ï†ï\n###############################################\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\nwarnings.filterwarnings('ignore')\n\n###############################################\n# BUSI Segmentation Dataset\n###############################################\nclass BUSISegmentationDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform  \n        self.samples = []  # Î∞òÎìúÏãú Ï¥àÍ∏∞Ìôî\n        self._prepare_samples()\n\n    def _prepare_samples(self):\n        labels = os.listdir(self.data_path)\n        for label in labels:\n            folder_path = os.path.join(self.data_path, label)\n            if not os.path.isdir(folder_path):\n                continue\n            files = os.listdir(folder_path)\n            image_files = sorted([f for f in files if '_mask' not in f and f.endswith('.png')])\n            mask_files  = sorted([f for f in files if '_mask' in f and f.endswith('.png')])\n            pattern_img = re.compile(rf'{re.escape(label)} \\((\\d+)\\)\\.png')\n            pattern_mask = re.compile(rf'{re.escape(label)} \\((\\d+)\\)_mask(?:_\\d+)?\\.png')\n            mask_dict = {}\n            for mf in mask_files:\n                m = pattern_mask.fullmatch(mf)\n                if m:\n                    idx = m.group(1)\n                    mask_dict.setdefault(idx, []).append(mf)\n            for im in image_files:\n                m = pattern_img.fullmatch(im)\n                if m:\n                    idx = m.group(1)\n                    img_path = os.path.join(folder_path, im)\n                    if idx in mask_dict:\n                        mask_paths = [os.path.join(folder_path, mf) for mf in mask_dict[idx]]\n                        combined_mask = None\n                        for mp in mask_paths:\n                            mask_img = Image.open(mp).convert('L')\n                            mask_arr = np.array(mask_img)\n                            mask_binary = (mask_arr > 128).astype(np.uint8)\n                            if combined_mask is None:\n                                combined_mask = mask_binary\n                            else:\n                                combined_mask = np.maximum(combined_mask, mask_binary)\n                        self.samples.append((img_path, combined_mask, label))\n                    else:\n                        image_pil = Image.open(img_path)\n                        empty_mask = np.zeros(image_pil.size[::-1], dtype=np.uint8)\n                        self.samples.append((img_path, empty_mask, label))\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, mask_array, label = self.samples[index]\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.fromarray((mask_array * 255).astype(np.uint8))\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        return image, mask, label\n\n###############################################\n# Per-image Z-score normalization (adaptive)\n###############################################\ndef z_score_normalize(tensor):\n    # tensor: (C, H, W)\n    mean = tensor.mean()\n    std = tensor.std() + 1e-6\n    return (tensor - mean) / std\n\n###############################################\n# CLAHE Augmentation Ìï®Ïàò (OpenCV)\n###############################################\ndef apply_clahe(image, clipLimit=2.0, tileGridSize=(8,8)):\n    img_np = np.array(image)\n    if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl, a, b))\n        final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    else:\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        final = clahe.apply(img_np)\n    return Image.fromarray(final)\n\n###############################################\n# joint_transform (Albumentations Í∏∞Î∞ò Augmentation)\n###############################################\ndef joint_transform(image, mask, size=(224,224)):\n    # 1. Geometric transforms (imageÏôÄ maskÏóê ÎèôÏãúÏóê Ï†ÅÏö©)\n    geom_transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=15, p=0.5),\n        A.ElasticTransform(alpha=100, sigma=10, alpha_affine=15, p=0.5),\n        A.Resize(height=size[0], width=size[1])\n    ])\n    image_np = np.array(image)\n    mask_np = np.array(mask)\n    augmented = geom_transform(image=image_np, mask=mask_np)\n    image = augmented['image']\n    mask = augmented['mask']\n    \n    # 2. Intensity transforms (Ïò§ÏßÅ imageÏóêÎßå Ï†ÅÏö©)\n    intensity_transform = A.Compose([\n        A.RandomBrightnessContrast(p=0.5),\n        A.CLAHE(clip_limit=3.0, p=0.5),\n        A.GaussianBlur(p=0.3)\n    ])\n    image = intensity_transform(image=image)['image']\n    \n    # 3. ToTensor Î∞è per-image z-score normalization (imageÏóê Ï†ÅÏö©)\n    image = transforms.ToTensor()(image)\n    image = z_score_normalize(image)\n    mask = transforms.ToTensor()(mask)\n    return image, mask\n\n###############################################\n# CutMix Ìï®Ïàò (Í∞ïÌôî: alpha=1.5, p=0.7)\n###############################################\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(images, masks, alpha=1.5, p=0.7):\n    if np.random.rand() > p:\n        return images, masks\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(images.size(0)).to(images.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n    masks[:, :, bbx1:bbx2, bby1:bby2] = masks[rand_index, :, bbx1:bbx2, bby1:bby2]\n    return images, masks\n\n###############################################\n# Advanced ÌõÑÏ≤òÎ¶¨: Morphological Closing Ìè¨Ìï®\n###############################################\ndef postprocess_mask(mask, min_size=100):\n    # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏûëÏùÄ Í∞ùÏ≤¥ Ï†úÍ±∞\n    labeled_mask = label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    # Morphological closing (hole filling)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n###############################################\n# 1. AttentionGate Î™®Îìà\n###############################################\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(AttentionGate, self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi\n\n###############################################\n# 2. MultiScaleFusion Î™®Îìà\n###############################################\nclass MultiScaleFusion(nn.Module):\n    def __init__(self, in_channels_list, out_channels):\n        super(MultiScaleFusion, self).__init__()\n        self.convs = nn.ModuleList([nn.Conv2d(ch, out_channels, kernel_size=1) for ch in in_channels_list])\n        self.fuse = nn.Sequential(\n            nn.Conv2d(len(in_channels_list)*out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, features):\n        target_size = features[-1].size()[2:]\n        processed = []\n        for i, f in enumerate(features):\n            if f.size()[2:] != target_size:\n                f = F.interpolate(f, size=target_size, mode='bilinear', align_corners=False)\n            f = self.convs[i](f)\n            processed.append(f)\n        out = torch.cat(processed, dim=1)\n        out = self.fuse(out)\n        return out\n\n###############################################\n# 3. Í∏∞ÌÉÄ Î™®Îç∏ Í¥ÄÎ†® Î™®Îìà\n###############################################\nclass StochasticDepth(nn.Module):\n    def __init__(self, p, mode=\"row\"):\n        super(StochasticDepth, self).__init__()\n        self.p = p\n        self.mode = mode\n\n    def forward(self, x):\n        if not self.training or self.p == 0.0:\n            return x\n        survival_rate = 1 - self.p\n        if self.mode == \"row\":\n            batch_size = x.shape[0]\n            noise = torch.rand(batch_size, 1, 1, 1, device=x.device, dtype=x.dtype)\n            binary_mask = (noise < survival_rate).float()\n            return x / survival_rate * binary_mask\n        else:\n            if torch.rand(1).item() < self.p:\n                return torch.zeros_like(x)\n            else:\n                return x\n\nclass GhostModule(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, ratio=2, dw_kernel_size=3, stride=1, padding=0, use_relu=True):\n        super(GhostModule, self).__init__()\n        self.out_channels = out_channels\n        self.primary_channels = int(torch.ceil(torch.tensor(out_channels / ratio)))\n        self.cheap_channels = out_channels - self.primary_channels\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(in_channels, self.primary_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(self.primary_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n        self.cheap_conv = nn.Sequential(\n            nn.Conv2d(self.primary_channels, self.cheap_channels, dw_kernel_size, stride=1,\n                      padding=dw_kernel_size//2, groups=self.primary_channels, bias=False),\n            nn.BatchNorm2d(self.cheap_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_conv(x1)\n        out = torch.cat([x1, x2], dim=1)\n        return out[:, :self.out_channels, :, :].contiguous()\n\ndef ghost_conv_block(in_channels, out_channels, use_relu=True):\n    return nn.Sequential(\n        GhostModule(in_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu),\n        GhostModule(out_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu)\n    )\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\nclass DynamicGating(nn.Module):\n    def __init__(self, num_branches, hidden_dim=64, dropout_prob=0.1, init_temperature=2.0, iterations=3):\n        super(DynamicGating, self).__init__()\n        self.temperature = init_temperature\n        self.iterations = iterations\n        self.dropout_prob = dropout_prob\n        self.fc = nn.Sequential(\n            nn.Linear(num_branches, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, num_branches)\n        )\n        self.layernorm = nn.LayerNorm(num_branches)\n        self.softmax = nn.Softmax(dim=1)\n        self.res_scale = nn.Parameter(torch.ones(1))\n        self.saved_log_probs = None\n        self.entropy = None\n        self.rl_loss = 0.0\n\n    def forward(self, features):\n        norm_features = self.layernorm(features)\n        logits = self.fc(norm_features)\n        logits = self.layernorm(logits)\n        scaled_logits = logits / self.temperature\n        gates = self.softmax(scaled_logits)\n        for _ in range(self.iterations - 1):\n            updated_features = norm_features + self.res_scale * gates\n            logits = self.fc(updated_features)\n            logits = self.layernorm(logits)\n            scaled_logits = logits / self.temperature\n            new_gates = self.softmax(scaled_logits)\n            gates = 0.5 * gates + 0.5 * new_gates\n        return gates\n\n###############################################\n###############################################\n###############################################\n\nclass QuadAgentBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5):\n        super().__init__()\n        branch_channels = out_channels // 4\n        self.branch1 = GhostModule(in_channels, branch_channels, ratio=4)\n        # branch2: kernel_size=3, padding=1Î°ú spatial size Ïú†ÏßÄ\n        self.branch2 = GhostModule(in_channels, branch_channels, kernel_size=3, padding=1, ratio=4)\n        self.branch3 = nn.Sequential(\n            GhostModule(in_channels, branch_channels, ratio=4),\n            SELayer(branch_channels, reduction=32)\n        )\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        # num_branchesÎ•º 3ÏúºÎ°ú ÏÑ§Ï†ï\n        self.gating = DynamicGating(num_branches=3, hidden_dim=gating_hidden_dim, dropout_prob=gating_dropout, init_temperature=gating_temperature)\n        self.fusion_conv = GhostModule(branch_channels * 3, out_channels, ratio=2)\n        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False) if in_channels != out_channels else nn.Identity()\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob)\n\n    def forward(self, x):\n        b1 = self.branch1(x)\n        b2 = self.branch2(x)\n        b3 = self.branch3(x)\n        gap_b1 = self.gap(b1).view(x.size(0), -1)\n        gap_b2 = self.gap(b2).view(x.size(0), -1)\n        gap_b3 = self.gap(b3).view(x.size(0), -1)\n        features = torch.stack([gap_b1.mean(dim=1), gap_b2.mean(dim=1), gap_b3.mean(dim=1)], dim=1)\n        gates = self.gating(features)  # (B, 3)\n        b1 = b1 * gates[:, 0].view(-1, 1, 1, 1)\n        b2 = b2 * gates[:, 1].view(-1, 1, 1, 1)\n        b3 = b3 * gates[:, 2].view(-1, 1, 1, 1)\n        out = torch.cat([b1, b2, b3], dim=1)\n        out = self.fusion_conv(out)\n        res = self.res_conv(x)\n        res = self.stochastic_depth(res)\n        return out + res\n\n\n\"\"\"\nüî• Í≤∞Í≥º Í∏∞ÎåÄ Ìö®Í≥º\nÍ∞úÏÑ† Î∞©Î≤ï\tÏÑ§Î™Ö\tÍ∏∞ÎåÄ Ìö®Í≥º\nÎ∏åÎûúÏπò Í∞úÏàò Ï∂ïÏÜå\t4Í∞ú ‚Üí 3Í∞úÎ°ú Î≥ÄÍ≤Ω\tÏó∞ÏÇ∞Îüâ ÏïΩ 25% Ï†àÍ∞ê\nDilation ‚Üí Dynamic Conv\tDepthwise Ï†ÅÏö©\tÍ≤ΩÎüâÌôî Î∞è ÏÑ±Îä• Ïú†ÏßÄ\nSE ‚Üí Lightweight SE\tÏ±ÑÎÑê Ï∂ïÏÜå ÌõÑ Ï†ÅÏö©\tÏÑ±Îä• Ïú†ÏßÄ, Î©îÎ™®Î¶¨ Ï†àÍ∞ê\nGating Í∞úÏÑ†\tSoftmax ‚Üí Gumbel-Softmax\tÌïôÏäµ ÏÜçÎèÑ Î∞è Í≤åÏù¥ÌåÖ ÏÑ±Îä• Ìñ•ÏÉÅ\nFusion ÏµúÏ†ÅÌôî\tConv1x1 ‚Üí GhostFusion\tÏó∞ÏÇ∞Îüâ Ï†àÍ∞ê, ÏÑ±Îä• Ïú†ÏßÄ\n\n\"\"\"\n###############################################\n###############################################\n###############################################\n\n\n###############################################\n# 4. PretrainedResNetUNet_AttentionFusion (Ï†ÑÏ≤¥ Î™®Îç∏)\n###############################################\n###############################################\n# EfficientNet-B0 Backbone Í∏∞Î∞ò UNet with Attention + Lite Bottleneck\n###############################################\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm  # timm ÎùºÏù¥Î∏åÎü¨Î¶¨ (ViT Î∞±Î≥∏)\nfrom medpy.metric import binary as medpy_binary\n\n###############################################\n# Transformer Bottleneck (Ï∂îÍ∞Ä)\n###############################################\nclass TransformerBottleneck(nn.Module):\n    def __init__(self, d_model=768, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1):\n        super(TransformerBottleneck, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n                                                    dim_feedforward=dim_feedforward, dropout=dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    def forward(self, x):\n        # x: (B, C, H, W) -> (HW, B, C)\n        B, C, H, W = x.size()\n        x = x.view(B, C, H*W).permute(2, 0, 1)\n        x = self.transformer(x)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\n###############################################\n# UNet Decoder (Í∞ÑÎã®Ìïú Decoder)\n###############################################\nclass UNetDecoder(nn.Module):\n    def __init__(self):\n        super(UNetDecoder, self).__init__()\n        self.up1 = nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True)\n        )\n        self.up2 = nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2)\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n        self.up3 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(96, 96, kernel_size=3, padding=1),\n            nn.BatchNorm2d(96),\n            nn.ReLU(inplace=True)\n        )\n        self.up4 = nn.ConvTranspose2d(96, 64, kernel_size=2, stride=2)\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        x = self.up1(x)\n        x = self.conv1(x)\n        x = self.up2(x)\n        x = self.conv2(x)\n        x = self.up3(x)\n        x = self.conv3(x)\n        x = self.up4(x)\n        x = self.conv4(x)\n        return x\n\n###############################################\n# DARKViT_UNet: ViT Í∏∞Î∞ò Encoder + QuadAgentBlock + Transformer Bottleneck + UNet Decoder\n###############################################\nfrom timm.models import create_model\n\nclass DARKViT_UNet(nn.Module):\n    def __init__(self, out_channels=1):\n        super(DARKViT_UNet, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True)\n        self.bottleneck = QuadAgentBlock(in_channels=1024, out_channels=1024, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1024, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1024, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n    \n    def forward(self, x):\n        features_list = self.encoder(x)  # features_list: [f0, f1, f2, f3]\n        # f3Í∞Ä NHWC ÌòïÏãùÎ°ú Î∞òÌôòÎêòÎäî Í≤ΩÏö∞ Ï∞®Ïõê Î≥ÄÌôò\n        f3 = features_list[-1].permute(0, 3, 1, 2)  # (B,1024,H,W)\n        b_out = self.bottleneck(f3)\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        # ÎßåÏïΩ decoderÍ∞Ä NCHW ÌòïÏãùÏùÑ ÏöîÍµ¨ÌïúÎã§Î©¥, features_list ÎÇò f3Îßå ÍµêÏ≤¥Ìï©ÎãàÎã§.\n        features_list[-1] = t_out2\n        decoded = self.decoder(features_list)\n        return self.final_conv(decoded)\n\n\n\n###############################################\n# 5. Loss & Metric Functions\n###############################################\nfrom segmentation_models_pytorch.losses import LovaszLoss\nlv_loss = LovaszLoss(mode='binary')\n\ndef focal_tversky_loss(pred, target, alpha=0.5, beta=0.5, gamma=4/3, smooth=1e-6):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred = pred.view(pred.size(0), -1)\n    target = target.view(target.size(0), -1)\n    tp = (pred * target).sum(dim=1)\n    fp = ((1 - target) * pred).sum(dim=1)\n    fn = (target * (1 - pred)).sum(dim=1)\n    tversky_index = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n    loss = (1 - tversky_index) ** gamma\n    return loss.mean()\n\ndef boundary_loss(pred, target):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_np = pred.detach().cpu().numpy()\n    target_np = target.detach().cpu().numpy()\n    boundary_masks = []\n    for i in range(pred_np.shape[0]):\n        gt_mask = target_np[i, 0]\n        boundary = find_boundaries(gt_mask, mode='thick')\n        boundary_masks.append(boundary.astype(np.float32))\n    boundary_masks = np.stack(boundary_masks, axis=0)[:, None, :, :]\n    boundary_masks_torch = torch.from_numpy(boundary_masks).to(pred.device)\n    intersect = (pred * boundary_masks_torch).sum()\n    denom = pred.sum() + boundary_masks_torch.sum()\n    boundary_dice = (2.0 * intersect) / (denom + 1e-6)\n    return 1.0 - boundary_dice\n\ndef combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    loss_ft = focal_tversky_loss(outputs, masks)\n    if pos_weight is not None:\n        loss_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, masks.float())\n    else:\n        loss_bce = nn.BCEWithLogitsLoss()(outputs, masks.float())\n    bl = boundary_loss(outputs, masks)\n    lv = lv_loss(outputs, masks)\n    return 0.4 * lv + 0.3 * loss_ft + 0.3 * loss_bce + lambda_boundary * bl\n\ndef dice_f1_precision_recall(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    precision = intersection / (pred_bin.sum() + smooth)\n    recall = intersection / (target_bin.sum() + smooth)\n    f1 = 2 * (precision * recall) / (precision + recall + smooth)\n    return f1.item(), precision.item(), recall.item()\n\ndef iou_metric(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    union = pred_bin.sum() + target_bin.sum() - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef postprocess_mask(mask, min_size=100):\n    labeled_mask = label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    # Kernel ÌÅ¨Í∏∞Î•º (7,7)Î°ú ÎäòÎ†§ ÌõÑÏ≤òÎ¶¨ Í∞ïÌôî\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7,7))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n\ndef compute_hd_metric(outputs, masks, threshold=0.5):\n    # outputsÏôÄ masksÎ•º ÌõÑÏ≤òÎ¶¨ÌïòÏó¨ binary maskÎ°ú Î≥ÄÌôòÌïú ÌõÑ, Í∞Å sampleÎßàÎã§ HD (hd95)Î•º Í≥ÑÏÇ∞\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n    masks_np = masks.detach().cpu().numpy()\n    hd_list = []\n    for i in range(outputs.size(0)):\n        pred_bin = (pred_probs[i, 0] > threshold).astype(np.uint8)\n        gt_bin = (masks_np[i, 0] > 0.5).astype(np.uint8)\n        try:\n            hd = medpy_binary.hd95(pred_bin, gt_bin)\n        except Exception:\n            hd = np.nan\n        hd_list.append(hd)\n    return np.nanmean(hd_list)\n\n\ndef compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach()\n    processed_preds = []\n    for i in range(pred_probs.size(0)):\n        pred_np = pred_probs[i].cpu().numpy()[0]\n        pred_bin = (pred_np > threshold).astype(np.uint8)\n        processed = postprocess_mask(pred_bin, min_size=100)\n        processed_preds.append(processed)\n    batch_iou = []\n    batch_f1 = []\n    batch_precision = []\n    batch_recall = []\n    for i in range(pred_probs.size(0)):\n        pred = processed_preds[i]\n        gt = masks[i].cpu().numpy()[0]\n        intersection = np.sum(pred * gt)\n        union = np.sum(pred) + np.sum(gt) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        batch_iou.append(iou)\n        precision = intersection / (np.sum(pred) + smooth)\n        recall = intersection / (np.sum(gt) + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        batch_f1.append(f1)\n        batch_precision.append(precision)\n        batch_recall.append(recall)\n    probs = pred_probs.cpu().numpy().flatten()\n    masks_np = masks.cpu().numpy().flatten()\n    try:\n        auc_score = roc_auc_score(masks_np, probs)\n    except ValueError:\n        auc_score = float('nan')\n    return auc_score, np.mean(batch_iou), np.mean(batch_f1), np.mean(batch_precision), np.mean(batch_recall)\n\ndef find_optimal_threshold_iou(model, dataloader, device, smooth=1e-6):\n    thresholds = np.linspace(0.1, 0.9, 17)  # 0.05 Í∞ÑÍ≤© ÏÑ∏Î∞ÄÌïú ÌÉêÏÉâ\n    best_threshold = 0.65\n    best_iou = 0.0\n    model.eval()\n    with torch.no_grad():\n        for t in thresholds:\n            total_iou = 0.0\n            sample_count = 0\n            for images, masks, _ in dataloader:\n                images = images.to(device)\n                masks = masks.to(device)\n                outputs = model(images)\n                pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n                masks_np = masks.cpu().numpy()\n                for i in range(outputs.size(0)):\n                    pred_bin = (pred_probs[i, 0] > t).astype(np.uint8)\n                    pred_processed = postprocess_mask(pred_bin, min_size=100)\n                    gt = masks_np[i, 0]\n                    intersection = np.sum(pred_processed * gt)\n                    union = np.sum(pred_processed) + np.sum(gt) - intersection\n                    iou_val = (intersection + smooth) / (union + smooth)\n                    total_iou += iou_val\n                    sample_count += 1\n            avg_iou = total_iou / sample_count if sample_count > 0 else 0\n            if avg_iou > best_iou:\n                best_iou = avg_iou\n                best_threshold = t\n    return best_threshold, best_iou\n\ndef evaluate_with_metrics(model, dataloader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None):\n    model.eval()\n    total_loss = 0.0\n    total_auc  = 0.0\n    total_iou  = 0.0\n    total_f1   = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    total_hd = 0.0  # HD Ìï©Í≥Ñ\n    total_samples = 0\n    with torch.no_grad():\n        for images, masks, _ in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=lambda_boundary)\n            auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=threshold, smooth=1e-6)\n            hd_val = compute_hd_metric(outputs, masks, threshold=threshold)\n            bs = images.size(0)\n            total_loss += loss.item() * bs\n            total_auc  += auc_score * bs\n            total_iou  += iou_val * bs\n            total_f1   += f1_val * bs\n            total_precision += prec * bs\n            total_recall += rec * bs\n            total_hd += hd_val * bs\n            total_samples += bs\n    avg_loss = total_loss / total_samples\n    avg_auc  = total_auc / total_samples\n    avg_iou  = total_iou / total_samples\n    avg_f1   = total_f1 / total_samples\n    avg_precision = total_precision / total_samples\n    avg_recall = total_recall / total_samples\n    avg_hd = total_hd / total_samples\n    return avg_loss, avg_auc, avg_iou, avg_f1, avg_precision, avg_recall, avg_hd\n\n\n###############################################\n# Hausdorff Distance (HD) Í≥ÑÏÇ∞ Ìï®Ïàò\n###############################################\ndef hausdorff_distance(pred, target):\n    # pred, target: numpy binary masks\n    return medpy_binary.hd95(pred, target)\n\n###############################################\n# TTA (Test Time Augmentation) Ìï®Ïàò\n###############################################\ndef tta_predict(model, image, device):\n    # image: single image tensor, shape (C, H, W)\n    def identity(x): return x\n    def hflip(x): return torch.flip(x, dims=[-1])\n    def vflip(x): return torch.flip(x, dims=[-2])\n    def rot90(x): return torch.rot90(x, k=1, dims=[-2, -1])\n    # Ïó≠Î≥ÄÌôò Ìï®Ïàò: flipÏùÄ ÎèôÏùºÌïú Ïó∞ÏÇ∞ÏúºÎ°ú Î≥µÍµ¨, rot90Ïùò Í≤ΩÏö∞ 3Ìöå ÌöåÏ†ÑÌïòÎ©¥ ÏõêÏÉÅÎ≥µÍµ¨\n    def inv_hflip(x): return torch.flip(x, dims=[-1])\n    def inv_vflip(x): return torch.flip(x, dims=[-2])\n    def inv_rot90(x): return torch.rot90(x, k=3, dims=[-2, -1])\n    \n    transforms_list = [\n        (identity, identity),\n        (hflip, inv_hflip),\n        (vflip, inv_vflip),\n        (rot90, inv_rot90)\n    ]\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for aug, inv in transforms_list:\n            augmented = aug(image)\n            output = model(augmented.unsqueeze(0).to(device))\n            output = torch.sigmoid(output)\n            output = inv(output).cpu()\n            predictions.append(output)\n    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n    return avg_prediction\n\n###############################################\n# Optimizer Scheduler Ìï®Ïàò\n###############################################\ndef get_lambda_rl(epoch):\n    return min(0.1, epoch / 100.0)\n\ndef get_lambda_entropy(epoch):\n    return max(0.01, 0.1 - (epoch / 100.0))\n\nclass WarmupCosineScheduler:\n    def __init__(self, optimizer, warmup_steps, max_steps, max_lr, min_lr=1e-6):\n        self.optimizer = optimizer\n        self.warmup_steps = warmup_steps\n        self.max_steps = max_steps\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.global_step = 0\n\n    def step(self):\n        self.global_step += 1\n        if self.global_step < self.warmup_steps:\n            lr = self.max_lr * (self.global_step / self.warmup_steps)\n        else:\n            progress = (self.global_step - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n            lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + math.cos(math.pi * progress))\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n\n###############################################\n# Îç∞Ïù¥ÌÑ∞ Î°úÎçî Î∞è BUSI Dataset ÏÑ§Ï†ï\n###############################################\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nfull_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\nindices = np.arange(len(full_dataset))\ntrain_val_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\ntrain_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=42)\ntrain_dataset = Subset(full_dataset, train_idx)\nval_dataset   = Subset(full_dataset, val_idx)\ntest_dataset  = Subset(full_dataset, test_idx)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=16, shuffle=True, num_workers=4,\n    worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id),\n    pin_memory=True, persistent_workers=True\n)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\n\n###############################################\n# Î™®Îç∏, Optimizer, Scheduler ÏÑ§Ï†ï\n###############################################\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = DARKViT_UNet(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\ndef calculate_pos_weight(loader):\n    total_pixels = 0\n    positive_pixels = 0\n    for images, masks, _ in loader:\n        positive_pixels += masks.sum().item()\n        total_pixels += masks.numel()\n    negative_pixels = total_pixels - positive_pixels\n    return torch.tensor(negative_pixels / (positive_pixels + 1e-6)).to(device)\n\npos_weight = calculate_pos_weight(train_loader)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nnum_epochs = 500\n\nscheduler = WarmupCosineScheduler(\n    optimizer, warmup_steps=10, max_steps=num_epochs,\n    max_lr=1e-4, min_lr=1e-5\n)\n\n###############################################\n# Training Loop\n###############################################\nfrom torch.cuda.amp import GradScaler, autocast\nscaler = GradScaler()\npatience = 20\nbest_val_f1 = 0.0\npatience_counter = 0\n\nfor epoch in range(num_epochs):\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    current_lambda_rl = get_lambda_rl(epoch)\n    current_lambda_entropy = get_lambda_entropy(epoch)\n\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        # CutMix ÌôúÏÑ±Ìôî (alpha=1.5, p=0.7)\n        images, masks = cutmix_data(images, masks, alpha=1.5, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss_seg = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=0.2, lambda_lovasz=0.6)\n            policy_loss = torch.tensor(0.0, device=loss_seg.device)  # RL ÎØ∏ÏÇ¨Ïö©\n            loss_total = loss_seg + current_lambda_rl * policy_loss\n        scaler.scale(loss_total).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        bs = images.size(0)\n        epoch_loss += loss_total.item() * bs\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step()\n    avg_loss = epoch_loss / total_samples\n    avg_auc = epoch_auc / total_samples\n    avg_iou = epoch_iou / total_samples\n    avg_f1 = epoch_f1 / total_samples\n    avg_precision = epoch_precision / total_samples\n    avg_recall = epoch_recall / total_samples\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n\n    \n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model.pth')\n        patience_counter = 0\n        print(\"üçÄ New best validation Dice achieved! Dice: {:.4f} üçÄ\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T07:55:04.347638Z","iopub.execute_input":"2025-03-15T07:55:04.347913Z","iopub.status.idle":"2025-03-15T07:55:24.081646Z","shell.execute_reply.started":"2025-03-15T07:55:04.347891Z","shell.execute_reply":"2025-03-15T07:55:24.080443Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/788M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55eb104de88541fa8b011108cc884012"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-0e5fde135e12>\u001b[0m in \u001b[0;36m<cell line: 819>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;31m###############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDARKViT_UNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-0e5fde135e12>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, out_channels)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_bottleneck1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerBottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_bottleneck2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerBottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNetDecoder_Swin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'UNetDecoder_Swin' is not defined"],"ename":"NameError","evalue":"name 'UNetDecoder_Swin' is not defined","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"test_loss, test_auc, test_iou, test_f1, test_precision, test_recall, test_hd = evaluate_with_metrics(model, test_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\nprint(f\"[Test] Loss: {test_loss:.4f}, AUC: {test_auc:.4f}, IoU: {test_iou:.4f}, Dice/F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, HD95: {test_hd:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T03:49:51.938392Z","iopub.execute_input":"2025-03-14T03:49:51.938690Z","iopub.status.idle":"2025-03-14T03:49:59.689957Z","shell.execute_reply.started":"2025-03-14T03:49:51.938665Z","shell.execute_reply":"2025-03-14T03:49:59.688693Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"[Test] Loss: 1.0922, AUC: 0.9580, IoU: 0.6721, Dice/F1: 0.6004, Precision: 0.6322, Recall: 0.6067, HD95: 23.9212\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# DARK-Swin","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport copy\nimport glob\nimport random\nimport warnings\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport cv2  # CLAHE Ï†ÅÏö©ÏùÑ ÏúÑÌï¥ OpenCV ÏÇ¨Ïö©\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage.segmentation import find_boundaries\nfrom skimage.measure import label, regionprops\nimport scipy.ndimage\n\n# Albumentations Í∏∞Î∞ò augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# medpy HD metric\nfrom medpy.metric import binary as medpy_binary\nfrom torchvision.transforms import ColorJitter\n\n###############################################\n# Seed Î∞è Warning ÏÑ§Ï†ï\n###############################################\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\nwarnings.filterwarnings('ignore')\n\n###############################################\n# BUSI Segmentation Dataset\n###############################################\nclass BUSISegmentationDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform  \n        self.samples = []  # Î∞òÎìúÏãú Ï¥àÍ∏∞Ìôî\n        self._prepare_samples()\n\n    def _prepare_samples(self):\n        labels = os.listdir(self.data_path)\n        for label in labels:\n            folder_path = os.path.join(self.data_path, label)\n            if not os.path.isdir(folder_path):\n                continue\n            files = os.listdir(folder_path)\n            image_files = sorted([f for f in files if '_mask' not in f and f.endswith('.png')])\n            mask_files  = sorted([f for f in files if '_mask' in f and f.endswith('.png')])\n            pattern_img = re.compile(rf'{re.escape(label)} \\((\\d+)\\)\\.png')\n            pattern_mask = re.compile(rf'{re.escape(label)} \\((\\d+)\\)_mask(?:_\\d+)?\\.png')\n            mask_dict = {}\n            for mf in mask_files:\n                m = pattern_mask.fullmatch(mf)\n                if m:\n                    idx = m.group(1)\n                    mask_dict.setdefault(idx, []).append(mf)\n            for im in image_files:\n                m = pattern_img.fullmatch(im)\n                if m:\n                    idx = m.group(1)\n                    img_path = os.path.join(folder_path, im)\n                    if idx in mask_dict:\n                        mask_paths = [os.path.join(folder_path, mf) for mf in mask_dict[idx]]\n                        combined_mask = None\n                        for mp in mask_paths:\n                            mask_img = Image.open(mp).convert('L')\n                            mask_arr = np.array(mask_img)\n                            mask_binary = (mask_arr > 128).astype(np.uint8)\n                            if combined_mask is None:\n                                combined_mask = mask_binary\n                            else:\n                                combined_mask = np.maximum(combined_mask, mask_binary)\n                        self.samples.append((img_path, combined_mask, label))\n                    else:\n                        image_pil = Image.open(img_path)\n                        empty_mask = np.zeros(image_pil.size[::-1], dtype=np.uint8)\n                        self.samples.append((img_path, empty_mask, label))\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, mask_array, label = self.samples[index]\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.fromarray((mask_array * 255).astype(np.uint8))\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        return image, mask, label\n\n###############################################\n# Per-image Z-score normalization (adaptive)\n###############################################\ndef z_score_normalize(tensor):\n    mean = tensor.mean()\n    std = tensor.std() + 1e-6\n    return (tensor - mean) / std\n\n###############################################\n# CLAHE Augmentation Ìï®Ïàò (OpenCV)\n###############################################\ndef apply_clahe(image, clipLimit=2.0, tileGridSize=(8,8)):\n    img_np = np.array(image)\n    if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl, a, b))\n        final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    else:\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        final = clahe.apply(img_np)\n    return Image.fromarray(final)\n\n###############################################\n# joint_transform (Albumentations Í∏∞Î∞ò Augmentation)\n###############################################\ndef joint_transform(image, mask, size=(224,224)):\n    geom_transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=10, p=0.5),\n        A.ElasticTransform(alpha=10, sigma=5, alpha_affine=5, p=0.3),\n        A.Resize(height=size[0], width=size[1])\n    ])\n    image_np = np.array(image)\n    mask_np = np.array(mask)\n    augmented = geom_transform(image=image_np, mask=mask_np)\n    image = augmented['image']\n    mask = augmented['mask']\n    \n    intensity_transform = A.Compose([\n        A.RandomBrightnessContrast(p=0.5),\n        #A.CLAHE(clip_limit=3.0, p=0.5),\n        A.CLAHE(clip_limit=1.0, tile_grid_size=(8,8), p=0.5),\n        A.GaussianBlur(p=0.3)\n    ])\n    image = intensity_transform(image=image)['image']\n    \n    image = transforms.ToTensor()(image)\n    image = z_score_normalize(image)\n    mask = transforms.ToTensor()(mask)\n    return image, mask\n\n###############################################\n# CutMix Ìï®Ïàò (alpha=1.5, p=0.7)\n###############################################\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(images, masks, alpha=1.5, p=0.7):\n    if np.random.rand() > p:\n        return images, masks\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(images.size(0)).to(images.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n    masks[:, :, bbx1:bbx2, bby1:bby2] = masks[rand_index, :, bbx1:bbx2, bby1:bby2]\n    return images, masks\n\n###############################################\n# Advanced ÌõÑÏ≤òÎ¶¨: Morphological Closing (Kernel 9√ó9)\n###############################################\ndef postprocess_mask(mask, min_size=100):\n    labeled_mask = label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n###############################################\n# AttentionGate Î™®Îìà\n###############################################\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(AttentionGate, self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi\n\n###############################################\n# MultiScaleFusion Î™®Îìà\n###############################################\nclass MultiScaleFusion(nn.Module):\n    def __init__(self, in_channels_list, out_channels):\n        super(MultiScaleFusion, self).__init__()\n        self.convs = nn.ModuleList([nn.Conv2d(ch, out_channels, kernel_size=1) for ch in in_channels_list])\n        self.fuse = nn.Sequential(\n            nn.Conv2d(len(in_channels_list) * out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, features):\n        target_size = features[-1].size()[2:]\n        processed = []\n        for i, f in enumerate(features):\n            if f.size()[2:] != target_size:\n                f = F.interpolate(f, size=target_size, mode='bilinear', align_corners=False)\n            f = self.convs[i](f)\n            processed.append(f)\n        out = torch.cat(processed, dim=1)\n        out = self.fuse(out)\n        return out\n\n###############################################\n# Í∏∞ÌÉÄ Î™®Îç∏ Í¥ÄÎ†® Î™®Îìà (StochasticDepth, GhostModule, SELayer, DynamicGating)\n###############################################\nclass StochasticDepth(nn.Module):\n    def __init__(self, p, mode=\"row\"):\n        super(StochasticDepth, self).__init__()\n        self.p = p\n        self.mode = mode\n\n    def forward(self, x):\n        if not self.training or self.p == 0.0:\n            return x\n        survival_rate = 1 - self.p\n        if self.mode == \"row\":\n            batch_size = x.shape[0]\n            noise = torch.rand(batch_size, 1, 1, 1, device=x.device, dtype=x.dtype)\n            binary_mask = (noise < survival_rate).float()\n            return x / survival_rate * binary_mask\n        else:\n            if torch.rand(1).item() < self.p:\n                return torch.zeros_like(x)\n            else:\n                return x\n\nclass GhostModule(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, ratio=2, dw_kernel_size=3, stride=1, padding=0, use_relu=True):\n        super(GhostModule, self).__init__()\n        self.out_channels = out_channels\n        self.primary_channels = int(torch.ceil(torch.tensor(out_channels / ratio)))\n        self.cheap_channels = out_channels - self.primary_channels\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(in_channels, self.primary_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(self.primary_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n        self.cheap_conv = nn.Sequential(\n            nn.Conv2d(self.primary_channels, self.cheap_channels, dw_kernel_size, stride=1,\n                      padding=dw_kernel_size // 2, groups=self.primary_channels, bias=False),\n            nn.BatchNorm2d(self.cheap_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_conv(x1)\n        out = torch.cat([x1, x2], dim=1)\n        return out[:, :self.out_channels, :, :].contiguous()\n\ndef ghost_conv_block(in_channels, out_channels, use_relu=True):\n    return nn.Sequential(\n        GhostModule(in_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu),\n        GhostModule(out_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu)\n    )\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\nclass DynamicGating(nn.Module):\n    def __init__(self, num_branches, hidden_dim=64, dropout_prob=0.1, init_temperature=2.0, iterations=3):\n        super(DynamicGating, self).__init__()\n        self.temperature = init_temperature\n        self.iterations = iterations\n        self.dropout_prob = dropout_prob\n        self.fc = nn.Sequential(\n            nn.Linear(num_branches, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, num_branches)\n        )\n        self.layernorm = nn.LayerNorm(num_branches)\n        self.softmax = nn.Softmax(dim=1)\n        self.res_scale = nn.Parameter(torch.ones(1))\n        self.saved_log_probs = None\n        self.entropy = None\n        self.rl_loss = 0.0\n\n    def forward(self, features):\n        norm_features = self.layernorm(features)\n        logits = self.fc(norm_features)\n        logits = self.layernorm(logits)\n        scaled_logits = logits / self.temperature\n        gates = self.softmax(scaled_logits)\n        for _ in range(self.iterations - 1):\n            updated_features = norm_features + self.res_scale * gates\n            logits = self.fc(updated_features)\n            logits = self.layernorm(logits)\n            scaled_logits = logits / self.temperature\n            new_gates = self.softmax(scaled_logits)\n            gates = 0.5 * gates + 0.5 * new_gates\n        return gates\n\n###############################################\n# QuadAgentBlock (Lite Î≤ÑÏ†Ñ, 3 Î∏åÎûúÏπò)\n###############################################\nclass QuadAgentBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5):\n        super().__init__()\n        branch_channels = out_channels // 4\n        self.branch1 = GhostModule(in_channels, branch_channels, ratio=4)\n        self.branch2 = GhostModule(in_channels, branch_channels, kernel_size=3, padding=1, ratio=4)\n        self.branch3 = nn.Sequential(\n            GhostModule(in_channels, branch_channels, ratio=4),\n            SELayer(branch_channels, reduction=32)\n        )\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.gating = DynamicGating(num_branches=3, hidden_dim=gating_hidden_dim, dropout_prob=gating_dropout, init_temperature=gating_temperature)\n        self.fusion_conv = GhostModule(branch_channels * 3, out_channels, ratio=2)\n        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False) if in_channels != out_channels else nn.Identity()\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob)\n\n    def forward(self, x):\n        b1 = self.branch1(x)\n        b2 = self.branch2(x)\n        b3 = self.branch3(x)\n        gap_b1 = self.gap(b1).view(x.size(0), -1)\n        gap_b2 = self.gap(b2).view(x.size(0), -1)\n        gap_b3 = self.gap(b3).view(x.size(0), -1)\n        features = torch.stack([gap_b1.mean(dim=1), gap_b2.mean(dim=1), gap_b3.mean(dim=1)], dim=1)\n        gates = self.gating(features)\n        b1 = b1 * gates[:, 0].view(-1, 1, 1, 1)\n        b2 = b2 * gates[:, 1].view(-1, 1, 1, 1)\n        b3 = b3 * gates[:, 2].view(-1, 1, 1, 1)\n        out = torch.cat([b1, b2, b3], dim=1)\n        out = self.fusion_conv(out)\n        res = self.res_conv(x)\n        res = self.stochastic_depth(res)\n        return out + res\n\n\n###############################################\n# DARKViT_UNet: ViT Í∏∞Î∞ò Encoder + QuadAgentBlock + Transformer Bottleneck + UNet Decoder\n###############################################\nfrom timm.models import create_model\n\nclass DARKViT_UNet(nn.Module):\n    def __init__(self, out_channels=1):\n        super(DARKViT_UNet, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True)\n        self.bottleneck = QuadAgentBlock(in_channels=1536, out_channels=1536, \n                                         gating_dropout=0.3, gating_hidden_dim=32, \n                                         gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, \n                                                             dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, \n                                                             dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n    \n    def forward(self, x):\n        # encoderÏóêÏÑú feature map Î¶¨Ïä§Ìä∏ ÏñªÍ∏∞: [f0, f1, f2, f3]\n        features = self.encoder(x)\n        \n        # ÎßåÏïΩ Í∞Å feature mapÏù¥ (N, H, W, C) ÌòïÏãùÏù¥ÎùºÎ©¥ (N, C, H, W)Î°ú Î≥ÄÌôò\n        for i in range(len(features)):\n            # f0: 192, f1:384, f2:768, f3:1536 Ï±ÑÎÑêÏù¥Ïñ¥Ïïº Ìï®\n            if features[i].shape[1] not in {192, 384, 768, 1536}:\n                features[i] = features[i].permute(0, 3, 1, 2).contiguous()\n        \n        # Î∞òÌôòÎêú ÏàúÏÑúÎäî [f0, f1, f2, f3]Ïù¥ÎØÄÎ°ú, decoderÍ∞Ä Í∏∞ÎåÄÌïòÎäî [f3, f2, f1, f0] ÏàúÏÑúÎ°ú Ïû¨Ï†ïÎ†¨\n        f0, f1, f2, f3 = features  \n        features_reordered = [f3, f2, f1, f0]\n        \n        # Ïù¥Ï†ú features_reordered[0]Îäî f3: (B,1536,7,7)\n        b_out = self.bottleneck(features_reordered[0])\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        features_reordered[0] = t_out2\n        \n        decoded = self.decoder(features_reordered)\n        return self.final_conv(decoded)\n\n\n\n###############################################\n# UNet Decoder for Swin Backbone\n###############################################\nclass UNetDecoder_Swin(nn.Module):\n    def __init__(self):\n        super(UNetDecoder_Swin, self).__init__()\n        # Swin‚ÄëLarge feature sizes (Í∞ÄÏ†ï):\n        # f0: (B,192,56,56), f1: (B,384,28,28), f2: (B,768,14,14), f3: (B,1536,7,7)\n        self.up1 = nn.ConvTranspose2d(1536, 768, kernel_size=2, stride=2)  # 7->14\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(768 + 768, 768, kernel_size=3, padding=1),\n            nn.BatchNorm2d(768),\n            nn.ReLU(inplace=True)\n        )\n        self.up2 = nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2)   # 14->28\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(384 + 384, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True)\n        )\n        self.up3 = nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2)   # 28->56\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(192 + 192, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n        self.up4 = nn.ConvTranspose2d(192, 64, kernel_size=2, stride=2)    # 56->112\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, features_list):\n        # features_list: [f0, f1, f2, f3]\n        f3, f2, f1, f0 = features_list  # ‚úÖ ÏàúÏÑú ÎßûÏ∂îÍ∏∞\n        x = f3  # (B,1536,7,7)\n        x = self.up1(x)  # -> (B,768,14,14)\n        x = torch.cat([x, f2], dim=1)  # f2: (B,768,14,14) ‚Üí (B,1536,14,14)\n        x = self.conv1(x)  # -> (B,768,14,14)\n        x = self.up2(x)  # -> (B,384,28,28)\n        x = torch.cat([x, f1], dim=1)  # f1: (B,384,28,28) ‚Üí (B,768,28,28)\n        x = self.conv2(x)  # -> (B,384,28,28)\n        x = self.up3(x)  # -> (B,192,56,56)\n        x = torch.cat([x, f0], dim=1)  # f0: (B,192,56,56) ‚Üí (B,384,56,56)\n        x = self.conv3(x)  # -> (B,192,56,56)\n        x = self.up4(x)  # -> (B,64,112,112)\n        x = self.conv4(x)  # -> (B,64,112,112)\n        x = F.interpolate(x, size=(224,224), mode='bilinear', align_corners=False)\n        return x\n\n###############################################\n# Transformer Bottleneck (Ïó∞ÏÜç Îëê Í∞ú Ï†ÅÏö©)\n###############################################\nclass TransformerBottleneck(nn.Module):\n    def __init__(self, d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1):\n        super(TransformerBottleneck, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                    dim_feedforward=dim_feedforward, dropout=dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    def forward(self, x):\n        B, C, H, W = x.size()\n        x = x.view(B, C, H * W).permute(2, 0, 1)\n        x = self.transformer(x)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\n###############################################\n# Swin‚ÄëTransformer Backbone Í∏∞Î∞ò UNet with Attention + Lite Bottleneck\n###############################################\nfrom timm.models import create_model\n\nclass PretrainedSwin_UNet_AttentionFusion(nn.Module):\n    def __init__(self, out_channels=1):\n        super(PretrainedSwin_UNet_AttentionFusion, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True)\n        self.bottleneck = QuadAgentBlock(in_channels=1536, out_channels=1536, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        features_list = self.encoder(x)  # [f0, f1, f2, f3]\n        \n        # ÎîîÎ≤ÑÍπÖ: Feature Map Shape ÌôïÏù∏\n        #print(\"Original feature shapes:\")\n        #for i, f in enumerate(features_list):\n        #    print(f\"f{i}: {f.shape}\")\n\n        # (N, H, W, C) ‚Üí (N, C, H, W) Î≥ÄÌôòÏù¥ ÌïÑÏöîÌïú Í≤ΩÏö∞ Ï†ÅÏö©\n        for i in range(len(features_list)):\n            if features_list[i].shape[1] not in {192, 384, 768, 1536}:  \n                features_list[i] = features_list[i].permute(0, 3, 1, 2).contiguous()\n\n        # ÎîîÏΩîÎçîÏóê Ïò¨Î∞îÎ•∏ ÏàúÏÑúÎ°ú Ï†ÑÎã¨ÎêòÎèÑÎ°ù Î≥ÄÌôò\n        f0, f1, f2, f3 = features_list  # Swin TransformerÏùò Í∏∞Î≥∏ Î∞òÌôò ÏàúÏÑú\n        features_list = [f3, f2, f1, f0]  # (B, 1536, 7, 7) ‚Üí (B, 192, 56, 56)\n\n        # ÎîîÎ≤ÑÍπÖ: Î≥ÄÌôò ÌõÑ Feature Map Shape ÌôïÏù∏\n        #print(\"\\nReordered feature shapes (for decoder):\")\n        #for i, f in enumerate(features_list):\n        #    print(f\"features_list[{i}]: {f.shape}\")\n\n        # BottleneckÍ≥º Transformer Bottleneck Ï†ÅÏö©\n        b_out = self.bottleneck(features_list[0])  # f3Í∞Ä Îì§Ïñ¥Í∞ÄÏïº Ìï®\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        features_list[0] = t_out2  # BottleneckÏùÑ Í±∞Ïπú f3\n\n        # UNet ÎîîÏΩîÎçî Ï†ÅÏö©\n        decoded = self.decoder(features_list)\n        return self.final_conv(decoded)\n\n\n\n\n###############################################\n# Loss & Metric Functions (HD Ìè¨Ìï®)\n###############################################\nfrom segmentation_models_pytorch.losses import LovaszLoss\nlv_loss = LovaszLoss(mode='binary')\n\ndef focal_tversky_loss(pred, target, alpha=0.5, beta=0.5, gamma=4/3, smooth=1e-6):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred = pred.view(pred.size(0), -1)\n    target = target.view(target.size(0), -1)\n    tp = (pred * target).sum(dim=1)\n    fp = ((1 - target) * pred).sum(dim=1)\n    fn = (target * (1 - pred)).sum(dim=1)\n    tversky_index = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n    loss = (1 - tversky_index) ** gamma\n    return loss.mean()\n\ndef boundary_loss(pred, target):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_np = pred.detach().cpu().numpy()\n    target_np = target.detach().cpu().numpy()\n    boundary_masks = []\n    for i in range(pred_np.shape[0]):\n        gt_mask = target_np[i, 0]\n        boundary = find_boundaries(gt_mask, mode='thick')\n        boundary_masks.append(boundary.astype(np.float32))\n    boundary_masks = np.stack(boundary_masks, axis=0)[:, None, :, :]\n    boundary_masks_torch = torch.from_numpy(boundary_masks).to(pred.device)\n    intersect = (pred * boundary_masks_torch).sum()\n    denom = pred.sum() + boundary_masks_torch.sum()\n    boundary_dice = (2.0 * intersect) / (denom + 1e-6)\n    return 1.0 - boundary_dice\n\ndef combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    loss_ft = focal_tversky_loss(outputs, masks)\n    if pos_weight is not None:\n        loss_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, masks.float())\n    else:\n        loss_bce = nn.BCEWithLogitsLoss()(outputs, masks.float())\n    bl = boundary_loss(outputs, masks)\n    lv = lv_loss(outputs, masks)\n    return 0.4 * lv + 0.3 * loss_ft + 0.3 * loss_bce + lambda_boundary * bl\n\ndef dice_f1_precision_recall(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    precision = intersection / (pred_bin.sum() + smooth)\n    recall = intersection / (target_bin.sum() + smooth)\n    f1 = 2 * (precision * recall) / (precision + recall + smooth)\n    return f1.item(), precision.item(), recall.item()\n\ndef iou_metric(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    union = pred_bin.sum() + target_bin.sum() - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach()\n    processed_preds = []\n    for i in range(pred_probs.size(0)):\n        pred_np = pred_probs[i].cpu().numpy()[0]\n        pred_bin = (pred_np > threshold).astype(np.uint8)\n        processed = postprocess_mask(pred_bin, min_size=100)\n        processed_preds.append(processed)\n    batch_iou = []\n    batch_f1 = []\n    batch_precision = []\n    batch_recall = []\n    for i in range(pred_probs.size(0)):\n        pred = processed_preds[i]\n        gt = masks[i].cpu().numpy()[0]\n        intersection = np.sum(pred * gt)\n        union = np.sum(pred) + np.sum(gt) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        batch_iou.append(iou)\n        precision = intersection / (np.sum(pred) + smooth)\n        recall = intersection / (np.sum(gt) + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        batch_f1.append(f1)\n        batch_precision.append(precision)\n        batch_recall.append(recall)\n    probs = pred_probs.cpu().numpy().flatten()\n    masks_np = masks.cpu().numpy().flatten()\n    try:\n        auc_score = roc_auc_score(masks_np, probs)\n    except ValueError:\n        auc_score = float('nan')\n    return auc_score, np.mean(batch_iou), np.mean(batch_f1), np.mean(batch_precision), np.mean(batch_recall)\n\ndef hausdorff_distance(pred, target):\n    return medpy_binary.hd95(pred, target)\n\ndef compute_hd_metric(outputs, masks, threshold=0.5):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n    masks_np = masks.detach().cpu().numpy()\n    hd_list = []\n    for i in range(outputs.size(0)):\n        pred_bin = (pred_probs[i, 0] > threshold).astype(np.uint8)\n        gt_bin = (masks_np[i, 0] > 0.5).astype(np.uint8)\n        try:\n            hd = medpy_binary.hd95(pred_bin, gt_bin)\n        except Exception:\n            hd = np.nan\n        hd_list.append(hd)\n    return np.nanmean(hd_list)\n\ndef evaluate_with_metrics(model, dataloader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None):\n    model.eval()\n    total_loss = 0.0\n    total_auc  = 0.0\n    total_iou  = 0.0\n    total_f1   = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    total_hd = 0.0\n    total_samples = 0\n    with torch.no_grad():\n        for images, masks, _ in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=lambda_boundary)\n            auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=threshold, smooth=1e-6)\n            hd_val = compute_hd_metric(outputs, masks, threshold=threshold)\n            bs = images.size(0)\n            total_loss += loss.item() * bs\n            total_auc  += auc_score * bs\n            total_iou  += iou_val * bs\n            total_f1   += f1_val * bs\n            total_precision += prec * bs\n            total_recall += rec * bs\n            total_hd += hd_val * bs\n            total_samples += bs\n    avg_loss = total_loss / total_samples\n    avg_auc  = total_auc / total_samples\n    avg_iou  = total_iou / total_samples\n    avg_f1   = total_f1 / total_samples\n    avg_precision = total_precision / total_samples\n    avg_recall = total_recall / total_samples\n    avg_hd = total_hd / total_samples\n    return avg_loss, avg_auc, avg_iou, avg_f1, avg_precision, avg_recall, avg_hd\n\n###############################################\n# TTA (Test Time Augmentation) Ìï®Ïàò (Noise & ColorJitter Ï∂îÍ∞Ä)\n###############################################\ndef tta_predict(model, image, device):\n    cj = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n    def add_noise(x, std=0.05):\n        noise = torch.randn_like(x) * std\n        return x + noise\n    def identity(x): return x\n    def hflip(x): return torch.flip(x, dims=[-1])\n    def vflip(x): return torch.flip(x, dims=[-2])\n    def rot90(x): return torch.rot90(x, k=1, dims=[-2, -1])\n    def inv_hflip(x): return torch.flip(x, dims=[-1])\n    def inv_vflip(x): return torch.flip(x, dims=[-2])\n    def inv_rot90(x): return torch.rot90(x, k=3, dims=[-2, -1])\n    \n    transforms_list = [\n        (identity, identity),\n        (hflip, inv_hflip),\n        (vflip, inv_vflip),\n        (rot90, inv_rot90),\n        (lambda x: add_noise(cj(x)), lambda x: x)\n    ]\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for aug, inv in transforms_list:\n            augmented = aug(image)\n            output = model(augmented.unsqueeze(0).to(device))\n            output = torch.sigmoid(output)\n            output = inv(output).cpu()\n            predictions.append(output)\n    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n    return avg_prediction\n\n###############################################\n# Optimizer Scheduler (SGDR: CosineAnnealingWarmRestarts)\n###############################################\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\n###############################################\n# Îç∞Ïù¥ÌÑ∞ Î°úÎçî Î∞è BUSI Dataset ÏÑ§Ï†ï\n###############################################\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nfull_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\nindices = np.arange(len(full_dataset))\ntrain_val_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\ntrain_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=42)\ntrain_dataset = Subset(full_dataset, train_idx)\nval_dataset = Subset(full_dataset, val_idx)\ntest_dataset = Subset(full_dataset, test_idx)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=16, shuffle=True, num_workers=4,\n    worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id),\n    pin_memory=True, persistent_workers=True\n)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\n\ndef calculate_pos_weight(loader):\n    total_pixels = 0\n    positive_pixels = 0\n    for images, masks, _ in loader:\n        positive_pixels += masks.sum().item()\n        total_pixels += masks.numel()\n    negative_pixels = total_pixels - positive_pixels\n    return torch.tensor(negative_pixels / (positive_pixels + 1e-6)).to(device)\n\n###############################################\n# Î™®Îç∏, Optimizer, Scheduler ÏÑ§Ï†ï\n###############################################\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\npos_weight = calculate_pos_weight(train_loader)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nnum_epochs = 500\n\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n\n\"\"\"###############################################\n# Training Loop (Early Stopping patience=50)\n###############################################\nfrom torch.cuda.amp import GradScaler, autocast\nscaler = GradScaler()\npatience = 15\nbest_val_f1 = 0.0\npatience_counter = 0\n\nfor epoch in range(num_epochs):\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss_seg = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=0.2, lambda_lovasz=0.6)\n            loss_total = loss_seg  # RL ÎØ∏ÏÇ¨Ïö©\n        scaler.scale(loss_total).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        bs = images.size(0)\n        epoch_loss += loss_total.item() * bs\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples\n    avg_auc = epoch_auc / total_samples\n    avg_iou = epoch_iou / total_samples\n    avg_f1 = epoch_f1 / total_samples\n    avg_precision = epoch_precision / total_samples\n    avg_recall = epoch_recall / total_samples\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n    \n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model.pth')\n        patience_counter = 0\n        print(\"üçÄ New best validation Dice achieved! Dice: {:.4f} üçÄ\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T07:55:38.941587Z","iopub.execute_input":"2025-03-15T07:55:38.941935Z","iopub.status.idle":"2025-03-15T07:55:52.346672Z","shell.execute_reply.started":"2025-03-15T07:55:38.941911Z","shell.execute_reply":"2025-03-15T07:55:52.345665Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'###############################################\\n# Training Loop (Early Stopping patience=50)\\n###############################################\\nfrom torch.cuda.amp import GradScaler, autocast\\nscaler = GradScaler()\\npatience = 15\\nbest_val_f1 = 0.0\\npatience_counter = 0\\n\\nfor epoch in range(num_epochs):\\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\\n    if hasattr(model, \\'bottleneck\\'):\\n        if isinstance(model, nn.DataParallel):\\n            model.module.bottleneck.gating.temperature = current_temp\\n        else:\\n            model.bottleneck.gating.temperature = current_temp\\n\\n    model.train()\\n    epoch_loss = 0.0\\n    epoch_auc = 0.0\\n    epoch_iou = 0.0\\n    epoch_f1 = 0.0\\n    epoch_precision = 0.0\\n    epoch_recall = 0.0\\n    total_samples = 0\\n\\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\\n        images = images.to(device)\\n        masks = masks.to(device)\\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\\n        optimizer.zero_grad()\\n        with autocast():\\n            outputs = model(images)\\n            loss_seg = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=0.2, lambda_lovasz=0.6)\\n            loss_total = loss_seg  # RL ÎØ∏ÏÇ¨Ïö©\\n        scaler.scale(loss_total).backward()\\n        scaler.unscale_(optimizer)\\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n        scaler.step(optimizer)\\n        scaler.update()\\n        bs = images.size(0)\\n        epoch_loss += loss_total.item() * bs\\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\\n        epoch_auc += auc_score * bs\\n        epoch_iou += iou_val * bs\\n        epoch_f1 += f1_val * bs\\n        epoch_precision += prec * bs\\n        epoch_recall += rec * bs\\n        total_samples += bs\\n\\n    scheduler.step(epoch)\\n    avg_loss = epoch_loss / total_samples\\n    avg_auc = epoch_auc / total_samples\\n    avg_iou = epoch_iou / total_samples\\n    avg_f1 = epoch_f1 / total_samples\\n    avg_precision = epoch_precision / total_samples\\n    avg_recall = epoch_recall / total_samples\\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\\n    \\n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\\n    \\n    if val_f1 > best_val_f1:\\n        best_val_f1 = val_f1\\n        best_weights = copy.deepcopy(model.state_dict())\\n        torch.save(best_weights, \\'best_model.pth\\')\\n        patience_counter = 0\\n        print(\"üçÄ New best validation Dice achieved! Dice: {:.4f} üçÄ\".format(best_val_f1))\\n    else:\\n        patience_counter += 1\\n        if patience_counter >= patience:\\n            print(f\"Early stopping at epoch {epoch+1}\")\\n            break\\n'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test_loss, test_auc, test_iou, test_f1, test_precision, test_recall, test_hd = evaluate_with_metrics(model, test_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\nprint(f\"[Test] Loss: {test_loss:.4f}, AUC: {test_auc:.4f}, IoU: {test_iou:.4f}, Dice/F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, HD95: {test_hd:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T07:12:15.215514Z","iopub.execute_input":"2025-03-14T07:12:15.215825Z","iopub.status.idle":"2025-03-14T07:12:24.460828Z","shell.execute_reply.started":"2025-03-14T07:12:15.215800Z","shell.execute_reply":"2025-03-14T07:12:24.459431Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"[Test] Loss: 1.0127, AUC: 0.9585, IoU: 0.7442, Dice/F1: 0.6459, Precision: 0.6761, Recall: 0.6465, HD95: 20.2945\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### üîç **SelectiveGatingBlockÍ≥º Ïú†ÏÇ¨Ìïú ÎÑ§Ïù¥Î∞ç ÏïÑÏù¥ÎîîÏñ¥**\nSelective GatingÏùò Í∞úÎÖêÏùÑ Î∞òÏòÅÌïòÎ©¥ÏÑúÎèÑ Í≥†Ïú†ÏÑ±ÏùÑ Ïú†ÏßÄÌï† Ïàò ÏûàÎäî ÏÉàÎ°úÏö¥ Ïù¥Î¶ÑÎì§ÏùÑ Ï†úÏïàÌï¥Î≥ºÍ≤åÏöî.\n\n---\n## üöÄ **ÏÉàÎ°úÏö¥ ÎÑ§Ïù¥Î∞ç ÌõÑÎ≥¥**\n| **Ïù¥Î¶Ñ**                         | **ÏÑ§Î™Ö** |\n|----------------------------------|------------------------------------------------------------|\n| **SelectiveGateBlock**           | `SelectiveGatingBlock`ÏùÑ Îçî Í∞ÑÍ≤∞ÌïòÍ≤å ÌëúÌòÑ |\n| **AdaptiveSelectionGate**        | ÏÑ†ÌÉùÏ†Å(Gating) & Ï†ÅÏùëÏ†Å(Adaptive) ÏöîÏÜåÎ•º Î∞òÏòÅ |\n| **FilteredGatingBlock**          | Î∂àÌïÑÏöîÌïú FeatureÎ•º \"Í±∏Îü¨ÎÇ¥Îäî(Filter)\" Í∞úÎÖêÏùÑ Î∞òÏòÅ |\n| **DynamicSelectionGate**         | ÎèôÏ†ÅÏúºÎ°ú ÏÑ†ÌÉùÌïòÎäî GatingÏùÑ Í∞ïÏ°∞ |\n| **SmartSelectiveGate**           | \"Ïä§ÎßàÌä∏Ìïú Feature ÏÑ†ÌÉù\"ÏùÑ Í∞ïÏ°∞ |\n| **FeatureSelectionGate**         | Feature SelectionÏùÑ ÏßÅÏ†ëÏ†ÅÏúºÎ°ú ÌëúÌòÑ |\n| **GatedSelectionModule (GSM)**   | Selection Í∞úÎÖêÍ≥º GatingÏùÑ Î™®ÎìàÎ°ú Ï°∞Ìï© (ÎÖºÎ¨∏Ïóê Ï†ÅÏ†à) |\n| **S-GateBlock**                  | \"Selective\"Ïùò ÏïΩÏûêÎ°ú Ï§ÑÏù∏ ÌòïÌÉú (ÏΩîÎìúÏóêÏÑú ÏßÅÍ¥ÄÏ†Å) |\n| **ActiveSelectionGate**          | \"Active Selection\"ÏùÑ ÌÜµÌï¥ FeatureÎ•º Í∞ïÏ°∞ÌïòÎäî ÎäêÎÇå |\n| **PrioritizedGatingBlock**       | Ï§ëÏöîÌïú FeatureÎ•º Ïö∞ÏÑ†Ï†ÅÏúºÎ°ú Î∞òÏòÅÌïòÎäî Í∞úÎÖê |\n\n---\n## üî• **ÏµúÏ¢Ö Ï∂îÏ≤ú**\n‚úÖ **`SelectiveGateBlock`** ‚Üí Í∏∞Ï°¥ Ïù¥Î¶ÑÏóêÏÑú Îçî ÏßÅÍ¥ÄÏ†ÅÏúºÎ°ú Ï∂ïÏïΩ  \n‚úÖ **`DynamicSelectionGate`** ‚Üí ÎèôÏ†Å ÏÑ†ÌÉùÏù¥ÎùºÎäî ÌïµÏã¨ Í∞úÎÖêÏùÑ Í∞ïÏ°∞  \n‚úÖ **`GatedSelectionModule (GSM)`** ‚Üí ÎÖºÎ¨∏ÏóêÏÑú ÏÇ¨Ïö©ÌïòÍ∏∞ Ï¢ãÏùÄ Í≥†Í∏âÏä§Îü¨Ïö¥ ÎÑ§Ïù¥Î∞ç  \n‚úÖ **`PrioritizedGatingBlock`** ‚Üí Ï§ëÏöîÌïú FeatureÎ•º ÏÑ†Î≥ÑÌïòÎäî Í∞úÎÖêÏùÑ Ìè¨Ìï®  \n\nüìå **ÏµúÏ¢Ö ÏùòÍ≤¨:**  \n- **Selective Gating** Í∞úÎÖêÏùÑ Ïú†ÏßÄÌïòÎ†§Î©¥ `SelectiveGateBlock`Ïù¥ Í∞ÄÏû• ÏßÅÍ¥ÄÏ†Å  \n- Ï¢Ä Îçî Ïó≠ÎèôÏ†ÅÏù∏ ÎäêÎÇåÏùÑ Ï£ºÍ≥† Ïã∂Îã§Î©¥ `DynamicSelectionGate` Ï∂îÏ≤ú  \n- ÎÖºÎ¨∏ÏóêÏÑúÎäî **GSM (GatedSelectionModule)** Í∞ôÏùÄ ÏïΩÏñ¥Í∞Ä ÍπîÎÅîÌï† ÏàòÎèÑ ÏûàÏùå!  \n\n> üëâ **\"SelectiveGatingBlock\" ÎåÄÏã† `SelectiveGateBlock`, `DynamicSelectionGate`, `GatedSelectionModule (GSM)`ÏùÑ Ï∂îÏ≤úÌï©ÎãàÎã§!** üöÄ\n\n\n\n### üìë **ÎÖºÎ¨∏Ïóê Ï†ÅÌï©Ìïú ÏÉàÎ°úÏö¥ Î∏îÎ°ù ÎÑ§Ïù¥Î∞ç 10Í∞ú Ï∂îÏ≤ú**  \n\nÎÖºÎ¨∏ÏóêÏÑú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù **Í≥†Í∏âÏä§ÎüΩÍ≥† Í∏∞Ïà†Ï†ÅÏù∏ ÎäêÎÇåÏùÑ Ï£ºÎäî Ïù¥Î¶Ñ**ÏùÑ ÏÑ†Ï†ïÌñàÏñ¥!  \n\n---\n\n### **1Ô∏è‚É£ Feature Selection & Gating Í∏∞Î∞ò**  \n1. **AdaptiveSelectiveGate (ASG)** ‚Üí ÏÑ†ÌÉùÏ†Å Gating + Ï†ÅÏùëÏ†Å Íµ¨Ï°∞  \n2. **DynamicFeatureGating (DFG)** ‚Üí Feature ÏÑ†ÌÉùÏùÑ ÎèôÏ†ÅÏúºÎ°ú ÏàòÌñâ  \n3. **HierarchicalSelectionGate (HSG)** ‚Üí Í≥ÑÏ∏µÏ†ÅÏúºÎ°ú FeatureÎ•º ÏÑ†ÌÉùÌïòÎäî Í∞úÎÖê  \n4. **PriorityGatedFusion (PGF)** ‚Üí Ï§ëÏöîÌïú FeatureÎ•º Ïö∞ÏÑ†Ï†ÅÏúºÎ°ú ÏÑ†ÌÉù ÌõÑ Í≤∞Ìï©  \n5. **Context-Aware Selection Gate (CSG)** ‚Üí Î¨∏Îß•(Context) Í∏∞Î∞òÏúºÎ°ú ÏÑ†ÌÉùÏ†Å ÌôúÏÑ±Ìôî  \n\n---\n\n### **2Ô∏è‚É£ Gating & FusionÏùÑ Í∞ïÏ°∞Ìïú Ïù¥Î¶Ñ**  \n6. **EfficientSelectiveGating (ESG)** ‚Üí Í≤ΩÎüâÌôîÏôÄ ÏÑ†ÌÉùÏ†Å GatingÏùÑ Í∞ïÏ°∞  \n7. **Multi-Scale Gated Fusion (MSGF)** ‚Üí Îã§Ï§ë Ìï¥ÏÉÅÎèÑ FeatureÏùò ÏÑ†ÌÉùÏ†Å ÏúµÌï©  \n8. **Neural Adaptive Selection Gate (NASG)** ‚Üí Ïã†Í≤ΩÎßù Í∏∞Î∞ò Ï†ÅÏùëÏ†Å ÏÑ†ÌÉù  \n9. **Attentive Gating Module (AGM)** ‚Üí AttentionÍ≥º GatingÏùÑ Í≤∞Ìï©Ìïú Íµ¨Ï°∞  \n10. **Selective Representation Learning Block (SRLB)** ‚Üí ÏÑ†ÌÉùÏ†Å Feature ÌïôÏäµÏùÑ Í∞ïÏ°∞  \n\n---\n\n### ‚úÖ **ÎÖºÎ¨∏Ïö© ÏµúÏ¢Ö Ï∂îÏ≤ú Top 3**  \n1. **AdaptiveSelectiveGate (ASG)** ‚Üí ÎÖºÎ¨∏Ïóê Ïì∞Í∏∞ ÏßÅÍ¥ÄÏ†ÅÏù¥Î©¥ÏÑú ÍπîÎÅî  \n2. **HierarchicalSelectionGate (HSG)** ‚Üí Í≥ÑÏ∏µÏ†Å Íµ¨Ï°∞ Í∞ïÏ°∞ (ÎÖºÎ¨∏ÏóêÏÑú Í≥†Í∏âÏä§ÎüΩÍ≤å Î≥¥ÏûÑ)  \n3. **Neural Adaptive Selection Gate (NASG)** ‚Üí Ïã†Í≤ΩÎßù Í∏∞Î∞ò Ï†ÅÏùëÏ†Å ÏÑ†ÌÉù (Í≥†Í∏â AI Î™®Îç∏ ÎäêÎÇå)  \n\nüìå **ÌäπÏßï**  \n- ÏïΩÏñ¥Î•º Ìè¨Ìï®Ìï¥ ÎÖºÎ¨∏ÏóêÏÑú ÏÇ¨Ïö©ÌïòÍ∏∞ Ïö©Ïù¥  \n- Í∏∞Ï°¥ `SelectiveGatingBlock`Í≥º Ï∞®Î≥ÑÌôîÎêòÎ©¥ÏÑúÎèÑ Í∞ôÏùÄ Í∞úÎÖêÏùÑ Ìè¨Ìï®  \n- ÏµúÏã† Ïó∞Íµ¨ Ìä∏Î†åÎìú(Adaptive, Dynamic, Context-Aware Îì±) Î∞òÏòÅ  \n\n> üöÄ **\"ÎÖºÎ¨∏Ïö© ÎÑ§Ïù¥Î∞ç\"ÏúºÎ°ú `AdaptiveSelectiveGate (ASG)`, `HierarchicalSelectionGate (HSG)`, `Neural Adaptive Selection Gate (NASG)` Ï∂îÏ≤ú!**","metadata":{}},{"cell_type":"markdown","source":"# 256x256","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport copy\nimport glob\nimport random\nimport warnings\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport cv2  # CLAHE Ï†ÅÏö©ÏùÑ ÏúÑÌï¥ OpenCV ÏÇ¨Ïö©\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage.segmentation import find_boundaries\nfrom skimage.measure import label, regionprops\nimport scipy.ndimage\n\n# Albumentations Í∏∞Î∞ò augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# medpy HD metric\nfrom medpy.metric import binary as medpy_binary\nfrom torchvision.transforms import ColorJitter\n\n###############################################\n# Seed Î∞è Warning ÏÑ§Ï†ï\n###############################################\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\nwarnings.filterwarnings('ignore')\n\n###############################################\n# BUSI Segmentation Dataset\n###############################################\nclass BUSISegmentationDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform  \n        self.samples = []  # Î∞òÎìúÏãú Ï¥àÍ∏∞Ìôî\n        self._prepare_samples()\n\n    def _prepare_samples(self):\n        labels = os.listdir(self.data_path)\n        for label in labels:\n            folder_path = os.path.join(self.data_path, label)\n            if not os.path.isdir(folder_path):\n                continue\n            files = os.listdir(folder_path)\n            image_files = sorted([f for f in files if '_mask' not in f and f.endswith('.png')])\n            mask_files  = sorted([f for f in files if '_mask' in f and f.endswith('.png')])\n            pattern_img = re.compile(rf'{re.escape(label)} \\((\\d+)\\)\\.png')\n            pattern_mask = re.compile(rf'{re.escape(label)} \\((\\d+)\\)_mask(?:_\\d+)?\\.png')\n            mask_dict = {}\n            for mf in mask_files:\n                m = pattern_mask.fullmatch(mf)\n                if m:\n                    idx = m.group(1)\n                    mask_dict.setdefault(idx, []).append(mf)\n            for im in image_files:\n                m = pattern_img.fullmatch(im)\n                if m:\n                    idx = m.group(1)\n                    img_path = os.path.join(folder_path, im)\n                    if idx in mask_dict:\n                        mask_paths = [os.path.join(folder_path, mf) for mf in mask_dict[idx]]\n                        combined_mask = None\n                        for mp in mask_paths:\n                            mask_img = Image.open(mp).convert('L')\n                            mask_arr = np.array(mask_img)\n                            mask_binary = (mask_arr > 128).astype(np.uint8)\n                            if combined_mask is None:\n                                combined_mask = mask_binary\n                            else:\n                                combined_mask = np.maximum(combined_mask, mask_binary)\n                        self.samples.append((img_path, combined_mask, label))\n                    else:\n                        image_pil = Image.open(img_path)\n                        empty_mask = np.zeros(image_pil.size[::-1], dtype=np.uint8)\n                        self.samples.append((img_path, empty_mask, label))\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, mask_array, label = self.samples[index]\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.fromarray((mask_array * 255).astype(np.uint8))\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        return image, mask, label\n\n###############################################\n# Per-image Z-score normalization (adaptive)\n###############################################\ndef z_score_normalize(tensor):\n    mean = tensor.mean()\n    std = tensor.std() + 1e-6\n    return (tensor - mean) / std\n\n###############################################\n# CLAHE Augmentation Ìï®Ïàò (OpenCV)\n###############################################\ndef apply_clahe(image, clipLimit=2.0, tileGridSize=(8,8)):\n    img_np = np.array(image)\n    if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl, a, b))\n        final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    else:\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        final = clahe.apply(img_np)\n    return Image.fromarray(final)\n\n###############################################\n# joint_transform (Albumentations Í∏∞Î∞ò Augmentation)\n###############################################\ndef joint_transform(image, mask, size=(256,256)):  # Í∏∞Î≥∏ sizeÎ•º (256,256)ÏúºÎ°ú Î≥ÄÍ≤Ω\n    geom_transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=10, p=0.5),\n        A.ElasticTransform(alpha=10, sigma=5, alpha_affine=5, p=0.3),\n        A.Resize(height=size[0], width=size[1])\n    ])\n    image_np = np.array(image)\n    mask_np = np.array(mask)\n    augmented = geom_transform(image=image_np, mask=mask_np)\n    image = augmented['image']\n    mask = augmented['mask']\n    \n    intensity_transform = A.Compose([\n        A.RandomBrightnessContrast(p=0.5),\n        #A.CLAHE(clip_limit=3.0, p=0.5),\n        A.CLAHE(clip_limit=1.0, tile_grid_size=(8,8), p=0.5),\n        A.GaussianBlur(p=0.3)\n    ])\n    image = intensity_transform(image=image)['image']\n    \n    image = transforms.ToTensor()(image)\n    image = z_score_normalize(image)\n    mask = transforms.ToTensor()(mask)\n    return image, mask\n\n###############################################\n# CutMix Ìï®Ïàò (alpha=1.5, p=0.7)\n###############################################\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(images, masks, alpha=1.5, p=0.7):\n    if np.random.rand() > p:\n        return images, masks\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(images.size(0)).to(images.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n    masks[:, :, bbx1:bbx2, bby1:bby2] = masks[rand_index, :, bbx1:bbx2, bby1:bby2]\n    return images, masks\n\n###############################################\n# Advanced ÌõÑÏ≤òÎ¶¨: Morphological Closing (Kernel 9√ó9)\n###############################################\ndef postprocess_mask(mask, min_size=100):\n    labeled_mask = label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n###############################################\n# AttentionGate Î™®Îìà\n###############################################\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(AttentionGate, self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        return x * psi\n\n###############################################\n# MultiScaleFusion Î™®Îìà\n###############################################\nclass MultiScaleFusion(nn.Module):\n    def __init__(self, in_channels_list, out_channels):\n        super(MultiScaleFusion, self).__init__()\n        self.convs = nn.ModuleList([nn.Conv2d(ch, out_channels, kernel_size=1) for ch in in_channels_list])\n        self.fuse = nn.Sequential(\n            nn.Conv2d(len(in_channels_list) * out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, features):\n        target_size = features[-1].size()[2:]\n        processed = []\n        for i, f in enumerate(features):\n            if f.size()[2:] != target_size:\n                f = F.interpolate(f, size=target_size, mode='bilinear', align_corners=False)\n            f = self.convs[i](f)\n            processed.append(f)\n        out = torch.cat(processed, dim=1)\n        out = self.fuse(out)\n        return out\n\n###############################################\n# Í∏∞ÌÉÄ Î™®Îç∏ Í¥ÄÎ†® Î™®Îìà (StochasticDepth, GhostModule, SELayer, DynamicGating)\n###############################################\nclass StochasticDepth(nn.Module):\n    def __init__(self, p, mode=\"row\"):\n        super(StochasticDepth, self).__init__()\n        self.p = p\n        self.mode = mode\n\n    def forward(self, x):\n        if not self.training or self.p == 0.0:\n            return x\n        survival_rate = 1 - self.p\n        if self.mode == \"row\":\n            batch_size = x.shape[0]\n            noise = torch.rand(batch_size, 1, 1, 1, device=x.device, dtype=x.dtype)\n            binary_mask = (noise < survival_rate).float()\n            return x / survival_rate * binary_mask\n        else:\n            if torch.rand(1).item() < self.p:\n                return torch.zeros_like(x)\n            else:\n                return x\n\nclass GhostModule(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, ratio=2, dw_kernel_size=3, stride=1, padding=0, use_relu=True):\n        super(GhostModule, self).__init__()\n        self.out_channels = out_channels\n        self.primary_channels = int(torch.ceil(torch.tensor(out_channels / ratio)))\n        self.cheap_channels = out_channels - self.primary_channels\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(in_channels, self.primary_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(self.primary_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n        self.cheap_conv = nn.Sequential(\n            nn.Conv2d(self.primary_channels, self.cheap_channels, dw_kernel_size, stride=1,\n                      padding=dw_kernel_size // 2, groups=self.primary_channels, bias=False),\n            nn.BatchNorm2d(self.cheap_channels),\n            nn.ReLU(inplace=True) if use_relu else nn.Identity()\n        )\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_conv(x1)\n        out = torch.cat([x1, x2], dim=1)\n        return out[:, :self.out_channels, :, :].contiguous()\n\ndef ghost_conv_block(in_channels, out_channels, use_relu=True):\n    return nn.Sequential(\n        GhostModule(in_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu),\n        GhostModule(out_channels, out_channels, kernel_size=3, ratio=2, dw_kernel_size=3, stride=1, padding=1, use_relu=use_relu)\n    )\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\nclass DynamicGating(nn.Module):\n    def __init__(self, num_branches, hidden_dim=64, dropout_prob=0.1, init_temperature=2.0, iterations=3):\n        super(DynamicGating, self).__init__()\n        self.temperature = init_temperature\n        self.iterations = iterations\n        self.dropout_prob = dropout_prob\n        self.fc = nn.Sequential(\n            nn.Linear(num_branches, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(hidden_dim, num_branches)\n        )\n        self.layernorm = nn.LayerNorm(num_branches)\n        self.softmax = nn.Softmax(dim=1)\n        self.res_scale = nn.Parameter(torch.ones(1))\n        self.saved_log_probs = None\n        self.entropy = None\n        self.rl_loss = 0.0\n\n    def forward(self, features):\n        norm_features = self.layernorm(features)\n        logits = self.fc(norm_features)\n        logits = self.layernorm(logits)\n        scaled_logits = logits / self.temperature\n        gates = self.softmax(scaled_logits)\n        for _ in range(self.iterations - 1):\n            updated_features = norm_features + self.res_scale * gates\n            logits = self.fc(updated_features)\n            logits = self.layernorm(logits)\n            scaled_logits = logits / self.temperature\n            new_gates = self.softmax(scaled_logits)\n            gates = 0.5 * gates + 0.5 * new_gates\n        return gates\n\n###############################################\n# QuadAgentBlock (Lite Î≤ÑÏ†Ñ, 3 Î∏åÎûúÏπò)\n###############################################\nclass QuadAgentBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5):\n        super().__init__()\n        branch_channels = out_channels // 4\n        self.branch1 = GhostModule(in_channels, branch_channels, ratio=4)\n        self.branch2 = GhostModule(in_channels, branch_channels, kernel_size=3, padding=1, ratio=4)\n        self.branch3 = nn.Sequential(\n            GhostModule(in_channels, branch_channels, ratio=4),\n            SELayer(branch_channels, reduction=32)\n        )\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.gating = DynamicGating(num_branches=3, hidden_dim=gating_hidden_dim, dropout_prob=gating_dropout, init_temperature=gating_temperature)\n        self.fusion_conv = GhostModule(branch_channels * 3, out_channels, ratio=2)\n        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False) if in_channels != out_channels else nn.Identity()\n        self.stochastic_depth = StochasticDepth(stochastic_depth_prob)\n\n    def forward(self, x):\n        b1 = self.branch1(x)\n        b2 = self.branch2(x)\n        b3 = self.branch3(x)\n        gap_b1 = self.gap(b1).view(x.size(0), -1)\n        gap_b2 = self.gap(b2).view(x.size(0), -1)\n        gap_b3 = self.gap(b3).view(x.size(0), -1)\n        features = torch.stack([gap_b1.mean(dim=1), gap_b2.mean(dim=1), gap_b3.mean(dim=1)], dim=1)\n        gates = self.gating(features)\n        b1 = b1 * gates[:, 0].view(-1, 1, 1, 1)\n        b2 = b2 * gates[:, 1].view(-1, 1, 1, 1)\n        b3 = b3 * gates[:, 2].view(-1, 1, 1, 1)\n        out = torch.cat([b1, b2, b3], dim=1)\n        out = self.fusion_conv(out)\n        res = self.res_conv(x)\n        res = self.stochastic_depth(res)\n        return out + res\n\n###############################################\n# DARKViT_UNet: ViT Í∏∞Î∞ò Encoder + QuadAgentBlock + Transformer Bottleneck + UNet Decoder\n###############################################\nfrom timm.models import create_model\n\nclass DARKViT_UNet(nn.Module):\n    def __init__(self, out_channels=1):\n        super(DARKViT_UNet, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True)\n        self.bottleneck = QuadAgentBlock(in_channels=1536, out_channels=1536, \n                                         gating_dropout=0.3, gating_hidden_dim=32, \n                                         gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, \n                                                             dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, \n                                                             dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n    \n    def forward(self, x):\n        # encoderÏóêÏÑú feature map Î¶¨Ïä§Ìä∏ ÏñªÍ∏∞: [f0, f1, f2, f3]\n        features = self.encoder(x)\n        \n        # ÎßåÏïΩ Í∞Å feature mapÏù¥ (N, H, W, C) ÌòïÏãùÏù¥ÎùºÎ©¥ (N, C, H, W)Î°ú Î≥ÄÌôò\n        for i in range(len(features)):\n            # f0: 192, f1:384, f2:768, f3:1536 Ï±ÑÎÑêÏù¥Ïñ¥Ïïº Ìï®\n            if features[i].shape[1] not in {192, 384, 768, 1536}:\n                features[i] = features[i].permute(0, 3, 1, 2).contiguous()\n        \n        # Î∞òÌôòÎêú ÏàúÏÑúÎäî [f0, f1, f2, f3]Ïù¥ÎØÄÎ°ú, decoderÍ∞Ä Í∏∞ÎåÄÌïòÎäî [f3, f2, f1, f0] ÏàúÏÑúÎ°ú Ïû¨Ï†ïÎ†¨\n        f0, f1, f2, f3 = features  \n        features_reordered = [f3, f2, f1, f0]\n        \n        # Ïù¥Ï†ú features_reordered[0]Îäî f3: (B,1536,7,7)\n        b_out = self.bottleneck(features_reordered[0])\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        features_reordered[0] = t_out2\n        \n        decoded = self.decoder(features_reordered)\n        return self.final_conv(decoded)\n\n###############################################\n# UNet Decoder for Swin Backbone\n###############################################\nclass UNetDecoder_Swin(nn.Module):\n    def __init__(self):\n        super(UNetDecoder_Swin, self).__init__()\n        # Swin‚ÄëLarge feature sizes (Í∞ÄÏ†ï):\n        # f0: (B,192,56,56), f1: (B,384,28,28), f2: (B,768,14,14), f3: (B,1536,7,7)\n        self.up1 = nn.ConvTranspose2d(1536, 768, kernel_size=2, stride=2)  # 7->14\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(768 + 768, 768, kernel_size=3, padding=1),\n            nn.BatchNorm2d(768),\n            nn.ReLU(inplace=True)\n        )\n        self.up2 = nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2)   # 14->28\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(384 + 384, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True)\n        )\n        self.up3 = nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2)   # 28->56\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(192 + 192, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n        self.up4 = nn.ConvTranspose2d(192, 64, kernel_size=2, stride=2)    # 56->112\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, features_list):\n        # features_list: [f0, f1, f2, f3]\n        f3, f2, f1, f0 = features_list  # ‚úÖ ÏàúÏÑú ÎßûÏ∂îÍ∏∞\n        x = f3  # (B,1536,7,7)\n        x = self.up1(x)  # -> (B,768,14,14)\n        x = torch.cat([x, f2], dim=1)  # f2: (B,768,14,14) ‚Üí (B,1536,14,14)\n        x = self.conv1(x)  # -> (B,768,14,14)\n        x = self.up2(x)  # -> (B,384,28,28)\n        x = torch.cat([x, f1], dim=1)  # f1: (B,384,28,28) ‚Üí (B,768,28,28)\n        x = self.conv2(x)  # -> (B,384,28,28)\n        x = self.up3(x)  # -> (B,192,56,56)\n        x = torch.cat([x, f0], dim=1)  # f0: (B,192,56,56) ‚Üí (B,384,56,56)\n        x = self.conv3(x)  # -> (B,192,56,56)\n        x = self.up4(x)  # -> (B,64,112,112)\n        x = self.conv4(x)  # -> (B,64,112,112)\n        # ÏµúÏ¢Ö interpolationÏùÑ (256,256)ÏúºÎ°ú Î≥ÄÍ≤Ω\n        x = F.interpolate(x, size=(256,256), mode='bilinear', align_corners=False)\n        return x\n\n###############################################\n# Transformer Bottleneck (Ïó∞ÏÜç Îëê Í∞ú Ï†ÅÏö©)\n###############################################\nclass TransformerBottleneck(nn.Module):\n    def __init__(self, d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1):\n        super(TransformerBottleneck, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                    dim_feedforward=dim_feedforward, dropout=dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    def forward(self, x):\n        B, C, H, W = x.size()\n        x = x.view(B, C, H * W).permute(2, 0, 1)\n        x = self.transformer(x)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\n###############################################\n# Swin‚ÄëTransformer Backbone Í∏∞Î∞ò UNet with Attention + Lite Bottleneck\n###############################################\nfrom timm.models import create_model\n\nclass PretrainedSwin_UNet_AttentionFusion(nn.Module):\n    def __init__(self, out_channels=1):\n        super(PretrainedSwin_UNet_AttentionFusion, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True, img_size=256)\n        self.bottleneck = QuadAgentBlock(in_channels=1536, out_channels=1536, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        features_list = self.encoder(x)  # [f0, f1, f2, f3]\n\n        # (N, H, W, C) ‚Üí (N, C, H, W) Î≥ÄÌôòÏù¥ ÌïÑÏöîÌïú Í≤ΩÏö∞ Ï†ÅÏö©\n        for i in range(len(features_list)):\n            if features_list[i].shape[1] not in {192, 384, 768, 1536}:  \n                features_list[i] = features_list[i].permute(0, 3, 1, 2).contiguous()\n\n        # ÎîîÏΩîÎçîÏóê Ïò¨Î∞îÎ•∏ ÏàúÏÑúÎ°ú Ï†ÑÎã¨ÎêòÎèÑÎ°ù Î≥ÄÌôò\n        f0, f1, f2, f3 = features_list  # Swin TransformerÏùò Í∏∞Î≥∏ Î∞òÌôò ÏàúÏÑú\n        features_list = [f3, f2, f1, f0]  # (B, 1536, 7, 7) ‚Üí (B, 192, 56, 56)\n\n        # BottleneckÍ≥º Transformer Bottleneck Ï†ÅÏö©\n        b_out = self.bottleneck(features_list[0])\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        features_list[0] = t_out2\n\n        decoded = self.decoder(features_list)\n        return self.final_conv(decoded)\n\n###############################################\n# Loss & Metric Functions (HD Ìè¨Ìï®)\n###############################################\nfrom segmentation_models_pytorch.losses import LovaszLoss\nlv_loss = LovaszLoss(mode='binary')\n\ndef focal_tversky_loss(pred, target, alpha=0.5, beta=0.5, gamma=4/3, smooth=1e-6):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred = pred.view(pred.size(0), -1)\n    target = target.view(target.size(0), -1)\n    tp = (pred * target).sum(dim=1)\n    fp = ((1 - target) * pred).sum(dim=1)\n    fn = (target * (1 - pred)).sum(dim=1)\n    tversky_index = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n    loss = (1 - tversky_index) ** gamma\n    return loss.mean()\n\ndef boundary_loss(pred, target):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_np = pred.detach().cpu().numpy()\n    target_np = target.detach().cpu().numpy()\n    boundary_masks = []\n    for i in range(pred_np.shape[0]):\n        gt_mask = target_np[i, 0]\n        boundary = find_boundaries(gt_mask, mode='thick')\n        boundary_masks.append(boundary.astype(np.float32))\n    boundary_masks = np.stack(boundary_masks, axis=0)[:, None, :, :]\n    boundary_masks_torch = torch.from_numpy(boundary_masks).to(pred.device)\n    intersect = (pred * boundary_masks_torch).sum()\n    denom = pred.sum() + boundary_masks_torch.sum()\n    boundary_dice = (2.0 * intersect) / (denom + 1e-6)\n    return 1.0 - boundary_dice\n\ndef combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    loss_ft = focal_tversky_loss(outputs, masks)\n    if pos_weight is not None:\n        loss_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, masks.float())\n    else:\n        loss_bce = nn.BCEWithLogitsLoss()(outputs, masks.float())\n    bl = boundary_loss(outputs, masks)\n    lv = lv_loss(outputs, masks)\n    return 0.4 * lv + 0.3 * loss_ft + 0.3 * loss_bce + lambda_boundary * bl\n\ndef dice_f1_precision_recall(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    precision = intersection / (pred_bin.sum() + smooth)\n    recall = intersection / (target_bin.sum() + smooth)\n    f1 = 2 * (precision * recall) / (precision + recall + smooth)\n    return f1.item(), precision.item(), recall.item()\n\ndef iou_metric(pred, target, threshold=0.5, smooth=1e-6):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    union = pred_bin.sum() + target_bin.sum() - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach()\n    processed_preds = []\n    for i in range(pred_probs.size(0)):\n        pred_np = pred_probs[i].cpu().numpy()[0]\n        pred_bin = (pred_np > threshold).astype(np.uint8)\n        processed = postprocess_mask(pred_bin, min_size=100)\n        processed_preds.append(processed)\n    batch_iou = []\n    batch_f1 = []\n    batch_precision = []\n    batch_recall = []\n    for i in range(pred_probs.size(0)):\n        pred = processed_preds[i]\n        gt = masks[i].cpu().numpy()[0]\n        intersection = np.sum(pred * gt)\n        union = np.sum(pred) + np.sum(gt) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        batch_iou.append(iou)\n        precision = intersection / (np.sum(pred) + smooth)\n        recall = intersection / (np.sum(gt) + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        batch_f1.append(f1)\n        batch_precision.append(precision)\n        batch_recall.append(recall)\n    probs = pred_probs.cpu().numpy().flatten()\n    masks_np = masks.cpu().numpy().flatten()\n    try:\n        auc_score = roc_auc_score(masks_np, probs)\n    except ValueError:\n        auc_score = float('nan')\n    return auc_score, np.mean(batch_iou), np.mean(batch_f1), np.mean(batch_precision), np.mean(batch_recall)\n\ndef hausdorff_distance(pred, target):\n    return medpy_binary.hd95(pred, target)\n\ndef compute_hd_metric(outputs, masks, threshold=0.5):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n    masks_np = masks.detach().cpu().numpy()\n    hd_list = []\n    for i in range(outputs.size(0)):\n        pred_bin = (pred_probs[i, 0] > threshold).astype(np.uint8)\n        gt_bin = (masks_np[i, 0] > 0.5).astype(np.uint8)\n        try:\n            hd = medpy_binary.hd95(pred_bin, gt_bin)\n        except Exception:\n            hd = np.nan\n        hd_list.append(hd)\n    return np.nanmean(hd_list)\n\ndef evaluate_with_metrics(model, dataloader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None):\n    model.eval()\n    total_loss = 0.0\n    total_auc  = 0.0\n    total_iou  = 0.0\n    total_f1   = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    total_hd = 0.0\n    total_samples = 0\n    with torch.no_grad():\n        for images, masks, _ in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=lambda_boundary)\n            auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=threshold, smooth=1e-6)\n            hd_val = compute_hd_metric(outputs, masks, threshold=threshold)\n            bs = images.size(0)\n            total_loss += loss.item() * bs\n            total_auc  += auc_score * bs\n            total_iou  += iou_val * bs\n            total_f1   += f1_val * bs\n            total_precision += prec * bs\n            total_recall += rec * bs\n            total_hd += hd_val * bs\n            total_samples += bs\n    avg_loss = total_loss / total_samples\n    avg_auc  = total_auc / total_samples\n    avg_iou  = total_iou / total_samples\n    avg_f1   = total_f1 / total_samples\n    avg_precision = total_precision / total_samples\n    avg_recall = total_recall / total_samples\n    avg_hd = total_hd / total_samples\n    return avg_loss, avg_auc, avg_iou, avg_f1, avg_precision, avg_recall, avg_hd\n\n###############################################\n# TTA (Test Time Augmentation) Ìï®Ïàò (Noise & ColorJitter Ï∂îÍ∞Ä)\n###############################################\ndef tta_predict(model, image, device):\n    cj = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n    def add_noise(x, std=0.05):\n        noise = torch.randn_like(x) * std\n        return x + noise\n    def identity(x): return x\n    def hflip(x): return torch.flip(x, dims=[-1])\n    def vflip(x): return torch.flip(x, dims=[-2])\n    def rot90(x): return torch.rot90(x, k=1, dims=[-2, -1])\n    def inv_hflip(x): return torch.flip(x, dims=[-1])\n    def inv_vflip(x): return torch.flip(x, dims=[-2])\n    def inv_rot90(x): return torch.rot90(x, k=3, dims=[-2, -1])\n    \n    transforms_list = [\n        (identity, identity),\n        (hflip, inv_hflip),\n        (vflip, inv_vflip),\n        (rot90, inv_rot90),\n        (lambda x: add_noise(cj(x)), lambda x: x)\n    ]\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for aug, inv in transforms_list:\n            augmented = aug(image)\n            output = model(augmented.unsqueeze(0).to(device))\n            output = torch.sigmoid(output)\n            output = inv(output).cpu()\n            predictions.append(output)\n    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n    return avg_prediction\n\n###############################################\n# Optimizer Scheduler (SGDR: CosineAnnealingWarmRestarts)\n###############################################\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\n###############################################\n# Îç∞Ïù¥ÌÑ∞ Î°úÎçî Î∞è BUSI Dataset ÏÑ§Ï†ï\n###############################################\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nfull_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\nindices = np.arange(len(full_dataset))\ntrain_val_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\ntrain_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=42)\ntrain_dataset = Subset(full_dataset, train_idx)\nval_dataset = Subset(full_dataset, val_idx)\ntest_dataset = Subset(full_dataset, test_idx)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=16, shuffle=True, num_workers=4,\n    worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id),\n    pin_memory=True, persistent_workers=True\n)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\n\ndef calculate_pos_weight(loader):\n    total_pixels = 0\n    positive_pixels = 0\n    for images, masks, _ in loader:\n        positive_pixels += masks.sum().item()\n        total_pixels += masks.numel()\n    negative_pixels = total_pixels - positive_pixels\n    return torch.tensor(negative_pixels / (positive_pixels + 1e-6)).to(device)\n\n###############################################\n# Î™®Îç∏, Optimizer, Scheduler ÏÑ§Ï†ï\n###############################################\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\npos_weight = calculate_pos_weight(train_loader)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nnum_epochs = 500\n\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n\n###############################################\n# Training Loop (Early Stopping patience=15)\n###############################################\nfrom torch.cuda.amp import GradScaler, autocast\nscaler = GradScaler()\npatience = 15\nbest_val_f1 = 0.0\npatience_counter = 0\n\"\"\"\nfor epoch in range(num_epochs):\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss_seg = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=0.2, lambda_lovasz=0.6)\n            loss_total = loss_seg  # RL ÎØ∏ÏÇ¨Ïö©\n        scaler.scale(loss_total).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        bs = images.size(0)\n        epoch_loss += loss_total.item() * bs\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples\n    avg_auc = epoch_auc / total_samples\n    avg_iou = epoch_iou / total_samples\n    avg_f1 = epoch_f1 / total_samples\n    avg_precision = epoch_precision / total_samples\n    avg_recall = epoch_recall / total_samples\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n    \n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model.pth')\n        patience_counter = 0\n        print(\"üçÄ New best validation Dice achieved! Dice: {:.4f} üçÄ\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:10:45.096703Z","iopub.execute_input":"2025-03-15T09:10:45.097035Z","iopub.status.idle":"2025-03-15T09:11:19.884214Z","shell.execute_reply.started":"2025-03-15T09:10:45.097012Z","shell.execute_reply":"2025-03-15T09:11:19.883056Z"},"jupyter":{"source_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/788M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9c9af1ce06d4c6b9b3e3dec2ca2643d"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\nfor epoch in range(num_epochs):\\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\\n    if hasattr(model, \\'bottleneck\\'):\\n        if isinstance(model, nn.DataParallel):\\n            model.module.bottleneck.gating.temperature = current_temp\\n        else:\\n            model.bottleneck.gating.temperature = current_temp\\n\\n    model.train()\\n    epoch_loss = 0.0\\n    epoch_auc = 0.0\\n    epoch_iou = 0.0\\n    epoch_f1 = 0.0\\n    epoch_precision = 0.0\\n    epoch_recall = 0.0\\n    total_samples = 0\\n\\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\\n        images = images.to(device)\\n        masks = masks.to(device)\\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\\n        optimizer.zero_grad()\\n        with autocast():\\n            outputs = model(images)\\n            loss_seg = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=0.2, lambda_lovasz=0.6)\\n            loss_total = loss_seg  # RL ÎØ∏ÏÇ¨Ïö©\\n        scaler.scale(loss_total).backward()\\n        scaler.unscale_(optimizer)\\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n        scaler.step(optimizer)\\n        scaler.update()\\n        bs = images.size(0)\\n        epoch_loss += loss_total.item() * bs\\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\\n        epoch_auc += auc_score * bs\\n        epoch_iou += iou_val * bs\\n        epoch_f1 += f1_val * bs\\n        epoch_precision += prec * bs\\n        epoch_recall += rec * bs\\n        total_samples += bs\\n\\n    scheduler.step(epoch)\\n    avg_loss = epoch_loss / total_samples\\n    avg_auc = epoch_auc / total_samples\\n    avg_iou = epoch_iou / total_samples\\n    avg_f1 = epoch_f1 / total_samples\\n    avg_precision = epoch_precision / total_samples\\n    avg_recall = epoch_recall / total_samples\\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\\n    \\n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight)\\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\\n    \\n    if val_f1 > best_val_f1:\\n        best_val_f1 = val_f1\\n        best_weights = copy.deepcopy(model.state_dict())\\n        torch.save(best_weights, \\'best_model.pth\\')\\n        patience_counter = 0\\n        print(\"üçÄ New best validation Dice achieved! Dice: {:.4f} üçÄ\".format(best_val_f1))\\n    else:\\n        patience_counter += 1\\n        if patience_counter >= patience:\\n            print(f\"Early stopping at epoch {epoch+1}\")\\n            break\\n'"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Puesudo Labeling","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport copy\nimport glob\nimport random\nimport warnings\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport cv2  # CLAHE Ï†ÅÏö©ÏùÑ ÏúÑÌï¥ OpenCV ÏÇ¨Ïö©\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage.segmentation import find_boundaries\nfrom skimage.measure import label, regionprops\nimport scipy.ndimage\n\n# Albumentations Í∏∞Î∞ò augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# medpy HD metric\nfrom medpy.metric import binary as medpy_binary\nfrom torchvision.transforms import ColorJitter\n\n###############################################\n# Seed Î∞è Warning ÏÑ§Ï†ï\n###############################################\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\nwarnings.filterwarnings('ignore')\n\n###############################################\n# BUSI Segmentation Dataset (ÎùºÎ≤® Îç∞Ïù¥ÌÑ∞)\n###############################################\nclass BUSISegmentationDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform  \n        self.samples = []  # Î∞òÎìúÏãú Ï¥àÍ∏∞Ìôî\n        self._prepare_samples()\n\n    def _prepare_samples(self):\n        labels = os.listdir(self.data_path)\n        for label in labels:\n            folder_path = os.path.join(self.data_path, label)\n            if not os.path.isdir(folder_path):\n                continue\n            files = os.listdir(folder_path)\n            image_files = sorted([f for f in files if '_mask' not in f and f.endswith('.png')])\n            mask_files  = sorted([f for f in files if '_mask' in f and f.endswith('.png')])\n            pattern_img = re.compile(rf'{re.escape(label)} \\((\\d+)\\)\\.png')\n            pattern_mask = re.compile(rf'{re.escape(label)} \\((\\d+)\\)_mask(?:_\\d+)?\\.png')\n            mask_dict = {}\n            for mf in mask_files:\n                m = pattern_mask.fullmatch(mf)\n                if m:\n                    idx = m.group(1)\n                    mask_dict.setdefault(idx, []).append(mf)\n            for im in image_files:\n                m = pattern_img.fullmatch(im)\n                if m:\n                    idx = m.group(1)\n                    img_path = os.path.join(folder_path, im)\n                    if idx in mask_dict:\n                        mask_paths = [os.path.join(folder_path, mf) for mf in mask_dict[idx]]\n                        combined_mask = None\n                        for mp in mask_paths:\n                            mask_img = Image.open(mp).convert('L')\n                            mask_arr = np.array(mask_img)\n                            mask_binary = (mask_arr > 128).astype(np.uint8)\n                            if combined_mask is None:\n                                combined_mask = mask_binary\n                            else:\n                                combined_mask = np.maximum(combined_mask, mask_binary)\n                        self.samples.append((img_path, combined_mask, label))\n                    else:\n                        image_pil = Image.open(img_path)\n                        empty_mask = np.zeros(image_pil.size[::-1], dtype=np.uint8)\n                        self.samples.append((img_path, empty_mask, label))\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, mask_array, label = self.samples[index]\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.fromarray((mask_array * 255).astype(np.uint8))\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        return image, mask, label\n\n###############################################\n# Per-image Z-score normalization (adaptive)\n###############################################\ndef z_score_normalize(tensor):\n    mean = tensor.mean()\n    std = tensor.std() + 1e-6\n    return (tensor - mean) / std\n\n###############################################\n# CLAHE Augmentation Ìï®Ïàò (OpenCV)\n###############################################\ndef apply_clahe(image, clipLimit=2.0, tileGridSize=(8,8)):\n    img_np = np.array(image)\n    if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl, a, b))\n        final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    else:\n        clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n        final = clahe.apply(img_np)\n    return Image.fromarray(final)\n\n###############################################\n# joint_transform (Albumentations Í∏∞Î∞ò Augmentation)\n# Ïó¨Í∏∞ÏÑú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏôÄ maskÎ•º 256√ó256ÏúºÎ°ú resize\n###############################################\ndef joint_transform(image, mask, size=(256,256)):\n    geom_transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=10, p=0.5),\n        A.ElasticTransform(alpha=10, sigma=5, alpha_affine=5, p=0.3),\n        A.Resize(height=size[0], width=size[1])\n    ])\n    image_np = np.array(image)\n    mask_np = np.array(mask)\n    augmented = geom_transform(image=image_np, mask=mask_np)\n    image = augmented['image']\n    mask = augmented['mask']\n    \n    intensity_transform = A.Compose([\n        A.RandomBrightnessContrast(p=0.5),\n        A.CLAHE(clip_limit=1.0, tile_grid_size=(8,8), p=0.5),\n        A.GaussianBlur(p=0.3)\n    ])\n    image = intensity_transform(image=image)['image']\n    \n    image = transforms.ToTensor()(image)\n    image = z_score_normalize(image)\n    mask = transforms.ToTensor()(mask)\n    return image, mask\n\n###############################################\n# CutMix Ìï®Ïàò (alpha=1.5, p=0.7)\n###############################################\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(images, masks, alpha=1.5, p=0.7):\n    if np.random.rand() > p:\n        return images, masks\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(images.size(0)).to(images.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n    masks[:, :, bbx1:bbx2, bby1:bby2] = masks[rand_index, :, bbx1:bbx2, bby1:bby2]\n    return images, masks\n\n###############################################\n# Advanced ÌõÑÏ≤òÎ¶¨: Morphological Closing (Kernel 9√ó9)\n###############################################\ndef postprocess_mask(mask, min_size=100):\n    labeled_mask = label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n# ----------------------------------------------------------------\n# ... (AttentionGate, MultiScaleFusion, StochasticDepth, GhostModule, SELayer, DynamicGating,\n# QuadAgentBlock, DARKViT_UNet, UNetDecoder_Swin, TransformerBottleneck, PretrainedSwin_UNet_AttentionFusion Îì±\n# Í∏∞Ï°¥ ÏΩîÎìú Ï†ïÏùòÎäî Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©) \n# ----------------------------------------------------------------\n\n# Ïó¨Í∏∞ÏÑúÎäî Ïù¥ÎØ∏ Í∏∞Ï°¥ ÏΩîÎìúÏùò Î™®Îç∏, loss, metric Ìï®ÏàòÎì§ÏùÑ Ï†ïÏùòÌñàÎã§Í≥† Í∞ÄÏ†ïÌï©ÎãàÎã§.\n# (ÏúÑÏóê Ï†úÍ≥µÎêú ÏΩîÎìúÏùò ÎÇòÎ®∏ÏßÄ Î∂ÄÎ∂ÑÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©)\n\n###############################################\n# Unlabeled Dataset ÌÅ¥ÎûòÏä§ (ÎùºÎ≤® ÏóÜÎäî Îç∞Ïù¥ÌÑ∞)\n###############################################\nclass UnlabeledDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform\n        # Ìè¥Îçî ÎÇ¥ Î™®Îì† PNG ÌååÏùº Í≤ΩÎ°ú ÏùΩÍ∏∞\n        self.image_paths = sorted([os.path.join(data_path, fname) for fname in os.listdir(data_path) if fname.endswith('.png')])\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        img_path = self.image_paths[index]\n        image = Image.open(img_path).convert(\"RGB\")\n        # maskÎäî ÏóÜÏúºÎØÄÎ°ú ÎçîÎØ∏ Í∞íÏùÑ ÏÇ¨Ïö© (Ïòà: None ÌòπÏùÄ 0ÏúºÎ°ú Ï±ÑÏõåÏßÑ Ïù¥ÎØ∏ÏßÄ)\n        dummy_mask = Image.new(\"L\", image.size, 0)\n        if self.transform:\n            image, _ = self.transform(image, dummy_mask)\n        else:\n            image = transforms.ToTensor()(image)\n        return image, img_path\n\n###############################################\n# Pseudo Labeling Ìï®Ïàò\n###############################################\ndef generate_pseudo_labels(model, unlabeled_loader, device, threshold=0.9):\n    model.eval()\n    pseudo_data = []  # (image, pseudo mask) ÌäúÌîå Î¶¨Ïä§Ìä∏\n    with torch.no_grad():\n        for images, paths in unlabeled_loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = torch.sigmoid(outputs)\n            # threshold Í∏∞Ï§ÄÏùÑ ÎÑòÎäî Î∂ÄÎ∂ÑÏùÑ pseudo labelÎ°ú ÏÇ¨Ïö© (Ïó¨Í∏∞ÏÑúÎäî ÌîΩÏÖÄ Îã®ÏúÑ)\n            pseudo_masks = (preds > threshold).float()\n            for i in range(images.size(0)):\n                # Í∞Å Ïù¥ÎØ∏ÏßÄÏôÄ Ìï¥Îãπ pseudo maskÎ•º CPUÎ°ú Í∞ÄÏ†∏ÏôÄ Ï†ÄÏû•\n                pseudo_data.append((images[i].cpu(), pseudo_masks[i].cpu()))\n    return pseudo_data\n\n###############################################\n# Combined Dataset: labeled + pseudo labeled Îç∞Ïù¥ÌÑ∞\n###############################################\nclass CombinedDataset(Dataset):\n    def __init__(self, labeled_dataset, pseudo_data):\n        self.labeled_dataset = labeled_dataset\n        self.pseudo_data = pseudo_data\n\n    def __len__(self):\n        return len(self.labeled_dataset) + len(self.pseudo_data)\n    \n    def __getitem__(self, index):\n        if index < len(self.labeled_dataset):\n            return self.labeled_dataset[index]\n        else:\n            pseudo_index = index - len(self.labeled_dataset)\n            image, mask = self.pseudo_data[pseudo_index]\n            # pseudo Îç∞Ïù¥ÌÑ∞Ïùò labelÏùÄ 'pseudo'Î°ú Íµ¨Î∂Ñ\n            return image, mask, \"pseudo\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:11:26.269769Z","iopub.execute_input":"2025-03-15T09:11:26.270146Z","iopub.status.idle":"2025-03-15T09:11:26.301681Z","shell.execute_reply.started":"2025-03-15T09:11:26.270110Z","shell.execute_reply":"2025-03-15T09:11:26.300865Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\n\n# /kaggle/working/ Í≤ΩÎ°úÏóê \"unlabeled-busi-images\" Ìè¥Îçî ÏÉùÏÑ±\nfolder_path = '/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/'\nos.makedirs(folder_path, exist_ok=True)\nprint(f\"Folder created: {folder_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:11:31.066240Z","iopub.execute_input":"2025-03-15T09:11:31.066584Z","iopub.status.idle":"2025-03-15T09:11:31.072857Z","shell.execute_reply.started":"2025-03-15T09:11:31.066562Z","shell.execute_reply":"2025-03-15T09:11:31.072177Z"}},"outputs":[{"name":"stdout","text":"Folder created: /kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler, autocast\nfrom tqdm import tqdm\nimport copy\n\n# 1. Device, Î™®Îç∏, Optimizer, Scheduler Îì± Ï¥àÍ∏∞ ÏÑ§Ï†ï\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ÎùºÎ≤® Îç∞Ïù¥ÌÑ∞ÏÖãÍ≥º unlabeled Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nlabeled_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\n\nunlabeled_data_path = '/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/'  # Ïã§Ï†ú unlabeled Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°úÎ°ú ÏàòÏ†ï\nunlabeled_dataset = UnlabeledDataset(unlabeled_data_path, transform=joint_transform)\n\nlabeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True, num_workers=4)\nunlabeled_loader = DataLoader(unlabeled_dataset, batch_size=16, shuffle=False, num_workers=4)\n\n# Î™®Îç∏ ÏÉùÏÑ± (Swin Î∞±Î≥∏Ïóê img_size=256 Ï†ÅÏö©ÎêòÏñ¥ ÏûàÎã§Í≥† Í∞ÄÏ†ï)\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n\nscaler = GradScaler()\npatience = 15\nbest_val_f1 = 0.0\npatience_counter = 0\n\n# 2. Ï¥àÍ∏∞ ÌïôÏäµ (ÎùºÎ≤® Îç∞Ïù¥ÌÑ∞Îßå ÏÇ¨Ïö©, Ïòà: 10 ÏóêÌè≠)\nnum_initial_epochs = 10\nprint(\"===> Initial training on labeled data...\")\n\nfor epoch in range(num_initial_epochs):\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_initial_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    total_samples = 0\n\n    for images, masks, _ in tqdm(labeled_loader, desc=f\"Initial Epoch {epoch+1}/{num_initial_epochs}\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        # CutMix Ï†ÅÏö© (ÏÑ†ÌÉù ÏÇ¨Ìï≠)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        bs = images.size(0)\n        epoch_loss += loss.item() * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples\n    print(f\"Initial Epoch {epoch+1}/{num_initial_epochs} | Loss: {avg_loss:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:11:36.687000Z","iopub.execute_input":"2025-03-15T09:11:36.687343Z","iopub.status.idle":"2025-03-15T09:22:20.833743Z","shell.execute_reply.started":"2025-03-15T09:11:36.687317Z","shell.execute_reply":"2025-03-15T09:22:20.832443Z"}},"outputs":[{"name":"stdout","text":"===> Initial training on labeled data...\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:24<00:00,  1.73s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 1/10 | Loss: 1.0146\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 2/10 | Loss: 0.9148\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 3/10 | Loss: 0.8546\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 4/10 | Loss: 0.8292\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 5/10 | Loss: 0.7959\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:01<00:00,  1.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 6/10 | Loss: 0.7556\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 7/10 | Loss: 0.7172\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 8/10 | Loss: 0.7016\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:01<00:00,  1.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 9/10 | Loss: 0.6822\n","output_type":"stream"},{"name":"stderr","text":"Initial Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:01<00:00,  1.26s/it]","output_type":"stream"},{"name":"stdout","text":"Initial Epoch 10/10 | Loss: 0.6603\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from skimage.measure import label as sk_label, regionprops\n\ndef postprocess_mask(mask, min_size=100):\n    # skimage.measureÏùò label Ìï®ÏàòÎ•º sk_labelÎ°ú Ìò∏Ï∂ú\n    labeled_mask = sk_label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n# 3. Pseudo Labeling: unlabeled Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ Î™®Îç∏ ÏòàÏ∏° ÏàòÌñâ\nprint(\"===> Generating pseudo labels on unlabeled data...\")\npseudo_data = generate_pseudo_labels(model, unlabeled_loader, device, threshold=0.9)\nprint(f\"Pseudo labels generated for {len(pseudo_data)} images.\")\n\n# 4. ÎùºÎ≤® Îç∞Ïù¥ÌÑ∞ÏôÄ pseudo labeled Îç∞Ïù¥ÌÑ∞Î•º Ìï©Ï≥ê CombinedDataset ÏÉùÏÑ±\ncombined_dataset = CombinedDataset(labeled_dataset, pseudo_data)\ncombined_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True, num_workers=4)\n\n# 5. Fine-tuning on Combined Dataset (labeled + pseudo labeled)\nnum_finetune_epochs = 20\nprint(\"===> Fine-tuning on combined dataset (labeled + pseudo labeled)...\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:22:20.835730Z","iopub.execute_input":"2025-03-15T09:22:20.836112Z","iopub.status.idle":"2025-03-15T09:22:21.060944Z","shell.execute_reply.started":"2025-03-15T09:22:20.836075Z","shell.execute_reply":"2025-03-15T09:22:21.059589Z"}},"outputs":[{"name":"stdout","text":"===> Generating pseudo labels on unlabeled data...\nPseudo labels generated for 0 images.\n===> Fine-tuning on combined dataset (labeled + pseudo labeled)...\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"from torch.cuda.amp import GradScaler, autocast\nfrom tqdm import tqdm\nimport copy\n\n# Í∏∞Î≥∏ ÏÑ§Ï†ï\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ± (Í≤ΩÎ°úÎäî Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°úÎ°ú ÏàòÏ†ï)\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nlabeled_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\nunlabeled_data_path = '/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/'\nunlabeled_dataset = UnlabeledDataset(unlabeled_data_path, transform=joint_transform)\n\n# DataLoader (ÎîîÎ≤ÑÍπÖÏùÑ ÏúÑÌï¥ num_workers=0 ÏÇ¨Ïö© - ÌïÑÏöî Ïãú Í∞íÏùÑ ÎäòÎ¶¨ÏÑ∏Ïöî)\nlabeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True, num_workers=0)\nval_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=False, num_workers=0)\n\n# Î™®Îç∏ ÏÉùÏÑ± (Î∞±Î≥∏ ÏÉùÏÑ± Ïãú img_size=256 Ï†ÅÏö©ÎêòÏñ¥ ÏûàÎã§Í≥† Í∞ÄÏ†ï)\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\n# ÏòµÌã∞ÎßàÏù¥Ï†ÄÏôÄ scheduler ÏÑ§Ï†ï: ÌïôÏäµÎ•†ÏùÑ ÎÇÆÍ≤å ÏÑ§Ï†ïÌïòÏó¨ ÏïàÏ†ïÌôî (Ïòà: 1e-5)\noptimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\nscaler = GradScaler()\n\n# Early stopping ÏÑ§Ï†ï\npatience = 50\nbest_val_f1 = 0.0\npatience_counter = 0\n\n# Ïò®ÎèÑ Ïä§ÏºÄÏ§Ñ: Ïò®ÎèÑÍ∞Ä ÏµúÏÜå 2.0 Ïù¥ÌïòÎ°ú ÎÇ¥Î†§Í∞ÄÏßÄ ÏïäÎèÑÎ°ù Ìï® (ÏµúÎåÄ 5.0ÏóêÏÑú ÏÑ†Ìòï Í∞êÏÜå)\nmin_temp = 2.0\nmax_temp = 5.0\n\nnum_epochs = 500\ncombined_loader = None  # Ïù¥ÌõÑ pseudo labeled Îç∞Ïù¥ÌÑ∞Î•º ÏúÑÌïú DataLoader\n\nfor epoch in range(num_epochs):\n    # Ïò®ÎèÑ Ï°∞Ï†à: ÏÑ†ÌòïÏ†ÅÏúºÎ°ú Í∞êÏÜåÌïòÎêò ÏµúÏÜåÍ∞íÏùÄ min_tempÎ°ú Ïú†ÏßÄ\n    current_temp = max_temp - ((max_temp - min_temp) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    # 0~29 epoch: labeled Îç∞Ïù¥ÌÑ∞Îßå ÏÇ¨Ïö©, Ïù¥ÌõÑÎ∂ÄÌÑ∞ combined_loader ÏÇ¨Ïö©\n    if epoch < 30:\n        train_loader = labeled_loader\n    else:\n        # epoch 30ÏóêÏÑú Ìïú Î≤àÎßå pseudo labeling ÏÉùÏÑ±\n        if epoch == 30:\n            print(\"===> Generating pseudo labels on unlabeled data...\")\n            pseudo_loader = DataLoader(unlabeled_dataset, batch_size=16, shuffle=False, num_workers=0)\n            pseudo_data = generate_pseudo_labels(model, pseudo_loader, device, threshold=0.9)\n            print(f\"Pseudo labels generated for {len(pseudo_data)} images.\")\n            combined_dataset = CombinedDataset(labeled_dataset, pseudo_data)\n            train_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True, num_workers=0)\n            combined_loader = train_loader\n        else:\n            train_loader = combined_loader\n\n    # Training phase\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        # CutMix Ï†ÅÏö© (ÏòµÏÖò)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6)\n        # ÎßåÏïΩ ÏÜêÏã§Í∞íÏù¥ nanÏù¥Î©¥ Ìï¥Îãπ Î∞∞ÏπòÎ•º Í±¥ÎÑàÎõ∞Í∏∞\n        if torch.isnan(loss):\n            print(\"Warning: loss is NaN, skipping batch\")\n            continue\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        # Îçî ÏóÑÍ≤©Ìïú gradient clipping: max_norm=0.5\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n        scaler.step(optimizer)\n        scaler.update()\n\n        bs = images.size(0)\n        epoch_loss += loss.item() * bs\n\n        # Î∞∞ÏπòÎ≥Ñ metric Í≥ÑÏÇ∞ (compute_batch_metrics_new Ìï®ÏàòÎäî ÎÇ¥Î∂ÄÏóêÏÑú GPU Ïó∞ÏÇ∞ ÎåÄÏã† NumPyÎ°ú Î≥ÄÌôòÌïòÎäî Î∂ÄÎ∂ÑÏù¥ ÏûàÏúºÎØÄÎ°ú, Ïó¨Í∏∞ÏÑúÎäî Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©)\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-5)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples if total_samples > 0 else float('nan')\n    avg_auc = epoch_auc / total_samples if total_samples > 0 else float('nan')\n    avg_iou = epoch_iou / total_samples if total_samples > 0 else float('nan')\n    avg_f1 = epoch_f1 / total_samples if total_samples > 0 else float('nan')\n    avg_precision = epoch_precision / total_samples if total_samples > 0 else float('nan')\n    avg_recall = epoch_recall / total_samples if total_samples > 0 else float('nan')\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, \"\n          f\"Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    # Validation ÌèâÍ∞Ä (ÎùºÎ≤® Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©)\n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(\n        model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, \"\n          f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n    \n    # Early stopping: validation Dice/F1Í∞Ä Í∞úÏÑ†ÎêòÏßÄ ÏïäÏúºÎ©¥ patience_counter Ï¶ùÍ∞Ä\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model_with_pseudo.pth')\n        patience_counter = 0\n        print(\"üçÄ New best validation Dice achieved! Dice: {:.4f} üçÄ\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"Training complete. Best model saved as 'best_model_with_pseudo.pth'.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-15T09:03:08.247793Z","iopub.execute_input":"2025-03-15T09:03:08.248038Z","execution_failed":"2025-03-15T09:09:20.796Z"},"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"from skimage.measure import label as sk_label, regionprops\n\ndef postprocess_mask(mask, min_size=100):\n    # skimage.measureÏùò label Ìï®ÏàòÎ•º sk_labelÎ°ú Ìò∏Ï∂ú\n    labeled_mask = sk_label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\ndef compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach()\n    processed_preds = []\n    for i in range(pred_probs.size(0)):\n        pred_np = pred_probs[i].cpu().numpy()[0]\n        pred_bin = (pred_np > threshold).astype(np.uint8)\n        processed = postprocess_mask(pred_bin, min_size=100)\n        processed_preds.append(processed)\n    batch_iou = []\n    batch_f1 = []\n    batch_precision = []\n    batch_recall = []\n    for i in range(pred_probs.size(0)):\n        pred = processed_preds[i]\n        gt = masks[i].cpu().numpy()[0]\n        intersection = np.sum(pred * gt)\n        union = np.sum(pred) + np.sum(gt) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        batch_iou.append(iou)\n        precision = intersection / (np.sum(pred) + smooth)\n        recall = intersection / (np.sum(gt) + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        batch_f1.append(f1)\n        batch_precision.append(precision)\n        batch_recall.append(recall)\n    # AUC Í≥ÑÏÇ∞: ground truthÏóê 0Í≥º 1Ïù¥ Î™®Îëê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏúºÎ©¥ 0ÏúºÎ°ú Î∞òÌôò\n    probs = pred_probs.cpu().numpy().flatten()\n    masks_np = masks.cpu().numpy().flatten()\n    if np.all(masks_np == 0) or np.all(masks_np == 1):\n        auc_score = 0.0\n    else:\n        try:\n            auc_score = roc_auc_score(masks_np, probs)\n        except Exception:\n            auc_score = 0.0\n    return auc_score, np.mean(batch_iou), np.mean(batch_f1), np.mean(batch_precision), np.mean(batch_recall)\n\ndef compute_hd_metric(outputs, masks, threshold=0.5):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n    masks_np = masks.detach().cpu().numpy()\n    hd_list = []\n    for i in range(outputs.size(0)):\n        pred_bin = (pred_probs[i, 0] > threshold).astype(np.uint8)\n        gt_bin = (masks_np[i, 0] > 0.5).astype(np.uint8)\n        # Îëê ÎßàÏä§ÌÅ¨ Î™®Îëê ÎπÑÏñ¥ÏûàÏúºÎ©¥ hd=0, ÌïúÏ™ΩÎßå ÎπÑÏñ¥ÏûàÏúºÎ©¥ hd=100 (ÏûÑÏùòÎ°ú ÎÜíÏùÄ Í∞í)\n        if np.sum(gt_bin)==0 and np.sum(pred_bin)==0:\n            hd = 0.0\n        elif np.sum(gt_bin)==0 or np.sum(pred_bin)==0:\n            hd = 100.0\n        else:\n            try:\n                hd = medpy_binary.hd95(pred_bin, gt_bin)\n            except Exception:\n                hd = np.nan\n        hd_list.append(hd)\n    return np.nanmean(hd_list) if len(hd_list) > 0 else 0.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:33:07.601231Z","iopub.execute_input":"2025-03-15T09:33:07.601612Z","iopub.status.idle":"2025-03-15T09:33:07.614894Z","shell.execute_reply.started":"2025-03-15T09:33:07.601588Z","shell.execute_reply":"2025-03-15T09:33:07.613975Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler, autocast\nfrom tqdm import tqdm\nimport copy\n\n# ------------------------------\n# Í∏∞Î≥∏ ÏÑ§Ï†ï: device, Îç∞Ïù¥ÌÑ∞ÏÖã, DataLoader, Î™®Îç∏, optimizer, scheduler\n# ------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ÎùºÎ≤® Îç∞Ïù¥ÌÑ∞ÏÖã (BUSI Îç∞Ïù¥ÌÑ∞)\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nlabeled_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\n\n# unlabeled Îç∞Ïù¥ÌÑ∞ÏÖã (Í≤ΩÎ°úÎäî Ïã§Ï†ú unlabeled Îç∞Ïù¥ÌÑ∞ ÏúÑÏπòÎ°ú ÏàòÏ†ï)\nunlabeled_data_path = '/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/'\nunlabeled_dataset = UnlabeledDataset(unlabeled_data_path, transform=joint_transform)\n\n# DataLoader ÏÑ§Ï†ï\nlabeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True, num_workers=0)\nval_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=False, num_workers=0)  # Í≤ÄÏ¶ùÏùÄ ÎùºÎ≤® Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©\n\n# Î™®Îç∏ ÏÉùÏÑ± (Î∞±Î≥∏ ÏÉùÏÑ± Ïãú img_size=256 Ï†ÅÏö©ÎêòÏñ¥ ÏûàÎã§Í≥† Í∞ÄÏ†ï)\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\nscaler = GradScaler()\n\n# Early stopping ÏÑ§Ï†ï: patience 15\npatience = 15\nbest_val_f1 = 0.0\npatience_counter = 0\n\n# ------------------------------\n# Training Loop: Ï†ÑÏ≤¥ 500 epoch\n# 0~29 epoch: labeled Îç∞Ïù¥ÌÑ∞Îßå ÏÇ¨Ïö©\n# 30 epochÎ∂ÄÌÑ∞: pseudo labelingÏùÑ ÏÉùÏÑ±ÌïòÏó¨ labeled + pseudo Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµ\n# ------------------------------\nnum_epochs = 500\ncombined_loader = None  # Ïù¥ÌõÑÏóê ÏÇ¨Ïö©Ìï† combined Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n\nfor epoch in range(num_epochs):\n    # Ïò®ÎèÑ Ï°∞Ï†à (Ïòà: 5.0 -> 1.0Î°ú ÏÑ†Ìòï Í∞êÏÜå)\n    current_temp = 5.0 - ((5.0 - 1.0) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    # 0~29 epoch: labeled_loader ÏÇ¨Ïö©, Ïù¥ÌõÑÎ∂ÄÌÑ∞ combined_loader ÏÇ¨Ïö©\n    if epoch < 30:\n        train_loader = labeled_loader\n    else:\n        # epoch 30ÏóêÏÑú Ìïú Î≤àÎßå pseudo labeling ÏÉùÏÑ±\n        if epoch == 30:\n            print(\"===> Generating pseudo labels on unlabeled data...\")\n            pseudo_data = generate_pseudo_labels(model, DataLoader(unlabeled_dataset, batch_size=16, shuffle=False, num_workers=4), device, threshold=0.9)\n            print(f\"Pseudo labels generated for {len(pseudo_data)} images.\")\n            # ÎùºÎ≤® Îç∞Ïù¥ÌÑ∞ÏôÄ pseudo Îç∞Ïù¥ÌÑ∞Î•º Í≤∞Ìï©ÌïòÏó¨ CombinedDataset ÏÉùÏÑ±\n            combined_dataset = CombinedDataset(labeled_dataset, pseudo_data)\n            train_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True, num_workers=4)\n            combined_loader = train_loader\n        else:\n            train_loader = combined_loader\n\n    # Training phase\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        bs = images.size(0)\n        epoch_loss += loss.item() * bs\n\n        # Î∞∞ÏπòÎ≥Ñ metric Í≥ÑÏÇ∞\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-6)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples\n    avg_auc = epoch_auc / total_samples\n    avg_iou = epoch_iou / total_samples\n    avg_f1 = epoch_f1 / total_samples\n    avg_precision = epoch_precision / total_samples\n    avg_recall = epoch_recall / total_samples\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    # Validation ÌèâÍ∞Ä (ÎùºÎ≤® Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©)\n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(\n        model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n    \n    # Early stopping: validation Dice/F1Í∞Ä Í∞úÏÑ†ÎêòÏßÄ ÏïäÏúºÎ©¥ patience_counter Ï¶ùÍ∞Ä\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model_with_pseudo.pth')\n        patience_counter = 0\n        print(\"üçÄ New best validation Dice achieved! Dice: {:.4f} üçÄ\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"Training complete. Best model saved as 'best_model_with_pseudo.pth'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T09:33:11.336155Z","iopub.execute_input":"2025-03-15T09:33:11.336493Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:47<00:00,  2.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500 | Loss: 1.0411, AUC: 0.8724, IoU: 0.4025, Dice/F1: 0.4417, Precision: 0.4259, Recall: 0.6278 (Temp: 5.00)\n[Val] Loss: 1.0729, AUC: 0.8207, IoU: 0.4467, Dice/F1: 0.5280, Precision: 0.4490, Recall: 0.7296, HD95: 54.2330\nüçÄ New best validation Dice achieved! Dice: 0.5280 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:41<00:00,  2.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/500 | Loss: 0.9619, AUC: 0.9496, IoU: 0.5168, Dice/F1: 0.5436, Precision: 0.5833, Recall: 0.5872 (Temp: 4.99)\n[Val] Loss: 1.0467, AUC: 0.8288, IoU: 0.5386, Dice/F1: 0.5379, Precision: 0.4766, Recall: 0.6975, HD95: 35.4523\nüçÄ New best validation Dice achieved! Dice: 0.5379 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:41<00:00,  2.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/500 | Loss: 0.9125, AUC: 0.9472, IoU: 0.5606, Dice/F1: 0.5732, Precision: 0.5921, Recall: 0.6266 (Temp: 4.98)\n[Val] Loss: 0.9464, AUC: 0.8183, IoU: 0.6432, Dice/F1: 0.5992, Precision: 0.7003, Recall: 0.5519, HD95: 28.1701\nüçÄ New best validation Dice achieved! Dice: 0.5992 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:41<00:00,  2.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/500 | Loss: 0.8651, AUC: 0.9588, IoU: 0.5884, Dice/F1: 0.5935, Precision: 0.6205, Recall: 0.6257 (Temp: 4.98)\n[Val] Loss: 0.9136, AUC: 0.8297, IoU: 0.6902, Dice/F1: 0.6459, Precision: 0.6442, Recall: 0.6849, HD95: 26.8383\nüçÄ New best validation Dice achieved! Dice: 0.6459 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:41<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/500 | Loss: 0.8130, AUC: 0.9564, IoU: 0.6242, Dice/F1: 0.6133, Precision: 0.6348, Recall: 0.6455 (Temp: 4.97)\n[Val] Loss: 0.7970, AUC: 0.8294, IoU: 0.7449, Dice/F1: 0.6575, Precision: 0.6984, Recall: 0.6487, HD95: 17.9238\nüçÄ New best validation Dice achieved! Dice: 0.6575 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/500 | Loss: 0.7998, AUC: 0.9636, IoU: 0.6401, Dice/F1: 0.6295, Precision: 0.6456, Recall: 0.6645 (Temp: 4.96)\n[Val] Loss: 0.7831, AUC: 0.8403, IoU: 0.7529, Dice/F1: 0.6628, Precision: 0.6818, Recall: 0.6698, HD95: 17.3736\nüçÄ New best validation Dice achieved! Dice: 0.6628 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/500 | Loss: 0.7372, AUC: 0.9723, IoU: 0.6921, Dice/F1: 0.6530, Precision: 0.6800, Recall: 0.6640 (Temp: 4.95)\n[Val] Loss: 0.8010, AUC: 0.8445, IoU: 0.7595, Dice/F1: 0.6768, Precision: 0.6627, Recall: 0.7132, HD95: 16.8521\nüçÄ New best validation Dice achieved! Dice: 0.6768 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:41<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/500 | Loss: 0.7493, AUC: 0.9778, IoU: 0.6717, Dice/F1: 0.6528, Precision: 0.6763, Recall: 0.6743 (Temp: 4.94)\n[Val] Loss: 0.7432, AUC: 0.8478, IoU: 0.7865, Dice/F1: 0.6952, Precision: 0.6884, Recall: 0.7191, HD95: 14.5468\nüçÄ New best validation Dice achieved! Dice: 0.6952 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/500 | Loss: 0.7089, AUC: 0.9833, IoU: 0.6902, Dice/F1: 0.6624, Precision: 0.6856, Recall: 0.6757 (Temp: 4.94)\n[Val] Loss: 0.6951, AUC: 0.8484, IoU: 0.8020, Dice/F1: 0.7038, Precision: 0.7098, Recall: 0.7136, HD95: 13.0606\nüçÄ New best validation Dice achieved! Dice: 0.7038 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/500 | Loss: 0.6972, AUC: 0.9854, IoU: 0.7224, Dice/F1: 0.6941, Precision: 0.7109, Recall: 0.7050 (Temp: 4.93)\n[Val] Loss: 0.6928, AUC: 0.8492, IoU: 0.8072, Dice/F1: 0.7088, Precision: 0.7163, Recall: 0.7158, HD95: 12.6782\nüçÄ New best validation Dice achieved! Dice: 0.7088 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/500 | Loss: 0.6853, AUC: 0.9874, IoU: 0.7355, Dice/F1: 0.6881, Precision: 0.7024, Recall: 0.7029 (Temp: 4.92)\n[Val] Loss: 0.6853, AUC: 0.8505, IoU: 0.8136, Dice/F1: 0.7094, Precision: 0.7145, Recall: 0.7191, HD95: 12.0262\nüçÄ New best validation Dice achieved! Dice: 0.7094 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/500 | Loss: 0.7033, AUC: 0.9791, IoU: 0.7045, Dice/F1: 0.6580, Precision: 0.6829, Recall: 0.6688 (Temp: 4.91)\n[Val] Loss: 0.8074, AUC: 0.8476, IoU: 0.7770, Dice/F1: 0.7064, Precision: 0.6906, Recall: 0.7375, HD95: 16.9035\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/500 Training:  16%|‚ñà‚ñã        | 8/49 [00:16<01:24,  2.07s/it]","output_type":"stream"},{"name":"stdout","text":"[Val] Loss: 0.7330, AUC: 0.8485, IoU: 0.7989, Dice/F1: 0.7055, Precision: 0.7069, Recall: 0.7186, HD95: 13.8151\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:39<00:00,  2.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/500 | Loss: 0.7098, AUC: 0.9848, IoU: 0.6932, Dice/F1: 0.6739, Precision: 0.6992, Recall: 0.6955 (Temp: 4.90)\n[Val] Loss: 0.7033, AUC: 0.8491, IoU: 0.7952, Dice/F1: 0.7062, Precision: 0.7279, Recall: 0.7028, HD95: 14.5662\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:41<00:00,  2.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/500 | Loss: 0.6825, AUC: 0.9858, IoU: 0.7220, Dice/F1: 0.6760, Precision: 0.6935, Recall: 0.6928 (Temp: 4.89)\n[Val] Loss: 0.6796, AUC: 0.8499, IoU: 0.8073, Dice/F1: 0.7103, Precision: 0.7140, Recall: 0.7253, HD95: 14.2156\nüçÄ New best validation Dice achieved! Dice: 0.7103 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/500 | Loss: 0.6644, AUC: 0.9865, IoU: 0.7305, Dice/F1: 0.6813, Precision: 0.7016, Recall: 0.6958 (Temp: 4.88)\n[Val] Loss: 0.6798, AUC: 0.8511, IoU: 0.8094, Dice/F1: 0.7163, Precision: 0.7374, Recall: 0.7112, HD95: 13.0505\nüçÄ New best validation Dice achieved! Dice: 0.7163 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/500 | Loss: 0.6410, AUC: 0.9900, IoU: 0.7484, Dice/F1: 0.6918, Precision: 0.7100, Recall: 0.7040 (Temp: 4.87)\n[Val] Loss: 0.6598, AUC: 0.8524, IoU: 0.8230, Dice/F1: 0.7236, Precision: 0.7219, Recall: 0.7400, HD95: 11.6106\nüçÄ New best validation Dice achieved! Dice: 0.7236 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/500 | Loss: 0.6433, AUC: 0.9891, IoU: 0.7370, Dice/F1: 0.6924, Precision: 0.7157, Recall: 0.7032 (Temp: 4.86)\n[Val] Loss: 0.5870, AUC: 0.8512, IoU: 0.8356, Dice/F1: 0.7288, Precision: 0.7340, Recall: 0.7355, HD95: 10.1023\nüçÄ New best validation Dice achieved! Dice: 0.7288 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/500 | Loss: 0.6225, AUC: 0.9905, IoU: 0.7419, Dice/F1: 0.7063, Precision: 0.7294, Recall: 0.7141 (Temp: 4.86)\n[Val] Loss: 0.5731, AUC: 0.8530, IoU: 0.8371, Dice/F1: 0.7276, Precision: 0.7208, Recall: 0.7456, HD95: 10.0082\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/500 | Loss: 0.6024, AUC: 0.9920, IoU: 0.7615, Dice/F1: 0.7171, Precision: 0.7330, Recall: 0.7264 (Temp: 4.85)\n[Val] Loss: 0.5905, AUC: 0.8533, IoU: 0.8385, Dice/F1: 0.7277, Precision: 0.7345, Recall: 0.7331, HD95: 10.3703\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/500 | Loss: 0.6161, AUC: 0.9908, IoU: 0.7480, Dice/F1: 0.7075, Precision: 0.7246, Recall: 0.7233 (Temp: 4.84)\n[Val] Loss: 0.5568, AUC: 0.8536, IoU: 0.8460, Dice/F1: 0.7344, Precision: 0.7464, Recall: 0.7345, HD95: 9.4208\nüçÄ New best validation Dice achieved! Dice: 0.7344 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/500 | Loss: 0.5878, AUC: 0.9931, IoU: 0.7615, Dice/F1: 0.7120, Precision: 0.7297, Recall: 0.7199 (Temp: 4.83)\n[Val] Loss: 0.5992, AUC: 0.8538, IoU: 0.8411, Dice/F1: 0.7317, Precision: 0.7225, Recall: 0.7522, HD95: 9.8819\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:41<00:00,  2.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/500 | Loss: 0.5679, AUC: 0.9948, IoU: 0.7809, Dice/F1: 0.7230, Precision: 0.7373, Recall: 0.7299 (Temp: 4.82)\n[Val] Loss: 0.5680, AUC: 0.8539, IoU: 0.8446, Dice/F1: 0.7345, Precision: 0.7248, Recall: 0.7556, HD95: 9.7436\nüçÄ New best validation Dice achieved! Dice: 0.7345 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:41<00:00,  2.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/500 | Loss: 0.5712, AUC: 0.9938, IoU: 0.7827, Dice/F1: 0.7211, Precision: 0.7287, Recall: 0.7348 (Temp: 4.82)\n[Val] Loss: 0.5736, AUC: 0.8541, IoU: 0.8534, Dice/F1: 0.7401, Precision: 0.7354, Recall: 0.7545, HD95: 9.4841\nüçÄ New best validation Dice achieved! Dice: 0.7401 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/500 | Loss: 0.5525, AUC: 0.9940, IoU: 0.7896, Dice/F1: 0.7353, Precision: 0.7426, Recall: 0.7483 (Temp: 4.81)\n[Val] Loss: 0.5249, AUC: 0.8543, IoU: 0.8551, Dice/F1: 0.7399, Precision: 0.7596, Recall: 0.7321, HD95: 8.7593\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26/500 | Loss: 0.5583, AUC: 0.9938, IoU: 0.7866, Dice/F1: 0.7353, Precision: 0.7504, Recall: 0.7424 (Temp: 4.80)\n[Val] Loss: 0.5313, AUC: 0.8545, IoU: 0.8615, Dice/F1: 0.7450, Precision: 0.7455, Recall: 0.7535, HD95: 8.5376\nüçÄ New best validation Dice achieved! Dice: 0.7450 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:40<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27/500 | Loss: 0.5379, AUC: 0.9958, IoU: 0.7882, Dice/F1: 0.7248, Precision: 0.7348, Recall: 0.7339 (Temp: 4.79)\n[Val] Loss: 0.5061, AUC: 0.8545, IoU: 0.8627, Dice/F1: 0.7444, Precision: 0.7520, Recall: 0.7450, HD95: 8.0646\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/500 Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/49 [00:51<00:48,  2.04s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Best model Í∞ÄÏ§ëÏπò Î°úÎìú (ÌååÏùºÎ™ÖÏùÄ ÌïôÏäµ Ïãú Ï†ÄÏû•Ìïú ÌååÏùºÎ™ÖÏúºÎ°ú Î≥ÄÍ≤Ω)\nmodel.load_state_dict(torch.load(\"best_model_with_pseudo.pth\", map_location=device))\n\n# Î™®Îç∏ÏùÑ evaluation Î™®ÎìúÎ°ú Ï†ÑÌôò\nmodel.eval()\nwith torch.no_grad():\n    test_loss, test_auc, test_iou, test_f1, test_precision, test_recall, test_hd = evaluate_with_metrics(\n        model, test_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=pos_weight\n    )\n\nprint(f\"[Test] Loss: {test_loss:.4f}, AUC: {test_auc:.4f}, IoU: {test_iou:.4f}, Dice/F1: {test_f1:.4f}, \"\n      f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, HD95: {test_hd:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:47:17.984664Z","iopub.execute_input":"2025-03-15T11:47:17.985067Z","iopub.status.idle":"2025-03-15T11:47:29.951580Z","shell.execute_reply.started":"2025-03-15T11:47:17.985031Z","shell.execute_reply":"2025-03-15T11:47:29.950307Z"}},"outputs":[{"name":"stdout","text":"[Test] Loss: 0.4169, AUC: 0.9989, IoU: 0.8918, Dice/F1: 0.7728, Precision: 0.7755, Recall: 0.7755, HD95: 5.6043\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"a = 1\na","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# full code","metadata":{}},{"cell_type":"code","source":"! pip install medpy\n! pip install segmentation_models_pytorch\n! pip install -U albumentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T08:28:01.485328Z","iopub.execute_input":"2025-03-15T08:28:01.485596Z","iopub.status.idle":"2025-03-15T08:28:27.515898Z","shell.execute_reply.started":"2025-03-15T08:28:01.485562Z","shell.execute_reply":"2025-03-15T08:28:27.515028Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting medpy\n  Downloading medpy-0.5.2.tar.gz (156 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from medpy) (1.13.1)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from medpy) (1.26.4)\nRequirement already satisfied: SimpleITK>=2.1 in /usr/local/lib/python3.10/dist-packages (from medpy) (2.4.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->medpy) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->medpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->medpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->medpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->medpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->medpy) (2024.2.0)\nBuilding wheels for collected packages: medpy\n  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for medpy: filename=MedPy-0.5.2-cp310-cp310-linux_x86_64.whl size=762835 sha256=045e8a1051aaa6bff85809d7c68395da517aa5dae22f0fa368964d5b67899b5f\n  Stored in directory: /root/.cache/pip/wheels/a1/b8/63/bdf557940ec60d1b8822e73ff9fbe7727ac19f009d46b5d175\nSuccessfully built medpy\nInstalling collected packages: medpy\nSuccessfully installed medpy-0.5.2\nCollecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\nCollecting efficientnet-pytorch>=0.6.1 (from segmentation_models_pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.29.0)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (11.0.0)\nCollecting pretrainedmodels>=0.7.1 (from segmentation_models_pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.17.0)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.0.12)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2.4.1)\nCollecting munch (from pretrainedmodels>=0.7.1->segmentation_models_pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9->segmentation_models_pytorch) (0.4.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation_models_pytorch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nDownloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=b8a602d8f8bb6772c8df742c89576c6283dc839dbadb0bba27ea1891a9716ba3\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=aa69f5f0f1582f2a4f6fb5e9d93a70ffd412619058e938e26d91f805e6acb0ab\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, pretrainedmodels, segmentation_models_pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.4.0\nRequirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\nCollecting albumentations\n  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\nCollecting albucore==0.0.23 (from albumentations)\n  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\nCollecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nDownloading albumentations-2.0.5-py3-none-any.whl (290 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\nDownloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\n  Attempting uninstall: albucore\n    Found existing installation: albucore 0.0.19\n    Uninstalling albucore-0.0.19:\n      Successfully uninstalled albucore-0.0.19\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.4.20\n    Uninstalling albumentations-1.4.20:\n      Successfully uninstalled albumentations-1.4.20\nSuccessfully installed albucore-0.0.23 albumentations-2.0.5 simsimd-6.2.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport re\nimport random\nimport warnings\nimport numpy as np\nfrom collections import defaultdict\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage.segmentation import find_boundaries\nfrom skimage.measure import label as sk_label, regionprops\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom medpy.metric import binary as medpy_binary\nfrom torchvision.transforms import ColorJitter\n\nimport os\n\n# /kaggle/working/ Í≤ΩÎ°úÏóê \"unlabeled-busi-images\" Ìè¥Îçî ÏÉùÏÑ±\nfolder_path = '/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/'\nos.makedirs(folder_path, exist_ok=True)\nprint(f\"Folder created: {folder_path}\")\n\n\n# ------------------------------\n# Seed Î∞è Warning ÏÑ§Ï†ï\n# ------------------------------\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\nwarnings.filterwarnings('ignore')\n\n# ------------------------------\n# BUSI Segmentation Dataset\n# ------------------------------\nclass BUSISegmentationDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform  \n        self.samples = []\n        self._prepare_samples()\n\n    def _prepare_samples(self):\n        labels = os.listdir(self.data_path)\n        for label in labels:\n            folder_path = os.path.join(self.data_path, label)\n            if not os.path.isdir(folder_path):\n                continue\n            files = os.listdir(folder_path)\n            image_files = sorted([f for f in files if '_mask' not in f and f.endswith('.png')])\n            mask_files  = sorted([f for f in files if '_mask' in f and f.endswith('.png')])\n            pattern_img = re.compile(rf'{re.escape(label)} \\((\\d+)\\)\\.png')\n            pattern_mask = re.compile(rf'{re.escape(label)} \\((\\d+)\\)_mask(?:_\\d+)?\\.png')\n            mask_dict = {}\n            for mf in mask_files:\n                m = pattern_mask.fullmatch(mf)\n                if m:\n                    idx = m.group(1)\n                    mask_dict.setdefault(idx, []).append(mf)\n            for im in image_files:\n                m = pattern_img.fullmatch(im)\n                if m:\n                    idx = m.group(1)\n                    img_path = os.path.join(folder_path, im)\n                    if idx in mask_dict:\n                        mask_paths = [os.path.join(folder_path, mf) for mf in mask_dict[idx]]\n                        combined_mask = None\n                        for mp in mask_paths:\n                            mask_img = Image.open(mp).convert('L')\n                            mask_arr = np.array(mask_img)\n                            mask_binary = (mask_arr > 128).astype(np.uint8)\n                            if combined_mask is None:\n                                combined_mask = mask_binary\n                            else:\n                                combined_mask = np.maximum(combined_mask, mask_binary)\n                        self.samples.append((img_path, combined_mask, label))\n                    else:\n                        image_pil = Image.open(img_path)\n                        empty_mask = np.zeros(image_pil.size[::-1], dtype=np.uint8)\n                        self.samples.append((img_path, empty_mask, label))\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, mask_array, label = self.samples[index]\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.fromarray((mask_array * 255).astype(np.uint8))\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        else:\n            image = transforms.ToTensor()(image)\n            mask = transforms.ToTensor()(mask)\n        return image, mask, label\n\n# ------------------------------\n# Utility Functions\n# ------------------------------\ndef z_score_normalize(tensor):\n    mean = tensor.mean()\n    std = tensor.std() + 1e-6\n    return (tensor - mean) / std\n\ndef joint_transform(image, mask, size=(256,256)):\n    geom_transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=10, p=0.5),\n        A.ElasticTransform(alpha=10, sigma=5, alpha_affine=5, p=0.3),\n        A.Resize(height=size[0], width=size[1])\n    ])\n    image_np = np.array(image)\n    mask_np = np.array(mask)\n    augmented = geom_transform(image=image_np, mask=mask_np)\n    image = augmented['image']\n    mask = augmented['mask']\n    \n    intensity_transform = A.Compose([\n        A.RandomBrightnessContrast(p=0.5),\n        A.CLAHE(clip_limit=1.0, tile_grid_size=(8,8), p=0.5),\n        A.GaussianBlur(p=0.3)\n    ])\n    image = intensity_transform(image=image)['image']\n    \n    image = transforms.ToTensor()(image)\n    image = z_score_normalize(image)\n    mask = transforms.ToTensor()(mask)\n    return image, mask\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(images, masks, alpha=1.5, p=0.7):\n    if np.random.rand() > p:\n        return images, masks\n    lam = np.random.beta(alpha, alpha)\n    rand_index = torch.randperm(images.size(0)).to(images.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n    masks[:, :, bbx1:bbx2, bby1:bby2] = masks[rand_index, :, bbx1:bbx2, bby1:bby2]\n    return images, masks\n\ndef postprocess_mask(mask, min_size=100):\n    # postprocess_mask: Îπà ÏòÅÏó≠ÏùÑ Ï†úÍ±∞ÌïòÎäî Ï≤òÎ¶¨\n    labeled_mask = sk_label(mask)\n    processed_mask = np.zeros_like(mask)\n    for region in regionprops(labeled_mask):\n        if region.area >= min_size:\n            processed_mask[labeled_mask == region.label] = 1\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n    closed = cv2.morphologyEx(processed_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    return closed\n\n# ------------------------------\n# Pseudo Labeling Í¥ÄÎ†® Dataset Î∞è Ìï®Ïàò\n# ------------------------------\nclass UnlabeledDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform\n        self.image_paths = sorted([os.path.join(data_path, fname) for fname in os.listdir(data_path) if fname.endswith('.png')])\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, index):\n        img_path = self.image_paths[index]\n        image = Image.open(img_path).convert(\"RGB\")\n        dummy_mask = Image.new(\"L\", image.size, 0)\n        if self.transform:\n            image, _ = self.transform(image, dummy_mask)\n        else:\n            image = transforms.ToTensor()(image)\n        return image, img_path\n\ndef generate_pseudo_labels(model, unlabeled_loader, device, threshold=0.9):\n    model.eval()\n    pseudo_data = []\n    with torch.no_grad():\n        for images, paths in unlabeled_loader:\n            images = images.to(device)\n            outputs = model(images)\n            preds = torch.sigmoid(outputs)\n            pseudo_masks = (preds > threshold).float()\n            for i in range(images.size(0)):\n                pseudo_data.append((images[i].cpu(), pseudo_masks[i].cpu()))\n    return pseudo_data\n\nclass CombinedDataset(Dataset):\n    def __init__(self, labeled_dataset, pseudo_data):\n        self.labeled_dataset = labeled_dataset\n        self.pseudo_data = pseudo_data\n\n    def __len__(self):\n        return len(self.labeled_dataset) + len(self.pseudo_data)\n    \n    def __getitem__(self, index):\n        if index < len(self.labeled_dataset):\n            return self.labeled_dataset[index]\n        else:\n            pseudo_index = index - len(self.labeled_dataset)\n            image, mask = self.pseudo_data[pseudo_index]\n            return image, mask, \"pseudo\"\n\n# ------------------------------\n# Model ÏïÑÌÇ§ÌÖçÏ≤ò (PretrainedSwin_UNet_AttentionFusion Îì±)\n# ------------------------------\n# ‚Äª Î™®Îç∏ Í¥ÄÎ†® ÏΩîÎìúÎäî Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©Ìï©ÎãàÎã§. (ÌïÑÏöîÌïú Í≤ΩÏö∞ Ï§ëÎ≥µÎêú Î∂ÄÎ∂ÑÏùÄ Ï†úÍ±∞ÌïòÏòÄÏùå)\nfrom timm.models import create_model\n\nclass UNetDecoder_Swin(nn.Module):\n    def __init__(self):\n        super(UNetDecoder_Swin, self).__init__()\n        self.up1 = nn.ConvTranspose2d(1536, 768, kernel_size=2, stride=2)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(768 + 768, 768, kernel_size=3, padding=1),\n            nn.BatchNorm2d(768),\n            nn.ReLU(inplace=True)\n        )\n        self.up2 = nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2)\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(384 + 384, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True)\n        )\n        self.up3 = nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2)\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(192 + 192, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n        self.up4 = nn.ConvTranspose2d(192, 64, kernel_size=2, stride=2)\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, features_list):\n        f3, f2, f1, f0 = features_list\n        x = f3\n        x = self.up1(x)\n        x = torch.cat([x, f2], dim=1)\n        x = self.conv1(x)\n        x = self.up2(x)\n        x = torch.cat([x, f1], dim=1)\n        x = self.conv2(x)\n        x = self.up3(x)\n        x = torch.cat([x, f0], dim=1)\n        x = self.conv3(x)\n        x = self.up4(x)\n        x = self.conv4(x)\n        x = F.interpolate(x, size=(256,256), mode='bilinear', align_corners=False)\n        return x\n\nclass TransformerBottleneck(nn.Module):\n    def __init__(self, d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1):\n        super(TransformerBottleneck, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                    dim_feedforward=dim_feedforward, dropout=dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    def forward(self, x):\n        B, C, H, W = x.size()\n        x = x.view(B, C, H * W).permute(2, 0, 1)\n        x = self.transformer(x)\n        x = x.permute(1, 2, 0).view(B, C, H, W)\n        return x\n\nclass PretrainedSwin_UNet_AttentionFusion(nn.Module):\n    def __init__(self, out_channels=1):\n        super(PretrainedSwin_UNet_AttentionFusion, self).__init__()\n        self.encoder = create_model('swin_large_patch4_window7_224', pretrained=True, features_only=True, img_size=256)\n        self.bottleneck = QuadAgentBlock(in_channels=1536, out_channels=1536, gating_dropout=0.3, gating_hidden_dim=32, gating_temperature=1.5, stochastic_depth_prob=0.5)\n        self.transformer_bottleneck1 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.transformer_bottleneck2 = TransformerBottleneck(d_model=1536, nhead=8, num_layers=1, dim_feedforward=2048, dropout=0.1)\n        self.decoder = UNetDecoder_Swin()\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        features_list = self.encoder(x)  # [f0, f1, f2, f3]\n\n        # (N, H, W, C) ‚Üí (N, C, H, W) Î≥ÄÌôòÏù¥ ÌïÑÏöîÌïú Í≤ΩÏö∞ Ï†ÅÏö©\n        for i in range(len(features_list)):\n            if features_list[i].shape[1] not in {192, 384, 768, 1536}:  \n                features_list[i] = features_list[i].permute(0, 3, 1, 2).contiguous()\n\n        # ÎîîÏΩîÎçîÏóê Ïò¨Î∞îÎ•∏ ÏàúÏÑúÎ°ú Ï†ÑÎã¨ÎêòÎèÑÎ°ù Î≥ÄÌôò\n        f0, f1, f2, f3 = features_list  # Swin TransformerÏùò Í∏∞Î≥∏ Î∞òÌôò ÏàúÏÑú\n        features_list = [f3, f2, f1, f0]  # (B, 1536, 7, 7) ‚Üí (B, 192, 56, 56)\n\n        # BottleneckÍ≥º Transformer Bottleneck Ï†ÅÏö©\n        b_out = self.bottleneck(features_list[0])\n        t_out1 = self.transformer_bottleneck1(b_out)\n        t_out2 = self.transformer_bottleneck2(t_out1)\n        features_list[0] = t_out2\n\n        decoded = self.decoder(features_list)\n        return self.final_conv(decoded)\n\n# ------------------------------\n# Loss Î∞è Metric Ìï®Ïàò\n# ------------------------------\nfrom segmentation_models_pytorch.losses import LovaszLoss\nlv_loss = LovaszLoss(mode='binary')\n\ndef focal_tversky_loss(pred, target, alpha=0.5, beta=0.5, gamma=4/3, smooth=1e-5):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred = pred.view(pred.size(0), -1)\n    target = target.view(target.size(0), -1)\n    tp = (pred * target).sum(dim=1)\n    fp = ((1 - target) * pred).sum(dim=1)\n    fn = (target * (1 - pred)).sum(dim=1)\n    tversky_index = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n    loss = (1 - tversky_index) ** gamma\n    return loss.mean()\n\ndef boundary_loss(pred, target):\n    pred = torch.sigmoid(pred)\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_np = pred.detach().cpu().numpy()\n    target_np = target.detach().cpu().numpy()\n    boundary_masks = []\n    for i in range(pred_np.shape[0]):\n        gt_mask = target_np[i, 0]\n        boundary = find_boundaries(gt_mask, mode='thick')\n        boundary_masks.append(boundary.astype(np.float32))\n    boundary_masks = np.stack(boundary_masks, axis=0)[:, None, :, :]\n    boundary_masks_torch = torch.from_numpy(boundary_masks).to(pred.device)\n    intersect = (pred * boundary_masks_torch).sum()\n    denom = pred.sum() + boundary_masks_torch.sum()\n    boundary_dice = (2.0 * intersect) / (denom + 1e-5)\n    return 1.0 - boundary_dice\n\ndef combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    loss_ft = focal_tversky_loss(outputs, masks)\n    if pos_weight is not None:\n        loss_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, masks.float())\n    else:\n        loss_bce = nn.BCEWithLogitsLoss()(outputs, masks.float())\n    bl = boundary_loss(outputs, masks)\n    lv = lv_loss(outputs, masks)\n    return 0.4 * lv + 0.3 * loss_ft + 0.3 * loss_bce + lambda_boundary * bl\n\ndef dice_f1_precision_recall(pred, target, threshold=0.5, smooth=1e-5):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    precision = intersection / (pred_bin.sum() + smooth)\n    recall = intersection / (target_bin.sum() + smooth)\n    f1 = 2 * (precision * recall) / (precision + recall + smooth)\n    return f1.item(), precision.item(), recall.item()\n\ndef iou_metric(pred, target, threshold=0.5, smooth=1e-5):\n    if pred.shape[-2:] != target.shape[-2:]:\n        pred = F.interpolate(pred, size=target.shape[-2:], mode='bilinear', align_corners=False)\n    pred_bin = (torch.sigmoid(pred) > threshold).float()\n    target_bin = target.float()\n    intersection = (pred_bin * target_bin).sum()\n    union = pred_bin.sum() + target_bin.sum() - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-5):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach()\n    processed_preds = []\n    for i in range(pred_probs.size(0)):\n        pred_np = pred_probs[i].cpu().numpy()[0]\n        pred_bin = (pred_np > threshold).astype(np.uint8)\n        processed = postprocess_mask(pred_bin, min_size=100)\n        processed_preds.append(processed)\n    batch_iou = []\n    batch_f1 = []\n    batch_precision = []\n    batch_recall = []\n    for i in range(pred_probs.size(0)):\n        pred = processed_preds[i]\n        gt = masks[i].cpu().numpy()[0]\n        intersection = np.sum(pred * gt)\n        union = np.sum(pred) + np.sum(gt) - intersection\n        iou = (intersection + smooth) / (union + smooth)\n        batch_iou.append(iou)\n        precision = intersection / (np.sum(pred) + smooth)\n        recall = intersection / (np.sum(gt) + smooth)\n        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n        batch_f1.append(f1)\n        batch_precision.append(precision)\n        batch_recall.append(recall)\n    probs = pred_probs.cpu().numpy().flatten()\n    masks_np = masks.cpu().numpy().flatten()\n    # AUC Í≥ÑÏÇ∞: ÎßåÏïΩ ground truthÏóê 0 ÎòêÎäî 1Îßå Ï°¥Ïû¨ÌïòÎ©¥ 0ÏùÑ Î∞òÌôò\n    if np.all(masks_np == 0) or np.all(masks_np == 1):\n        auc_score = 0.0\n    else:\n        try:\n            auc_score = roc_auc_score(masks_np, probs)\n        except ValueError:\n            auc_score = 0.0\n    return auc_score, np.mean(batch_iou), np.mean(batch_f1), np.mean(batch_precision), np.mean(batch_recall)\n\ndef compute_hd_metric(outputs, masks, threshold=0.5):\n    if outputs.shape[-2:] != masks.shape[-2:]:\n        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n    pred_probs = torch.sigmoid(outputs).detach().cpu().numpy()\n    masks_np = masks.detach().cpu().numpy()\n    hd_list = []\n    for i in range(outputs.size(0)):\n        pred_bin = (pred_probs[i, 0] > threshold).astype(np.uint8)\n        gt_bin = (masks_np[i, 0] > 0.5).astype(np.uint8)\n        try:\n            hd = medpy_binary.hd95(pred_bin, gt_bin)\n        except Exception:\n            hd = np.nan\n        hd_list.append(hd)\n    return np.nanmean(hd_list)\n\ndef evaluate_with_metrics(model, dataloader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None):\n    model.eval()\n    total_loss = 0.0\n    total_auc  = 0.0\n    total_iou  = 0.0\n    total_f1   = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    total_hd = 0.0\n    total_samples = 0\n    with torch.no_grad():\n        for images, masks, _ in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=pos_weight, lambda_boundary=lambda_boundary)\n            auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=threshold, smooth=1e-5)\n            hd_val = compute_hd_metric(outputs, masks, threshold=threshold)\n            bs = images.size(0)\n            total_loss += loss.item() * bs\n            total_auc  += auc_score * bs\n            total_iou  += iou_val * bs\n            total_f1   += f1_val * bs\n            total_precision += prec * bs\n            total_recall += rec * bs\n            total_hd += hd_val * bs\n            total_samples += bs\n    avg_loss = total_loss / total_samples\n    avg_auc  = total_auc / total_samples\n    avg_iou  = total_iou / total_samples\n    avg_f1   = total_f1 / total_samples\n    avg_precision = total_precision / total_samples\n    avg_recall = total_recall / total_samples\n    avg_hd = total_hd / total_samples\n    return avg_loss, avg_auc, avg_iou, avg_f1, avg_precision, avg_recall, avg_hd\n\n# ------------------------------\n# TTA Ìï®Ïàò (Test Time Augmentation)\n# ------------------------------\ndef tta_predict(model, image, device):\n    cj = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n    def add_noise(x, std=0.05):\n        noise = torch.randn_like(x) * std\n        return x + noise\n    def identity(x): return x\n    def hflip(x): return torch.flip(x, dims=[-1])\n    def vflip(x): return torch.flip(x, dims=[-2])\n    def rot90(x): return torch.rot90(x, k=1, dims=[-2, -1])\n    def inv_hflip(x): return torch.flip(x, dims=[-1])\n    def inv_vflip(x): return torch.flip(x, dims=[-2])\n    def inv_rot90(x): return torch.rot90(x, k=3, dims=[-2, -1])\n    \n    transforms_list = [\n        (identity, identity),\n        (hflip, inv_hflip),\n        (vflip, inv_vflip),\n        (rot90, inv_rot90),\n        (lambda x: add_noise(cj(x)), lambda x: x)\n    ]\n    predictions = []\n    model.eval()\n    with torch.no_grad():\n        for aug, inv in transforms_list:\n            augmented = aug(image)\n            output = model(augmented.unsqueeze(0).to(device))\n            output = torch.sigmoid(output)\n            output = inv(output).cpu()\n            predictions.append(output)\n    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n    return avg_prediction\n\n# ------------------------------\n# Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏÑ§Ï†ï (ÌÖåÏä§Ìä∏ Ìè¨Ìï®)\n# ------------------------------\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\nfull_dataset = BUSISegmentationDataset(data_path, transform=joint_transform)\nindices = np.arange(len(full_dataset))\ntrain_val_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\ntrain_idx, val_idx = train_test_split(train_val_idx, test_size=0.25, random_state=42)\ntrain_dataset = Subset(full_dataset, train_idx)\nval_dataset = Subset(full_dataset, val_idx)\ntest_dataset = Subset(full_dataset, test_idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0,\n                          worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id))\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n\ndef calculate_pos_weight(loader):\n    total_pixels = 0\n    positive_pixels = 0\n    for images, masks, _ in loader:\n        positive_pixels += masks.sum().item()\n        total_pixels += masks.numel()\n    negative_pixels = total_pixels - positive_pixels\n    return torch.tensor(negative_pixels / (positive_pixels + 1e-6)).to(device)\n\n# ------------------------------\n# Î™®Îç∏, Optimizer, Scheduler ÏÑ§Ï†ï (Training Loop)\n# ------------------------------\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = PretrainedSwin_UNet_AttentionFusion(out_channels=1)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\npos_weight = calculate_pos_weight(train_loader)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nscheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n\nscaler = GradScaler()\npatience = 50\nbest_val_f1 = 0.0\npatience_counter = 0\n\nmin_temp = 2.0\nmax_temp = 5.0\n\nnum_epochs = 500\ncombined_loader = None\n\nfor epoch in range(num_epochs):\n    current_temp = max_temp - ((max_temp - min_temp) * epoch / num_epochs)\n    if hasattr(model, 'bottleneck'):\n        if isinstance(model, nn.DataParallel):\n            model.module.bottleneck.gating.temperature = current_temp\n        else:\n            model.bottleneck.gating.temperature = current_temp\n\n    model.train()\n    epoch_loss = 0.0\n    epoch_auc = 0.0\n    epoch_iou = 0.0\n    epoch_f1 = 0.0\n    epoch_precision = 0.0\n    epoch_recall = 0.0\n    total_samples = 0\n\n    if epoch < 30:\n        train_loader = train_loader  # labeled_loader ÏÇ¨Ïö©\n    else:\n        if epoch == 30:\n            print(\"===> Generating pseudo labels on unlabeled data...\")\n            pseudo_loader = DataLoader(UnlabeledDataset('/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/', transform=joint_transform),\n                                       batch_size=16, shuffle=False, num_workers=0)\n            pseudo_data = generate_pseudo_labels(model, pseudo_loader, device, threshold=0.9)\n            print(f\"Pseudo labels generated for {len(pseudo_data)} images.\")\n            combined_dataset = CombinedDataset(train_dataset, pseudo_data)\n            combined_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True, num_workers=0)\n            train_loader = combined_loader\n        else:\n            train_loader = combined_loader\n\n    for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        images, masks = cutmix_data(images, masks, alpha=0.8, p=0.7)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(images)\n            loss = combined_loss(outputs, masks, pos_weight=None, lambda_boundary=0.2, lambda_lovasz=0.6)\n        if torch.isnan(loss):\n            print(\"Warning: loss is NaN, skipping batch\")\n            continue\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n        scaler.step(optimizer)\n        scaler.update()\n\n        bs = images.size(0)\n        epoch_loss += loss.item() * bs\n\n        auc_score, iou_val, f1_val, prec, rec = compute_batch_metrics_new(outputs, masks, threshold=0.5, smooth=1e-5)\n        epoch_auc += auc_score * bs\n        epoch_iou += iou_val * bs\n        epoch_f1 += f1_val * bs\n        epoch_precision += prec * bs\n        epoch_recall += rec * bs\n        total_samples += bs\n\n    scheduler.step(epoch)\n    avg_loss = epoch_loss / total_samples if total_samples > 0 else float('nan')\n    avg_auc = epoch_auc / total_samples if total_samples > 0 else float('nan')\n    avg_iou = epoch_iou / total_samples if total_samples > 0 else float('nan')\n    avg_f1 = epoch_f1 / total_samples if total_samples > 0 else float('nan')\n    avg_precision = epoch_precision / total_samples if total_samples > 0 else float('nan')\n    avg_recall = epoch_recall / total_samples if total_samples > 0 else float('nan')\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}, AUC: {avg_auc:.4f}, IoU: {avg_iou:.4f}, Dice/F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f} (Temp: {current_temp:.2f})\")\n    \n    val_loss, val_auc, val_iou, val_f1, val_precision, val_recall, val_hd = evaluate_with_metrics(model, val_loader, device, threshold=0.5, lambda_boundary=0.2, pos_weight=None)\n    print(f\"[Val] Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, IoU: {val_iou:.4f}, Dice/F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, HD95: {val_hd:.4f}\")\n    \n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_weights = copy.deepcopy(model.state_dict())\n        torch.save(best_weights, 'best_model_with_pseudo.pth')\n        patience_counter = 0\n        print(\"üçÄ New best validation Dice achieved! Dice: {:.4f} üçÄ\".format(best_val_f1))\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"Training complete. Best model saved as 'best_model_with_pseudo.pth'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T08:29:50.130748Z","iopub.execute_input":"2025-03-15T08:29:50.131102Z","iopub.status.idle":"2025-03-15T08:32:24.231181Z","shell.execute_reply.started":"2025-03-15T08:29:50.131072Z","shell.execute_reply":"2025-03-15T08:32:24.229959Z"}},"outputs":[{"name":"stdout","text":"Folder created: /kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/unlabeled-busi-images/\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/500 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:23<00:00,  2.79s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500 | Loss: 1.0333, AUC: 0.8713, IoU: 0.3501, Dice/F1: 0.4121, Precision: 0.4011, Recall: 0.5522 (Temp: 5.00)\n[Val] Loss: 1.0126, AUC: 0.9479, IoU: 0.4751, Dice/F1: 0.5198, Precision: 0.4828, Recall: 0.6790, HD95: 66.5344\nüçÄ New best validation Dice achieved! Dice: 0.5198 üçÄ\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/500 Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 11/30 [00:25<00:43,  2.29s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-aaa0dd000a28>\u001b[0m in \u001b[0;36m<cell line: 570>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mauc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_batch_metrics_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0mepoch_auc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mauc_score\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mepoch_iou\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0miou_val\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-aaa0dd000a28>\u001b[0m in \u001b[0;36mcompute_batch_metrics_new\u001b[0;34m(outputs, masks, threshold, smooth)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mauc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mauc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         return _average_binary_score(\n\u001b[0m\u001b[1;32m    573\u001b[0m             \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_fpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_fpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    342\u001b[0m         )\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \"\"\"\n\u001b[0;32m--> 992\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;31m# sort scores and corresponding truth values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m     \u001b[0mdesc_score_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mergesort\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdesc_score_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \"\"\"\n\u001b[0;32m-> 1133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}